#!/usr/bin/env python3
"""
Download all embedding models required for the Context Graph project.
Models are saved to /home/cabdru/contextgraph/models/
"""

import os
import sys
from pathlib import Path
from huggingface_hub import snapshot_download
import torch

MODELS_DIR = Path("/home/cabdru/contextgraph/models")

# Model definitions based on PRD spec
MODELS = {
    "semantic": {
        "repo_id": "intfloat/e5-large-v2",
        "description": "E5-Large semantic embeddings (1024D)",
    },
    "code": {
        "repo_id": "Qodo/Qodo-Embed-1-1.5B",
        "description": "Qodo-Embed Qwen2-based code embeddings (1536D)",
    },
    "multimodal": {
        "repo_id": "openai/clip-vit-large-patch14",
        "description": "CLIP for multimodal embeddings (768D)",
    },
    "sparse": {
        "repo_id": "naver/splade-cocondenser-ensembledistil",
        "description": "SPLADE sparse embeddings (~30K sparse)",
    },
    "late-interaction": {
        "repo_id": "colbert-ir/colbertv2.0",
        "description": "ColBERT late-interaction (128D/token)",
    },
    "entity": {
        "repo_id": "sentence-transformers/all-MiniLM-L6-v2",
        "description": "Entity embeddings fallback (384D)",
    },
    "causal": {
        "repo_id": "allenai/longformer-base-4096",
        "description": "Longformer for causal embeddings (768D)",
    },
    "contextual": {
        "repo_id": "intfloat/e5-base-v2",
        "description": "E5-base-v2 asymmetric retrieval (768D) - query/passage prefixes",
    },
    "graph": {
        "repo_id": "sentence-transformers/paraphrase-MiniLM-L6-v2",
        "description": "Base for graph embeddings (384D)",
        "subdir": "graph",
    },
}

def download_model(name: str, config: dict) -> bool:
    """Download a single model."""
    subdir = config.get("subdir", name)
    target_dir = MODELS_DIR / subdir
    target_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*60}")
    print(f"Downloading: {name}")
    print(f"Repository: {config['repo_id']}")
    print(f"Description: {config['description']}")
    print(f"Target: {target_dir}")
    print('='*60)

    try:
        snapshot_download(
            repo_id=config["repo_id"],
            local_dir=str(target_dir),
            local_dir_use_symlinks=False,
            resume_download=True,
        )
        print(f"SUCCESS: {name} downloaded to {target_dir}")
        return True
    except Exception as e:
        print(f"ERROR: Failed to download {name}: {e}")
        return False

def main():
    print("Context Graph - Embedding Model Downloader")
    print(f"Target directory: {MODELS_DIR}")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA device: {torch.cuda.get_device_name(0)}")

    MODELS_DIR.mkdir(parents=True, exist_ok=True)

    results = {}
    for name, config in MODELS.items():
        results[name] = download_model(name, config)

    print("\n" + "="*60)
    print("DOWNLOAD SUMMARY")
    print("="*60)

    success = sum(1 for v in results.values() if v)
    failed = sum(1 for v in results.values() if not v)

    for name, status in results.items():
        status_str = "OK" if status else "FAILED"
        print(f"  {name}: {status_str}")

    print(f"\nTotal: {success} succeeded, {failed} failed")

    # Write config file for the Rust crate
    config_path = MODELS_DIR / "models_config.toml"
    with open(config_path, "w") as f:
        f.write("# Auto-generated embedding models configuration\n")
        f.write("# Generated by download_models.py\n\n")
        f.write("[models]\n")
        for name, config in MODELS.items():
            subdir = config.get("subdir", name)
            f.write(f'{name} = {{ path = "{MODELS_DIR / subdir}", repo = "{config["repo_id"]}" }}\n')

    print(f"\nConfig written to: {config_path}")

    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main())
