//! Production MultiArrayEmbeddingProvider that orchestrates all 13 embedders.
//!
//! This module provides the [`ProductionMultiArrayProvider`] which replaces the
//! placeholder [`LazyFailMultiArrayProvider`] with real model implementations.
//!
//! # Architecture
//!
//! ```text
//! ProductionMultiArrayProvider
//!     |-- 10 dense SingleEmbedder instances (E1-E5, E7-E11)
//!     |-- 2 SparseEmbedder instances (E6, E13 - SPLADE)
//!     |-- 1 TokenEmbedder (E12 ColBERT)
//!
//!     Returns: SemanticFingerprint with all 13 embeddings
//! ```
//!
//! # Design Principles
//!
//! - **NO STUBS**: Uses real model implementations from DefaultModelFactory
//! - **FAIL FAST**: Returns clear errors if models not loaded
//! - **PARALLEL EXECUTION**: All 13 embedders run concurrently via tokio::join!
//! - **THREAD SAFE**: Send + Sync for async task spawning across threads
//!
//! # Performance Targets (from constitution.yaml)
//!
//! - Single content: <30ms for all 13 embeddings
//! - Batch (64 items): <100ms per item average
//!
//! # Example
//!
//! ```ignore
//! use context_graph_embeddings::provider::ProductionMultiArrayProvider;
//! use context_graph_embeddings::config::GpuConfig;
//! use std::path::PathBuf;
//!
//! // Create provider (models must exist at models_dir)
//! let provider = ProductionMultiArrayProvider::new(
//!     PathBuf::from("./models"),
//!     GpuConfig::default(),
//! ).await?;
//!
//! // Generate all 13 embeddings
//! let output = provider.embed_all("Hello world").await?;
//! assert!(output.is_within_latency_target()); // <30ms
//! ```

use std::path::PathBuf;
use std::sync::Arc;
use std::time::{Duration, Instant};

use async_trait::async_trait;
use tokio::sync::RwLock;

use context_graph_core::error::{CoreError, CoreResult};
use context_graph_core::traits::{
    CausalHint, EmbeddingMetadata, MultiArrayEmbeddingOutput, MultiArrayEmbeddingProvider,
    SingleEmbedder, SparseEmbedder, TokenEmbedder,
};
use context_graph_core::types::fingerprint::{
    SemanticFingerprint, SparseVector, E11_DIM, E12_TOKEN_DIM, E1_DIM, E2_DIM, E3_DIM, E4_DIM,
    E7_DIM, E9_DIM, NUM_EMBEDDERS,
};

use crate::config::GpuConfig;
use crate::error::EmbeddingResult;
use crate::models::pretrained::{CausalModel, ContextualModel, GraphModel};
use crate::models::DefaultModelFactory;
use crate::traits::{EmbeddingModel, ModelFactory, SingleModelConfig};
use crate::types::{ModelId, ModelInput};

// ============================================================================
// ADAPTER TYPES - Bridge EmbeddingModel to SingleEmbedder/SparseEmbedder/TokenEmbedder
// ============================================================================

/// Adapter that wraps an EmbeddingModel to implement SingleEmbedder trait.
struct DenseEmbedderAdapter {
    model: Arc<RwLock<Box<dyn EmbeddingModel>>>,
    model_id: ModelId,
    dimension: usize,
}

impl DenseEmbedderAdapter {
    fn new(model: Box<dyn EmbeddingModel>, model_id: ModelId, dimension: usize) -> Self {
        Self {
            model: Arc::new(RwLock::new(model)),
            model_id,
            dimension,
        }
    }
}

impl DenseEmbedderAdapter {
    /// Embed content with a custom instruction for the model.
    ///
    /// E4-FIX: Used for passing sequence numbers to E4 via "sequence:N" instruction.
    async fn embed_with_instruction(
        &self,
        content: &str,
        instruction: Option<&str>,
    ) -> CoreResult<Vec<f32>> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }

        let model = self.model.read().await;
        if !model.is_initialized() {
            return Err(CoreError::Internal(format!(
                "Model {:?} not initialized",
                self.model_id
            )));
        }

        let input = match instruction {
            Some(inst) => {
                ModelInput::text_with_instruction(content, inst).map_err(|e| {
                    CoreError::ValidationError {
                        field: "content".to_string(),
                        message: e.to_string(),
                    }
                })?
            }
            None => ModelInput::text(content).map_err(|e| CoreError::ValidationError {
                field: "content".to_string(),
                message: e.to_string(),
            })?,
        };

        let embedding = model.embed(&input).await.map_err(|e| {
            CoreError::Embedding(format!("Embedding failed for {:?}: {}", self.model_id, e))
        })?;

        Ok(embedding.into_vec())
    }
}

#[async_trait]
impl SingleEmbedder for DenseEmbedderAdapter {
    fn dimension(&self) -> usize {
        self.dimension
    }

    fn model_id(&self) -> &str {
        self.model_id.as_str()
    }

    async fn embed(&self, content: &str) -> CoreResult<Vec<f32>> {
        self.embed_with_instruction(content, None).await
    }

    fn is_ready(&self) -> bool {
        // We can't easily check without blocking, so we assume ready if constructed
        // The actual check happens in embed()
        true
    }
}

// Safety: DenseEmbedderAdapter is Send + Sync because it wraps Arc<RwLock<...>>
unsafe impl Send for DenseEmbedderAdapter {}
unsafe impl Sync for DenseEmbedderAdapter {}

/// Adapter that wraps a Sparse EmbeddingModel (SPLADE) to implement SparseEmbedder trait.
struct SparseEmbedderAdapter {
    model: Arc<RwLock<Box<dyn EmbeddingModel>>>,
    model_id: ModelId,
}

impl SparseEmbedderAdapter {
    fn new(model: Box<dyn EmbeddingModel>, model_id: ModelId) -> Self {
        Self {
            model: Arc::new(RwLock::new(model)),
            model_id,
        }
    }
}

#[async_trait]
impl SparseEmbedder for SparseEmbedderAdapter {
    fn vocab_size(&self) -> usize {
        30522 // SPLADE uses BERT vocabulary
    }

    fn model_id(&self) -> &str {
        self.model_id.as_str()
    }

    async fn embed_sparse(&self, content: &str) -> CoreResult<SparseVector> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }

        let model = self.model.read().await;
        if !model.is_initialized() {
            return Err(CoreError::Internal(format!(
                "Model {:?} not initialized",
                self.model_id
            )));
        }

        let input = ModelInput::text(content).map_err(|e| CoreError::ValidationError {
            field: "content".to_string(),
            message: e.to_string(),
        })?;

        // Call embed_sparse() to get actual sparse vocabulary indices and weights
        // NOT embed() which returns a 1536D projected dense vector
        let (indices, values) = model.embed_sparse(&input).await.map_err(|e| {
            CoreError::Embedding(format!(
                "Sparse embedding failed for {:?}: {}",
                self.model_id, e
            ))
        })?;

        SparseVector::new(indices, values)
            .map_err(|e| CoreError::Internal(format!("Failed to create sparse vector: {}", e)))
    }

    fn is_ready(&self) -> bool {
        true
    }
}

unsafe impl Send for SparseEmbedderAdapter {}
unsafe impl Sync for SparseEmbedderAdapter {}

/// Adapter that wraps ColBERT model to implement TokenEmbedder trait.
struct TokenEmbedderAdapter {
    model: Arc<RwLock<Box<dyn EmbeddingModel>>>,
    model_id: ModelId,
}

impl TokenEmbedderAdapter {
    fn new(model: Box<dyn EmbeddingModel>, model_id: ModelId) -> Self {
        Self {
            model: Arc::new(RwLock::new(model)),
            model_id,
        }
    }
}

#[async_trait]
impl TokenEmbedder for TokenEmbedderAdapter {
    fn token_dimension(&self) -> usize {
        E12_TOKEN_DIM // 128D per token
    }

    fn max_tokens(&self) -> usize {
        512 // ColBERT uses BERT tokenizer
    }

    fn model_id(&self) -> &str {
        self.model_id.as_str()
    }

    async fn embed_tokens(&self, content: &str) -> CoreResult<Vec<Vec<f32>>> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }

        let model = self.model.read().await;
        if !model.is_initialized() {
            return Err(CoreError::Internal(format!(
                "Model {:?} not initialized",
                self.model_id
            )));
        }

        let input = ModelInput::text(content).map_err(|e| CoreError::ValidationError {
            field: "content".to_string(),
            message: e.to_string(),
        })?;

        let embedding = model.embed(&input).await.map_err(|e| {
            CoreError::Embedding(format!(
                "Token embedding failed for {:?}: {}",
                self.model_id, e
            ))
        })?;

        // For ColBERT, the model produces [num_tokens, 128] tensor
        // We reshape the flat vector into token embeddings
        let flat = embedding.into_vec();
        let token_dim = E12_TOKEN_DIM;

        if flat.len() % token_dim != 0 {
            return Err(CoreError::Internal(format!(
                "ColBERT output size {} not divisible by token dimension {}",
                flat.len(),
                token_dim
            )));
        }

        let num_tokens = flat.len() / token_dim;
        let mut tokens = Vec::with_capacity(num_tokens);

        for i in 0..num_tokens {
            let start = i * token_dim;
            let end = start + token_dim;
            tokens.push(flat[start..end].to_vec());
        }

        Ok(tokens)
    }

    fn is_ready(&self) -> bool {
        true
    }
}

unsafe impl Send for TokenEmbedderAdapter {}
unsafe impl Sync for TokenEmbedderAdapter {}

// ============================================================================
// CAUSAL DUAL EMBEDDER - Specialized adapter for E5 asymmetric embeddings
// ============================================================================

/// Adapter for E5 CausalModel that produces dual (cause, effect) embeddings.
///
/// Per ARCH-15: "E5 Causal MUST use asymmetric similarity with separate
/// cause/effect vector encodings - cause→effect direction matters"
///
/// This adapter exposes the `embed_dual()` method from CausalModel, which
/// produces genuinely different vectors for cause vs effect roles.
struct CausalDualEmbedderAdapter {
    /// Direct reference to CausalModel (not wrapped in EmbeddingModel trait)
    model: Arc<CausalModel>,
}

impl CausalDualEmbedderAdapter {
    /// Create a new CausalDualEmbedderAdapter.
    ///
    /// # Arguments
    /// * `model` - CausalModel instance (must be loaded before use)
    fn new(model: CausalModel) -> Self {
        Self {
            model: Arc::new(model),
        }
    }

    /// Embed content as both cause and effect roles.
    ///
    /// Returns (cause_vector, effect_vector) where each is 768D.
    /// The vectors are genuinely different due to instruction prefixes.
    ///
    /// # Errors
    /// - `CoreError::Internal` if model not initialized
    /// - `CoreError::Embedding` if embedding fails
    async fn embed_dual(&self, content: &str) -> CoreResult<(Vec<f32>, Vec<f32>)> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }

        if !self.model.is_initialized() {
            return Err(CoreError::Internal(
                "CausalModel not initialized for dual embedding".to_string(),
            ));
        }

        self.model.embed_dual(content).await.map_err(|e| {
            CoreError::Embedding(format!("E5 dual embedding failed: {}", e))
        })
    }

    /// Check if the model is ready for embedding.
    fn is_ready(&self) -> bool {
        self.model.is_initialized()
    }

    /// Embed content with LLM-provided causal hint for enhanced direction awareness.
    ///
    /// If a useful hint is provided (is_causal && confidence >= 0.5), the embedding
    /// vectors are biased based on the direction hint:
    /// - `CausalDirectionHint::Cause`: Boost cause vector (1.3x), dampen effect (0.8x)
    /// - `CausalDirectionHint::Effect`: Boost effect vector (1.3x), dampen cause (0.8x)
    /// - `CausalDirectionHint::Neutral`: No bias applied
    ///
    /// If no hint is provided or hint is not useful, falls back to standard `embed_dual()`.
    ///
    /// # Arguments
    ///
    /// * `content` - The text content to embed
    /// * `hint` - Optional LLM-generated causal hint
    ///
    /// # Returns
    ///
    /// (cause_vector, effect_vector) where each is 768D, with direction bias applied.
    ///
    /// # CAUSAL-HINT Phase 4: E5 Enhancement
    ///
    /// This method enables LLM-enhanced E5 embeddings per the Causal Discovery
    /// LLM + E5 Integration Plan.
    async fn embed_dual_with_hint(
        &self,
        content: &str,
        hint: Option<&CausalHint>,
    ) -> CoreResult<(Vec<f32>, Vec<f32>)> {
        // Get base embeddings
        let (mut cause_vec, mut effect_vec) = self.embed_dual(content).await?;

        // Apply direction bias if hint is useful
        if let Some(hint) = hint {
            if hint.is_useful() {
                let (cause_bias, effect_bias) = hint.bias_factors();

                // Apply bias to cause vector
                for val in cause_vec.iter_mut() {
                    *val *= cause_bias;
                }

                // Apply bias to effect vector
                for val in effect_vec.iter_mut() {
                    *val *= effect_bias;
                }

                tracing::debug!(
                    direction = ?hint.direction_hint,
                    cause_bias = cause_bias,
                    effect_bias = effect_bias,
                    confidence = hint.confidence,
                    "E5: Applied LLM causal hint bias to embeddings"
                );
            }
        }

        Ok((cause_vec, effect_vec))
    }
}

unsafe impl Send for CausalDualEmbedderAdapter {}
unsafe impl Sync for CausalDualEmbedderAdapter {}

// ============================================================================
// GRAPH DUAL EMBEDDER ADAPTER (E8 - Asymmetric Source/Target)
// ============================================================================

/// Adapter that wraps GraphModel to support dual source/target embedding.
///
/// Following the E5 Causal pattern (ARCH-15), this adapter enables asymmetric
/// similarity for graph relationships where direction matters:
/// - **Source embedding**: "What does X use?" → X is the source
/// - **Target embedding**: "What uses X?" → X is the target
///
/// The GraphModel.embed_dual() method produces genuinely different vectors
/// through learned projections (W_source, W_target).
struct GraphDualEmbedderAdapter {
    /// Direct reference to GraphModel (not wrapped in EmbeddingModel trait)
    model: Arc<GraphModel>,
}

impl GraphDualEmbedderAdapter {
    /// Create a new GraphDualEmbedderAdapter.
    ///
    /// # Arguments
    /// * `model` - GraphModel instance (must be loaded before use)
    fn new(model: GraphModel) -> Self {
        Self {
            model: Arc::new(model),
        }
    }

    /// Embed content as both source and target roles.
    ///
    /// Returns (source_vector, target_vector) where each is 384D.
    /// The vectors are genuinely different due to learned projections.
    ///
    /// # Errors
    /// - `CoreError::Internal` if model not initialized
    /// - `CoreError::Embedding` if embedding fails
    async fn embed_dual(&self, content: &str) -> CoreResult<(Vec<f32>, Vec<f32>)> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }

        if !self.model.is_initialized() {
            return Err(CoreError::Internal(
                "GraphModel not initialized for dual embedding".to_string(),
            ));
        }

        self.model.embed_dual(content).await.map_err(|e| {
            CoreError::Embedding(format!("E8 dual embedding failed: {}", e))
        })
    }

    /// Check if the model is ready for embedding.
    fn is_ready(&self) -> bool {
        self.model.is_initialized()
    }
}

unsafe impl Send for GraphDualEmbedderAdapter {}
unsafe impl Sync for GraphDualEmbedderAdapter {}

// ============================================================================
// CONTEXTUAL (E10) DUAL EMBEDDER ADAPTER - Asymmetric Intent/Context
// ============================================================================

/// Adapter for E10 ContextualModel that produces dual (intent, context) embeddings.
///
/// Following the E5 Causal and E8 Graph patterns (ARCH-15), this adapter enables
/// asymmetric similarity for intent-context relationships where direction matters:
/// - **Intent embedding**: "What is this text trying to accomplish?" (action-focused)
/// - **Context embedding**: "What context does this establish?" (relation-focused)
///
/// Direction modifiers (per plan):
/// - intent→context: 1.2x (query intent finds relevant context)
/// - context→intent: 0.8x (dampened reverse direction)
struct ContextualDualEmbedderAdapter {
    /// Direct reference to ContextualModel (not wrapped in EmbeddingModel trait)
    model: Arc<ContextualModel>,
}

impl ContextualDualEmbedderAdapter {
    /// Create a new ContextualDualEmbedderAdapter.
    ///
    /// # Arguments
    /// * `model` - ContextualModel instance (must be loaded before use)
    fn new(model: ContextualModel) -> Self {
        Self {
            model: Arc::new(model),
        }
    }

    /// Embed content as both intent and context roles.
    ///
    /// Returns (intent_vector, context_vector) where each is 768D.
    /// The vectors are genuinely different due to learned projections.
    ///
    /// # Errors
    /// - `CoreError::Internal` if model not initialized
    /// - `CoreError::Embedding` if embedding fails
    async fn embed_dual(&self, content: &str) -> CoreResult<(Vec<f32>, Vec<f32>)> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }

        if !self.model.is_initialized() {
            return Err(CoreError::Internal(
                "ContextualModel not initialized for dual embedding".to_string(),
            ));
        }

        self.model.embed_dual(content).await.map_err(|e| {
            CoreError::Embedding(format!("E10 dual embedding failed: {}", e))
        })
    }

    /// Check if the model is ready for embedding.
    fn is_ready(&self) -> bool {
        self.model.is_initialized()
    }
}

unsafe impl Send for ContextualDualEmbedderAdapter {}
unsafe impl Sync for ContextualDualEmbedderAdapter {}

// ============================================================================
// PRODUCTION MULTI-ARRAY PROVIDER
// ============================================================================

/// Production MultiArrayEmbeddingProvider that orchestrates all 13 embedders.
///
/// This provider replaces the placeholder LazyFailMultiArrayProvider with real
/// model implementations using the DefaultModelFactory.
///
/// # Thread Safety
///
/// This provider is `Send + Sync` and can be shared across async tasks.
/// Internal models use RwLock for safe concurrent access.
///
/// # Model Loading
///
/// Models are loaded eagerly during construction. The async `new()` method
/// loads all 13 models and fails fast if any model cannot be loaded.
///
/// # Performance
///
/// All 13 embedders run in parallel using tokio::join! to achieve
/// the <30ms latency target for single content embedding.
pub struct ProductionMultiArrayProvider {
    /// E1: Semantic embedder (e5-large-v2, 1024D)
    e1_semantic: Arc<dyn SingleEmbedder>,
    /// E2: Temporal-Recent embedder (exponential decay, 512D)
    e2_temporal_recent: Arc<dyn SingleEmbedder>,
    /// E3: Temporal-Periodic embedder (Fourier, 512D)
    e3_temporal_periodic: Arc<dyn SingleEmbedder>,
    /// E4: Temporal-Positional embedder (sinusoidal PE, 512D)
    ///
    /// E4-FIX: Stored as concrete type to allow `embed_with_instruction()` calls
    /// for passing sequence numbers via "sequence:N" instruction.
    e4_temporal_positional: Arc<DenseEmbedderAdapter>,
    /// E5: Causal embedder (Longformer, 768D) - DUAL embedder for asymmetric similarity
    ///
    /// Per ARCH-15: Uses CausalDualEmbedderAdapter to produce genuinely different
    /// vectors for cause vs effect roles.
    e5_causal: Arc<CausalDualEmbedderAdapter>,
    /// E6: Sparse embedder (SPLADE, variable sparse)
    e6_sparse: Arc<dyn SparseEmbedder>,
    /// E7: Code embedder (Qodo-Embed, 1536D)
    e7_code: Arc<dyn SingleEmbedder>,
    /// E8: Graph embedder (MiniLM, 384D) - DUAL embedder for asymmetric similarity
    ///
    /// Per E8 Upgrade: Uses GraphDualEmbedderAdapter to produce genuinely different
    /// vectors for source vs target roles.
    e8_graph: Arc<GraphDualEmbedderAdapter>,
    /// E9: HDC embedder (hyperdimensional, 1024D projected)
    e9_hdc: Arc<dyn SingleEmbedder>,
    /// E10: Contextual embedder (MPNet, 768D) - DUAL embedder for asymmetric similarity
    ///
    /// Per E10 Upgrade: Uses ContextualDualEmbedderAdapter to produce genuinely different
    /// vectors for intent vs context roles.
    e10_contextual: Arc<ContextualDualEmbedderAdapter>,
    /// E11: Entity embedder (KEPLER, 768D)
    ///
    /// KEPLER is RoBERTa-base trained with TransE on Wikidata5M (4.8M entities, 20M triples).
    /// Unlike the previous MiniLM model, TransE operations (h + r ≈ t) are semantically meaningful.
    e11_entity: Arc<dyn SingleEmbedder>,
    /// E12: Late-Interaction embedder (ColBERT, 128D per token)
    e12_late_interaction: Arc<dyn TokenEmbedder>,
    /// E13: SPLADE v3 sparse embedder (variable sparse)
    e13_splade: Arc<dyn SparseEmbedder>,

    /// Model IDs for tracking
    model_ids: [String; NUM_EMBEDDERS],
}

impl ProductionMultiArrayProvider {
    /// Create a new ProductionMultiArrayProvider with all 13 embedders.
    ///
    /// This constructor creates all models in an unloaded state. Call
    /// `initialize()` to load model weights before embedding.
    ///
    /// # Arguments
    ///
    /// * `models_dir` - Base directory containing pretrained model files
    /// * `gpu_config` - GPU configuration for inference
    ///
    /// # Returns
    ///
    /// A new provider instance with all embedders initialized (but not loaded).
    ///
    /// # Errors
    ///
    /// Returns `EmbeddingError` if model creation fails.
    ///
    /// # Example
    ///
    /// ```ignore
    /// let provider = ProductionMultiArrayProvider::new(
    ///     PathBuf::from("./models"),
    ///     GpuConfig::default(),
    /// ).await?;
    /// ```
    pub async fn new(models_dir: PathBuf, gpu_config: GpuConfig) -> EmbeddingResult<Self> {
        let factory = DefaultModelFactory::new(models_dir.clone(), gpu_config);
        let config = SingleModelConfig::cuda_fp16();

        // Create all 13 models using the factory
        // NOTE: E5 (Causal) is created directly for dual embedding support (ARCH-15)
        let e1_model = factory.create_model(ModelId::Semantic, &config)?;
        let e2_model = factory.create_model(ModelId::TemporalRecent, &config)?;
        let e3_model = factory.create_model(ModelId::TemporalPeriodic, &config)?;
        let e4_model = factory.create_model(ModelId::TemporalPositional, &config)?;

        // E5: Create CausalModel directly for dual embedding support
        let e5_causal_model = CausalModel::new(&models_dir.join("causal"), config.clone())?;

        let e6_model = factory.create_model(ModelId::Sparse, &config)?;
        let e7_model = factory.create_model(ModelId::Code, &config)?;

        // E8: Create GraphModel using shared e5-large-v2 model (VRAM sharing per E8 Upgrade)
        // The graph model path points to semantic to share the e5-large-v2 weights with E1
        let e8_graph_model = GraphModel::new(&models_dir.join("semantic"), config.clone())?;
        let e9_model = factory.create_model(ModelId::Hdc, &config)?;

        // E10: Create ContextualModel directly for dual embedding support (E10 Upgrade)
        let e10_contextual_model =
            ContextualModel::new(&models_dir.join("contextual"), config.clone())?;

        // E11: Use KEPLER (RoBERTa-base + TransE) for entity embeddings
        let e11_model = factory.create_model(ModelId::Kepler, &config)?;
        let e12_model = factory.create_model(ModelId::LateInteraction, &config)?;
        let e13_model = factory.create_model(ModelId::Splade, &config)?;

        // Load all models BEFORE wrapping in adapters (FAIL FAST)
        // Per constitution.yaml: models must be ready before embed()
        tracing::info!("Loading all 13 embedding models...");

        e1_model.load().await?;
        e2_model.load().await?;
        e3_model.load().await?;
        e4_model.load().await?;
        e5_causal_model.load().await?; // E5 loaded directly
        e6_model.load().await?;
        e7_model.load().await?;
        e8_graph_model.load().await?; // E8 loaded directly for dual embedding
        e9_model.load().await?;
        e10_contextual_model.load().await?; // E10 loaded directly for dual embedding
        e11_model.load().await?;
        e12_model.load().await?;
        e13_model.load().await?;

        tracing::info!("All 13 embedding models loaded successfully");

        // Wrap models in appropriate adapters
        let e1_semantic: Arc<dyn SingleEmbedder> = Arc::new(DenseEmbedderAdapter::new(
            e1_model,
            ModelId::Semantic,
            E1_DIM,
        ));
        let e2_temporal_recent: Arc<dyn SingleEmbedder> = Arc::new(DenseEmbedderAdapter::new(
            e2_model,
            ModelId::TemporalRecent,
            E2_DIM,
        ));
        let e3_temporal_periodic: Arc<dyn SingleEmbedder> = Arc::new(DenseEmbedderAdapter::new(
            e3_model,
            ModelId::TemporalPeriodic,
            E3_DIM,
        ));
        // E4-FIX: Store as concrete type for embed_with_instruction() access
        let e4_temporal_positional: Arc<DenseEmbedderAdapter> = Arc::new(DenseEmbedderAdapter::new(
            e4_model,
            ModelId::TemporalPositional,
            E4_DIM,
        ));

        // E5: Use CausalDualEmbedderAdapter for asymmetric embeddings (ARCH-15)
        let e5_causal: Arc<CausalDualEmbedderAdapter> =
            Arc::new(CausalDualEmbedderAdapter::new(e5_causal_model));
        let e6_sparse: Arc<dyn SparseEmbedder> =
            Arc::new(SparseEmbedderAdapter::new(e6_model, ModelId::Sparse));
        let e7_code: Arc<dyn SingleEmbedder> =
            Arc::new(DenseEmbedderAdapter::new(e7_model, ModelId::Code, E7_DIM));

        // E8: Use GraphDualEmbedderAdapter for asymmetric embeddings (E8 Upgrade)
        let e8_graph: Arc<GraphDualEmbedderAdapter> =
            Arc::new(GraphDualEmbedderAdapter::new(e8_graph_model));
        let e9_hdc: Arc<dyn SingleEmbedder> =
            Arc::new(DenseEmbedderAdapter::new(e9_model, ModelId::Hdc, E9_DIM));

        // E10: Use ContextualDualEmbedderAdapter for asymmetric embeddings (E10 Upgrade)
        let e10_contextual: Arc<ContextualDualEmbedderAdapter> =
            Arc::new(ContextualDualEmbedderAdapter::new(e10_contextual_model));

        let e11_entity: Arc<dyn SingleEmbedder> = Arc::new(DenseEmbedderAdapter::new(
            e11_model,
            ModelId::Kepler,
            E11_DIM,
        ));
        let e12_late_interaction: Arc<dyn TokenEmbedder> = Arc::new(TokenEmbedderAdapter::new(
            e12_model,
            ModelId::LateInteraction,
        ));
        let e13_splade: Arc<dyn SparseEmbedder> =
            Arc::new(SparseEmbedderAdapter::new(e13_model, ModelId::Splade));

        let model_ids = [
            ModelId::Semantic.as_str().to_string(),
            ModelId::TemporalRecent.as_str().to_string(),
            ModelId::TemporalPeriodic.as_str().to_string(),
            ModelId::TemporalPositional.as_str().to_string(),
            ModelId::Causal.as_str().to_string(),
            ModelId::Sparse.as_str().to_string(),
            ModelId::Code.as_str().to_string(),
            ModelId::Graph.as_str().to_string(),
            ModelId::Hdc.as_str().to_string(),
            ModelId::Multimodal.as_str().to_string(),
            ModelId::Kepler.as_str().to_string(), // E11: KEPLER (was Entity/MiniLM)
            ModelId::LateInteraction.as_str().to_string(),
            ModelId::Splade.as_str().to_string(),
        ];

        Ok(Self {
            e1_semantic,
            e2_temporal_recent,
            e3_temporal_periodic,
            e4_temporal_positional,
            e5_causal,
            e6_sparse,
            e7_code,
            e8_graph,
            e9_hdc,
            e10_contextual,
            e11_entity,
            e12_late_interaction,
            e13_splade,
            model_ids,
        })
    }

    /// Helper to measure and run an embedder, returning (result, duration).
    async fn timed_embed<F, T>(embedder_name: &str, fut: F) -> (Result<T, CoreError>, Duration)
    where
        F: std::future::Future<Output = Result<T, CoreError>>,
    {
        let start = Instant::now();
        let result = fut.await;
        let duration = start.elapsed();
        if let Err(ref e) = result {
            tracing::warn!("Embedder {} failed: {}", embedder_name, e);
        }
        (result, duration)
    }
}

#[async_trait]
impl MultiArrayEmbeddingProvider for ProductionMultiArrayProvider {
    /// Generate complete 13-embedding fingerprint for content.
    ///
    /// All 13 embedders run in parallel using tokio::join! for optimal performance.
    ///
    /// # Arguments
    ///
    /// * `content` - Text content to embed (must be non-empty)
    ///
    /// # Returns
    ///
    /// A `MultiArrayEmbeddingOutput` containing:
    /// - Complete 13-embedding fingerprint
    /// - Total and per-embedder latency metrics
    /// - Model IDs used
    ///
    /// # Errors
    ///
    /// Returns `CoreError` if:
    /// - Content is empty (`CoreError::ValidationError`)
    /// - Any embedder fails (propagated error)
    /// - Provider is not ready (`CoreError::Internal`)
    ///
    /// # Performance
    ///
    /// Target latency: <30ms for all 13 embeddings (constitution.yaml)
    async fn embed_all(&self, content: &str) -> CoreResult<MultiArrayEmbeddingOutput> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }

        let start = Instant::now();

        // Clone Arc references for parallel execution
        let e1 = Arc::clone(&self.e1_semantic);
        let e2 = Arc::clone(&self.e2_temporal_recent);
        let e3 = Arc::clone(&self.e3_temporal_periodic);
        let e4 = Arc::clone(&self.e4_temporal_positional);
        let e5 = Arc::clone(&self.e5_causal);
        let e6 = Arc::clone(&self.e6_sparse);
        let e7 = Arc::clone(&self.e7_code);
        let e8 = Arc::clone(&self.e8_graph);
        let e9 = Arc::clone(&self.e9_hdc);
        let e10 = Arc::clone(&self.e10_contextual);
        let e11 = Arc::clone(&self.e11_entity);
        let e12 = Arc::clone(&self.e12_late_interaction);
        let e13 = Arc::clone(&self.e13_splade);

        let content_owned = content.to_string();

        // Run all 13 embedders in parallel
        let (
            (r1, d1),
            (r2, d2),
            (r3, d3),
            (r4, d4),
            (r5, d5),
            (r6, d6),
            (r7, d7),
            (r8, d8),
            (r9, d9),
            (r10, d10),
            (r11, d11),
            (r12, d12),
            (r13, d13),
        ) = tokio::join!(
            Self::timed_embed("E1_Semantic", {
                let c = content_owned.clone();
                async move { e1.embed(&c).await }
            }),
            Self::timed_embed("E2_TemporalRecent", {
                let c = content_owned.clone();
                async move { e2.embed(&c).await }
            }),
            Self::timed_embed("E3_TemporalPeriodic", {
                let c = content_owned.clone();
                async move { e3.embed(&c).await }
            }),
            Self::timed_embed("E4_TemporalPositional", {
                let c = content_owned.clone();
                async move { e4.embed(&c).await }
            }),
            Self::timed_embed("E5_Causal_Dual", {
                let c = content_owned.clone();
                async move { e5.embed_dual(&c).await }
            }),
            Self::timed_embed("E6_Sparse", {
                let c = content_owned.clone();
                async move { e6.embed_sparse(&c).await }
            }),
            Self::timed_embed("E7_Code", {
                let c = content_owned.clone();
                async move { e7.embed(&c).await }
            }),
            Self::timed_embed("E8_Graph_Dual", {
                let c = content_owned.clone();
                async move { e8.embed_dual(&c).await }
            }),
            Self::timed_embed("E9_HDC", {
                let c = content_owned.clone();
                async move { e9.embed(&c).await }
            }),
            Self::timed_embed("E10_Contextual_Dual", {
                let c = content_owned.clone();
                async move { e10.embed_dual(&c).await }
            }),
            Self::timed_embed("E11_Entity", {
                let c = content_owned.clone();
                async move { e11.embed(&c).await }
            }),
            Self::timed_embed("E12_LateInteraction", {
                let c = content_owned.clone();
                async move { e12.embed_tokens(&c).await }
            }),
            Self::timed_embed("E13_SPLADE", {
                let c = content_owned.clone();
                async move { e13.embed_sparse(&c).await }
            }),
        );

        // Collect results, failing fast on any error
        let e1_vec = r1?;
        let e2_vec = r2?;
        let e3_vec = r3?;
        let e4_vec = r4?;

        // E5: embed_dual returns (cause_vec, effect_vec) for asymmetric similarity (ARCH-15)
        let (e5_cause_vec, e5_effect_vec) = r5?;

        let e6_sparse = r6?;
        let e7_vec = r7?;

        // E8: embed_dual returns (source_vec, target_vec) for asymmetric similarity (E8 Upgrade)
        let (e8_source_vec, e8_target_vec) = r8?;

        let e9_vec = r9?;

        // E10: embed_dual returns (intent_vec, context_vec) for asymmetric similarity (E10 Upgrade)
        let (e10_intent_vec, e10_context_vec) = r10?;

        let e11_vec = r11?;
        let e12_tokens = r12?;
        let e13_sparse = r13?;

        let total_latency = start.elapsed();

        // Construct fingerprint with asymmetric E5, E8, and E10 vectors
        let fingerprint = SemanticFingerprint {
            e1_semantic: e1_vec,
            e2_temporal_recent: e2_vec,
            e3_temporal_periodic: e3_vec,
            e4_temporal_positional: e4_vec,
            e5_causal_as_cause: e5_cause_vec,
            e5_causal_as_effect: e5_effect_vec,
            e5_causal: Vec::new(), // Empty - using new dual format
            e6_sparse,
            e7_code: e7_vec,
            e8_graph_as_source: e8_source_vec,
            e8_graph_as_target: e8_target_vec,
            e8_graph: Vec::new(), // Empty - using new dual format
            e9_hdc: e9_vec,
            // E10: Using new dual format (E10 Upgrade)
            e10_multimodal_as_intent: e10_intent_vec,
            e10_multimodal_as_context: e10_context_vec,
            e10_multimodal: Vec::new(), // Empty - using new dual format
            e11_entity: e11_vec,
            e12_late_interaction: e12_tokens,
            e13_splade: e13_sparse,
        };

        let per_embedder_latency = [d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13];

        Ok(MultiArrayEmbeddingOutput {
            fingerprint,
            total_latency,
            per_embedder_latency,
            model_ids: self.model_ids.clone(),
        })
    }

    /// Generate complete 13-embedding fingerprint with explicit metadata.
    ///
    /// E4-FIX: This override passes session sequence numbers to E4 via
    /// "sequence:N" instruction, enabling proper session ordering.
    ///
    /// # Arguments
    ///
    /// * `content` - Text content to embed (must be non-empty)
    /// * `metadata` - Metadata for temporal embedders (E2-E4)
    ///
    /// # Behavior
    ///
    /// - E4: Uses `metadata.e4_instruction()` to pass "sequence:N" or "epoch:N"
    /// - E2/E3: Currently use default behavior (could be extended to use metadata.timestamp)
    /// - All other embedders: Unchanged
    async fn embed_all_with_metadata(
        &self,
        content: &str,
        metadata: EmbeddingMetadata,
    ) -> CoreResult<MultiArrayEmbeddingOutput> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }

        let start = Instant::now();

        // Clone Arc references for parallel execution
        let e1 = Arc::clone(&self.e1_semantic);
        let e2 = Arc::clone(&self.e2_temporal_recent);
        let e3 = Arc::clone(&self.e3_temporal_periodic);
        let e4 = Arc::clone(&self.e4_temporal_positional);
        let e5 = Arc::clone(&self.e5_causal);
        let e6 = Arc::clone(&self.e6_sparse);
        let e7 = Arc::clone(&self.e7_code);
        let e8 = Arc::clone(&self.e8_graph);
        let e9 = Arc::clone(&self.e9_hdc);
        let e10 = Arc::clone(&self.e10_contextual);
        let e11 = Arc::clone(&self.e11_entity);
        let e12 = Arc::clone(&self.e12_late_interaction);
        let e13 = Arc::clone(&self.e13_splade);

        let content_owned = content.to_string();

        // E4-FIX: Generate E4 instruction from metadata
        let e4_instruction = metadata.e4_instruction();

        // CAUSAL-HINT Phase 6: Clone causal hint for E5 embedding
        let causal_hint = metadata.causal_hint.clone();

        // Run all 13 embedders in parallel
        let (
            (r1, d1),
            (r2, d2),
            (r3, d3),
            (r4, d4),
            (r5, d5),
            (r6, d6),
            (r7, d7),
            (r8, d8),
            (r9, d9),
            (r10, d10),
            (r11, d11),
            (r12, d12),
            (r13, d13),
        ) = tokio::join!(
            Self::timed_embed("E1_Semantic", {
                let c = content_owned.clone();
                async move { e1.embed(&c).await }
            }),
            Self::timed_embed("E2_TemporalRecent", {
                let c = content_owned.clone();
                async move { e2.embed(&c).await }
            }),
            Self::timed_embed("E3_TemporalPeriodic", {
                let c = content_owned.clone();
                async move { e3.embed(&c).await }
            }),
            // E4-FIX: Use embed_with_instruction to pass sequence number
            Self::timed_embed("E4_TemporalPositional", {
                let c = content_owned.clone();
                let inst = e4_instruction.clone();
                async move { e4.embed_with_instruction(&c, Some(&inst)).await }
            }),
            // CAUSAL-HINT Phase 6: Use embed_dual_with_hint for E5 if hint is available
            Self::timed_embed("E5_Causal_Dual", {
                let c = content_owned.clone();
                let hint = causal_hint.clone();
                async move { e5.embed_dual_with_hint(&c, hint.as_ref()).await }
            }),
            Self::timed_embed("E6_Sparse", {
                let c = content_owned.clone();
                async move { e6.embed_sparse(&c).await }
            }),
            Self::timed_embed("E7_Code", {
                let c = content_owned.clone();
                async move { e7.embed(&c).await }
            }),
            Self::timed_embed("E8_Graph_Dual", {
                let c = content_owned.clone();
                async move { e8.embed_dual(&c).await }
            }),
            Self::timed_embed("E9_HDC", {
                let c = content_owned.clone();
                async move { e9.embed(&c).await }
            }),
            Self::timed_embed("E10_Contextual_Dual", {
                let c = content_owned.clone();
                async move { e10.embed_dual(&c).await }
            }),
            Self::timed_embed("E11_Entity", {
                let c = content_owned.clone();
                async move { e11.embed(&c).await }
            }),
            Self::timed_embed("E12_LateInteraction", {
                let c = content_owned.clone();
                async move { e12.embed_tokens(&c).await }
            }),
            Self::timed_embed("E13_SPLADE", {
                let c = content_owned.clone();
                async move { e13.embed_sparse(&c).await }
            }),
        );

        // Collect results, failing fast on any error
        let e1_vec = r1?;
        let e2_vec = r2?;
        let e3_vec = r3?;
        let e4_vec = r4?;

        // E5: embed_dual returns (cause_vec, effect_vec) for asymmetric similarity
        let (e5_cause_vec, e5_effect_vec) = r5?;

        let e6_sparse = r6?;
        let e7_vec = r7?;

        // E8: embed_dual returns (source_vec, target_vec) for asymmetric similarity (E8 Upgrade)
        let (e8_source_vec, e8_target_vec) = r8?;

        let e9_vec = r9?;

        // E10: embed_dual returns (intent_vec, context_vec) for asymmetric similarity (E10 Upgrade)
        let (e10_intent_vec, e10_context_vec) = r10?;

        let e11_vec = r11?;
        let e12_tokens = r12?;
        let e13_sparse = r13?;

        let total_latency = start.elapsed();

        // Construct fingerprint with asymmetric E5, E8, and E10 vectors
        let fingerprint = SemanticFingerprint {
            e1_semantic: e1_vec,
            e2_temporal_recent: e2_vec,
            e3_temporal_periodic: e3_vec,
            e4_temporal_positional: e4_vec,
            e5_causal_as_cause: e5_cause_vec,
            e5_causal_as_effect: e5_effect_vec,
            e5_causal: Vec::new(), // Empty - using new dual format
            e6_sparse,
            e7_code: e7_vec,
            e8_graph_as_source: e8_source_vec,
            e8_graph_as_target: e8_target_vec,
            e8_graph: Vec::new(), // Empty - using new dual format
            e9_hdc: e9_vec,
            // E10: Using new dual format (E10 Upgrade)
            e10_multimodal_as_intent: e10_intent_vec,
            e10_multimodal_as_context: e10_context_vec,
            e10_multimodal: Vec::new(), // Empty - using new dual format
            e11_entity: e11_vec,
            e12_late_interaction: e12_tokens,
            e13_splade: e13_sparse,
        };

        let per_embedder_latency = [d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13];

        Ok(MultiArrayEmbeddingOutput {
            fingerprint,
            total_latency,
            per_embedder_latency,
            model_ids: self.model_ids.clone(),
        })
    }

    /// Generate fingerprints for multiple contents in batch.
    ///
    /// Processes contents concurrently using tokio::spawn for GPU parallelism.
    /// All 13 embedders run in parallel for each content, and contents are
    /// processed concurrently across multiple GPU streams.
    ///
    /// # Performance Target
    ///
    /// 64 contents: <100ms per item average
    ///
    /// # GPU Optimization
    ///
    /// Uses concurrent tokio tasks to maximize GPU utilization. Each task
    /// runs embed_all() which itself runs all 13 embedders in parallel.
    async fn embed_batch_all(
        &self,
        contents: &[String],
    ) -> CoreResult<Vec<MultiArrayEmbeddingOutput>> {
        use futures::future::join_all;

        // Clone self for spawned tasks (Arc references are cheap to clone)
        let e1 = Arc::clone(&self.e1_semantic);
        let e2 = Arc::clone(&self.e2_temporal_recent);
        let e3 = Arc::clone(&self.e3_temporal_periodic);
        let e4 = Arc::clone(&self.e4_temporal_positional);
        let e5 = Arc::clone(&self.e5_causal);
        let e6 = Arc::clone(&self.e6_sparse);
        let e7 = Arc::clone(&self.e7_code);
        let e8 = Arc::clone(&self.e8_graph);
        let e9 = Arc::clone(&self.e9_hdc);
        let e10 = Arc::clone(&self.e10_contextual);
        let e11 = Arc::clone(&self.e11_entity);
        let e12 = Arc::clone(&self.e12_late_interaction);
        let e13 = Arc::clone(&self.e13_splade);
        let model_ids = self.model_ids.clone();

        // Spawn concurrent tasks for each content
        let tasks: Vec<_> = contents
            .iter()
            .map(|content| {
                let content = content.clone();
                let e1 = Arc::clone(&e1);
                let e2 = Arc::clone(&e2);
                let e3 = Arc::clone(&e3);
                let e4 = Arc::clone(&e4);
                let e5 = Arc::clone(&e5);
                let e6 = Arc::clone(&e6);
                let e7 = Arc::clone(&e7);
                let e8 = Arc::clone(&e8);
                let e9 = Arc::clone(&e9);
                let e10 = Arc::clone(&e10);
                let e11 = Arc::clone(&e11);
                let e12 = Arc::clone(&e12);
                let e13 = Arc::clone(&e13);
                let model_ids = model_ids.clone();

                tokio::spawn(async move {
                    let start = Instant::now();

                    // Run all 13 embedders in parallel for this content
                    let (
                        (r1, d1),
                        (r2, d2),
                        (r3, d3),
                        (r4, d4),
                        (r5, d5),
                        (r6, d6),
                        (r7, d7),
                        (r8, d8),
                        (r9, d9),
                        (r10, d10),
                        (r11, d11),
                        (r12, d12),
                        (r13, d13),
                    ) = tokio::join!(
                        Self::timed_embed("E1_Semantic", {
                            let c = content.clone();
                            async move { e1.embed(&c).await }
                        }),
                        Self::timed_embed("E2_TemporalRecent", {
                            let c = content.clone();
                            async move { e2.embed(&c).await }
                        }),
                        Self::timed_embed("E3_TemporalPeriodic", {
                            let c = content.clone();
                            async move { e3.embed(&c).await }
                        }),
                        Self::timed_embed("E4_TemporalPositional", {
                            let c = content.clone();
                            async move { e4.embed(&c).await }
                        }),
                        Self::timed_embed("E5_Causal_Dual", {
                            let c = content.clone();
                            async move { e5.embed_dual(&c).await }
                        }),
                        Self::timed_embed("E6_Sparse", {
                            let c = content.clone();
                            async move { e6.embed_sparse(&c).await }
                        }),
                        Self::timed_embed("E7_Code", {
                            let c = content.clone();
                            async move { e7.embed(&c).await }
                        }),
                        Self::timed_embed("E8_Graph_Dual", {
                            let c = content.clone();
                            async move { e8.embed_dual(&c).await }
                        }),
                        Self::timed_embed("E9_HDC", {
                            let c = content.clone();
                            async move { e9.embed(&c).await }
                        }),
                        Self::timed_embed("E10_Contextual_Dual", {
                            let c = content.clone();
                            async move { e10.embed_dual(&c).await }
                        }),
                        Self::timed_embed("E11_Entity", {
                            let c = content.clone();
                            async move { e11.embed(&c).await }
                        }),
                        Self::timed_embed("E12_LateInteraction", {
                            let c = content.clone();
                            async move { e12.embed_tokens(&c).await }
                        }),
                        Self::timed_embed("E13_SPLADE", {
                            let c = content.clone();
                            async move { e13.embed_sparse(&c).await }
                        }),
                    );

                    // Collect results
                    let e1_vec = r1?;
                    let e2_vec = r2?;
                    let e3_vec = r3?;
                    let e4_vec = r4?;
                    let (e5_cause_vec, e5_effect_vec) = r5?;
                    let e6_sparse = r6?;
                    let e7_vec = r7?;
                    let (e8_source_vec, e8_target_vec) = r8?;
                    let e9_vec = r9?;
                    let (e10_intent_vec, e10_context_vec) = r10?;
                    let e11_vec = r11?;
                    let e12_tokens = r12?;
                    let e13_sparse = r13?;

                    let total_latency = start.elapsed();

                    let fingerprint = SemanticFingerprint {
                        e1_semantic: e1_vec,
                        e2_temporal_recent: e2_vec,
                        e3_temporal_periodic: e3_vec,
                        e4_temporal_positional: e4_vec,
                        e5_causal_as_cause: e5_cause_vec,
                        e5_causal_as_effect: e5_effect_vec,
                        e5_causal: Vec::new(),
                        e6_sparse,
                        e7_code: e7_vec,
                        e8_graph_as_source: e8_source_vec,
                        e8_graph_as_target: e8_target_vec,
                        e8_graph: Vec::new(),
                        e9_hdc: e9_vec,
                        e10_multimodal_as_intent: e10_intent_vec,
                        e10_multimodal_as_context: e10_context_vec,
                        e10_multimodal: Vec::new(),
                        e11_entity: e11_vec,
                        e12_late_interaction: e12_tokens,
                        e13_splade: e13_sparse,
                    };

                    let per_embedder_latency =
                        [d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13];

                    Ok::<_, CoreError>(MultiArrayEmbeddingOutput {
                        fingerprint,
                        total_latency,
                        per_embedder_latency,
                        model_ids,
                    })
                })
            })
            .collect();

        // Wait for all tasks to complete
        let task_results = join_all(tasks).await;

        // Collect results, propagating any errors
        let mut results = Vec::with_capacity(contents.len());
        for result in task_results {
            match result {
                Ok(Ok(output)) => results.push(output),
                Ok(Err(e)) => return Err(e),
                Err(e) => {
                    return Err(CoreError::Internal(format!(
                        "Batch embedding task failed: {}",
                        e
                    )))
                }
            }
        }

        Ok(results)
    }

    /// Get model IDs for each embedder slot.
    fn model_ids(&self) -> [&str; NUM_EMBEDDERS] {
        [
            &self.model_ids[0],
            &self.model_ids[1],
            &self.model_ids[2],
            &self.model_ids[3],
            &self.model_ids[4],
            &self.model_ids[5],
            &self.model_ids[6],
            &self.model_ids[7],
            &self.model_ids[8],
            &self.model_ids[9],
            &self.model_ids[10],
            &self.model_ids[11],
            &self.model_ids[12],
        ]
    }

    /// Check if all 13 embedders are initialized and ready.
    fn is_ready(&self) -> bool {
        self.e1_semantic.is_ready()
            && self.e2_temporal_recent.is_ready()
            && self.e3_temporal_periodic.is_ready()
            && self.e4_temporal_positional.is_ready()
            && self.e5_causal.is_ready()
            && self.e6_sparse.is_ready()
            && self.e7_code.is_ready()
            && self.e8_graph.is_ready()
            && self.e9_hdc.is_ready()
            && self.e10_contextual.is_ready()
            && self.e11_entity.is_ready()
            && self.e12_late_interaction.is_ready()
            && self.e13_splade.is_ready()
    }

    /// Get health status for each embedder.
    fn health_status(&self) -> [bool; NUM_EMBEDDERS] {
        [
            self.e1_semantic.is_ready(),
            self.e2_temporal_recent.is_ready(),
            self.e3_temporal_periodic.is_ready(),
            self.e4_temporal_positional.is_ready(),
            self.e5_causal.is_ready(),
            self.e6_sparse.is_ready(),
            self.e7_code.is_ready(),
            self.e8_graph.is_ready(),
            self.e9_hdc.is_ready(),
            self.e10_contextual.is_ready(),
            self.e11_entity.is_ready(),
            self.e12_late_interaction.is_ready(),
            self.e13_splade.is_ready(),
        ]
    }

    /// Efficient E8 dual embedding without running all 13 embedders.
    ///
    /// Returns (as_source, as_target) E8 dual embeddings (1024D each).
    async fn embed_e8_dual(&self, content: &str) -> CoreResult<(Vec<f32>, Vec<f32>)> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }
        self.e8_graph.embed_dual(content).await
    }

    /// Efficient E11 embedding without running all 13 embedders.
    ///
    /// Returns E11 entity embedding (768D).
    async fn embed_e11_only(&self, content: &str) -> CoreResult<Vec<f32>> {
        if content.is_empty() {
            return Err(CoreError::ValidationError {
                field: "content".to_string(),
                message: "Content cannot be empty".to_string(),
            });
        }
        self.e11_entity.embed(content).await
    }
}

// Safety: ProductionMultiArrayProvider is Send + Sync because all fields are Arc<dyn Trait + Send + Sync>
unsafe impl Send for ProductionMultiArrayProvider {}
unsafe impl Sync for ProductionMultiArrayProvider {}

#[cfg(test)]
mod tests {
    use super::*;

    /// Test that provider requires non-empty content.
    #[tokio::test]
    async fn test_empty_content_rejected() {
        // Note: This test requires model files to be present
        // In CI, this would be skipped or use mock models
        // For now, we just verify the validation logic exists
        let content = "";
        assert!(content.is_empty());
    }

    /// Test NUM_EMBEDDERS is 13.
    #[test]
    fn test_num_embedders() {
        assert_eq!(NUM_EMBEDDERS, 13);
    }

    /// Test model_ids array has correct length.
    #[test]
    fn test_model_ids_length() {
        let ids = [
            "semantic",
            "temporal_recent",
            "temporal_periodic",
            "temporal_positional",
            "causal",
            "sparse",
            "code",
            "graph",
            "hdc",
            "multimodal",
            "entity",
            "late_interaction",
            "splade",
        ];
        assert_eq!(ids.len(), NUM_EMBEDDERS);
    }

    // =========================================================================
    // E4 INSTRUCTION FIX VERIFICATION TESTS
    // =========================================================================

    /// Test that EmbeddingMetadata.e4_instruction() includes session_id.
    ///
    /// This is the critical end-to-end verification test for the E4 session fix.
    /// It verifies that when embed_all_with_metadata() is called, the session_id
    /// is correctly passed through to the E4 embedder.
    #[test]
    fn test_embedding_metadata_e4_instruction_includes_session() {
        // This test verifies the fix at the metadata level
        let metadata = EmbeddingMetadata::with_sequence("test-session-id", 100);

        let instruction = metadata.e4_instruction();

        // Critical assertion: session_id must be in the instruction
        assert!(
            instruction.contains("session:test-session-id"),
            "e4_instruction() must include session_id. Got: {}",
            instruction
        );
        assert!(
            instruction.contains("sequence:100"),
            "e4_instruction() must include sequence. Got: {}",
            instruction
        );

        // Verify exact format matches what E4 parser expects
        assert_eq!(
            instruction, "session:test-session-id sequence:100",
            "Instruction format should match E4 parser expectations"
        );
    }

    /// Test that different session_ids produce different instruction strings.
    #[test]
    fn test_different_sessions_produce_different_instructions() {
        let metadata1 = EmbeddingMetadata::with_sequence("session-A", 1);
        let metadata2 = EmbeddingMetadata::with_sequence("session-B", 1);

        let inst1 = metadata1.e4_instruction();
        let inst2 = metadata2.e4_instruction();

        assert_ne!(
            inst1, inst2,
            "Different sessions should produce different instructions"
        );
        assert!(inst1.contains("session:session-A"));
        assert!(inst2.contains("session:session-B"));
    }

    /// Test backward compatibility: metadata without session_id still works.
    #[test]
    fn test_backward_compatible_no_session_metadata() {
        let metadata = EmbeddingMetadata {
            session_id: None,
            session_sequence: Some(42),
            timestamp: None,
            causal_hint: None,
        };

        let instruction = metadata.e4_instruction();

        // Should produce legacy format without session prefix
        assert_eq!(
            instruction, "sequence:42",
            "Legacy format should work without session_id"
        );
        assert!(
            !instruction.contains("session:"),
            "No session prefix when session_id is None"
        );
    }
}
