# M05-T63: Implement Synthetic Data Seeding Support

## Task Metadata
- **ID**: M05-T63
- **Title**: Implement Synthetic Data Seeding Support
- **Module**: 05 - UTL Integration
- **Layer**: surface
- **Priority**: medium
- **Estimated Hours**: 3
- **Created**: 2026-01-04
- **Status**: pending

## Description

Implement synthetic data seeding support for bootstrapping empty graphs per PRD Section 5.

Per PRD:
- "Synthetic Data Seeding: Bootstrap empty graphs with project documentation"
- `bin/reasoning seed --source ./README.md --target-nodes 200`

This enables:
1. Bootstrapping new Context Graph instances
2. Pre-populating with project documentation
3. Creating initial graph structure for new agents
4. Setting lifecycle stage to appropriate initial state

## File Paths

### Implementation
- `crates/context-graph-utl/src/seeding/generator.rs`
- `crates/context-graph-utl/src/seeding/mod.rs`
- `crates/context-graph-utl/src/seeding/parser.rs`

### Binary
- `crates/context-graph-mcp/src/bin/reasoning.rs` (seed subcommand)

### Tests
- `crates/context-graph-utl/tests/seeding_tests.rs`

## Dependencies

| Task ID | Title | Status |
|---------|-------|--------|
| M05-T22 | Implement UtlProcessor | pending |
| M05-T19 | Implement LifecycleManager | pending |
| M05-T21 | Implement LearningSignal and UtlState | pending |

## Acceptance Criteria

- [ ] SeedingConfig struct with source/target options
- [ ] Document parser for markdown/text files
- [ ] Node generator from document chunks
- [ ] UTL state initialization for seeded nodes
- [ ] CLI subcommand `reasoning seed`
- [ ] Target node count configurable

## Specification References

- `contextgraphprd.md` Section 5 System Lifecycle
- `contextgraphprd.md` Section 12 Implementation Roadmap

## Implementation Notes

### Seeding Configuration

```rust
use serde::{Serialize, Deserialize};
use std::path::PathBuf;

/// Configuration for synthetic data seeding
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SeedingConfig {
    /// Source file(s) to seed from
    pub source: PathBuf,

    /// Target number of nodes to generate
    pub target_nodes: usize,

    /// Chunk size for splitting documents
    pub chunk_size: usize,

    /// Chunk overlap for context preservation
    pub chunk_overlap: usize,

    /// Whether to generate edges between related chunks
    pub generate_edges: bool,

    /// Initial lifecycle stage for seeded graph
    pub initial_stage: LifecycleStage,

    /// Domain to assign to seeded nodes
    pub domain: Option<String>,

    /// Agent ID for seeded nodes
    pub agent_id: String,

    /// Seed for reproducibility
    pub seed: Option<u64>,
}

impl Default for SeedingConfig {
    fn default() -> Self {
        Self {
            source: PathBuf::from("README.md"),
            target_nodes: 200,
            chunk_size: 512,      // tokens
            chunk_overlap: 64,    // tokens
            generate_edges: true,
            initial_stage: LifecycleStage::Infancy,
            domain: None,
            agent_id: "seeder".to_string(),
            seed: None,
        }
    }
}
```

### Document Parser

```rust
/// Parses documents into seedable chunks
pub struct DocumentParser {
    config: SeedingConfig,
}

/// A parsed document ready for seeding
#[derive(Debug, Clone)]
pub struct ParsedDocument {
    /// Document title/name
    pub title: String,

    /// Source file path
    pub source_path: PathBuf,

    /// Parsed sections
    pub sections: Vec<DocumentSection>,

    /// Total token count
    pub total_tokens: usize,
}

/// A section of a parsed document
#[derive(Debug, Clone)]
pub struct DocumentSection {
    /// Section heading (if any)
    pub heading: Option<String>,

    /// Heading level (1-6 for markdown)
    pub level: u8,

    /// Section content
    pub content: String,

    /// Token count
    pub token_count: usize,

    /// Parent section index (for hierarchy)
    pub parent_index: Option<usize>,
}

impl DocumentParser {
    pub fn new(config: SeedingConfig) -> Self {
        Self { config }
    }

    /// Parse a file into sections
    pub fn parse(&self, path: &Path) -> Result<ParsedDocument, SeedingError> {
        let content = std::fs::read_to_string(path)?;
        let extension = path.extension().and_then(|s| s.to_str()).unwrap_or("");

        match extension {
            "md" | "markdown" => self.parse_markdown(&content, path),
            "txt" => self.parse_plaintext(&content, path),
            "rs" | "py" | "js" | "ts" => self.parse_code(&content, path),
            _ => self.parse_plaintext(&content, path),
        }
    }

    /// Parse markdown document
    fn parse_markdown(
        &self,
        content: &str,
        path: &Path
    ) -> Result<ParsedDocument, SeedingError> {
        let mut sections = Vec::new();
        let mut current_content = String::new();
        let mut current_heading: Option<String> = None;
        let mut current_level = 0u8;
        let mut heading_stack: Vec<usize> = Vec::new();

        for line in content.lines() {
            if line.starts_with('#') {
                // Save previous section
                if !current_content.is_empty() {
                    let parent_index = heading_stack.last().copied();
                    sections.push(DocumentSection {
                        heading: current_heading.take(),
                        level: current_level,
                        content: current_content.trim().to_string(),
                        token_count: self.count_tokens(&current_content),
                        parent_index,
                    });
                }

                // Parse new heading
                let level = line.chars().take_while(|c| *c == '#').count() as u8;
                let heading = line.trim_start_matches('#').trim().to_string();

                // Update heading stack
                while heading_stack.len() > 0
                    && sections.get(*heading_stack.last().unwrap())
                        .map(|s| s.level >= level)
                        .unwrap_or(false)
                {
                    heading_stack.pop();
                }
                heading_stack.push(sections.len());

                current_heading = Some(heading);
                current_level = level;
                current_content.clear();
            } else {
                current_content.push_str(line);
                current_content.push('\n');
            }
        }

        // Save final section
        if !current_content.is_empty() {
            let parent_index = heading_stack.last().copied();
            sections.push(DocumentSection {
                heading: current_heading,
                level: current_level,
                content: current_content.trim().to_string(),
                token_count: self.count_tokens(&current_content),
                parent_index,
            });
        }

        let total_tokens = sections.iter().map(|s| s.token_count).sum();

        Ok(ParsedDocument {
            title: path.file_stem()
                .and_then(|s| s.to_str())
                .unwrap_or("Untitled")
                .to_string(),
            source_path: path.to_path_buf(),
            sections,
            total_tokens,
        })
    }

    fn parse_plaintext(
        &self,
        content: &str,
        path: &Path,
    ) -> Result<ParsedDocument, SeedingError> {
        // Split by paragraphs
        let paragraphs: Vec<&str> = content.split("\n\n")
            .filter(|p| !p.trim().is_empty())
            .collect();

        let sections = paragraphs.iter().enumerate()
            .map(|(i, p)| DocumentSection {
                heading: None,
                level: 0,
                content: p.trim().to_string(),
                token_count: self.count_tokens(p),
                parent_index: if i > 0 { Some(i - 1) } else { None },
            })
            .collect();

        let total_tokens = sections.iter().map(|s| s.token_count).sum();

        Ok(ParsedDocument {
            title: path.file_stem()
                .and_then(|s| s.to_str())
                .unwrap_or("Untitled")
                .to_string(),
            source_path: path.to_path_buf(),
            sections,
            total_tokens,
        })
    }

    fn parse_code(
        &self,
        content: &str,
        path: &Path,
    ) -> Result<ParsedDocument, SeedingError> {
        // Simple code splitting by function/class definitions
        // This is a basic implementation - could use tree-sitter for better parsing
        let mut sections = Vec::new();
        let mut current_block = String::new();

        for line in content.lines() {
            // Detect function/class boundaries (simplified)
            let is_boundary = line.starts_with("fn ")
                || line.starts_with("pub fn ")
                || line.starts_with("class ")
                || line.starts_with("def ")
                || line.starts_with("function ")
                || line.starts_with("impl ");

            if is_boundary && !current_block.is_empty() {
                sections.push(DocumentSection {
                    heading: Some(self.extract_definition_name(&current_block)),
                    level: 1,
                    content: current_block.trim().to_string(),
                    token_count: self.count_tokens(&current_block),
                    parent_index: None,
                });
                current_block.clear();
            }

            current_block.push_str(line);
            current_block.push('\n');
        }

        // Save final block
        if !current_block.is_empty() {
            sections.push(DocumentSection {
                heading: Some(self.extract_definition_name(&current_block)),
                level: 1,
                content: current_block.trim().to_string(),
                token_count: self.count_tokens(&current_block),
                parent_index: None,
            });
        }

        let total_tokens = sections.iter().map(|s| s.token_count).sum();

        Ok(ParsedDocument {
            title: path.file_stem()
                .and_then(|s| s.to_str())
                .unwrap_or("Untitled")
                .to_string(),
            source_path: path.to_path_buf(),
            sections,
            total_tokens,
        })
    }

    fn count_tokens(&self, text: &str) -> usize {
        // Simplified token counting (whitespace-based)
        // In production, use a proper tokenizer
        text.split_whitespace().count()
    }

    fn extract_definition_name(&self, block: &str) -> String {
        block.lines()
            .next()
            .map(|l| l.trim().to_string())
            .unwrap_or_else(|| "Unknown".to_string())
    }
}
```

### Node Generator

```rust
/// Generates memory nodes from parsed documents
pub struct SeedNodeGenerator {
    config: SeedingConfig,
    utl_processor: Arc<UtlProcessor>,
}

/// Result of seeding operation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SeedingResult {
    /// Number of nodes created
    pub nodes_created: usize,

    /// Number of edges created
    pub edges_created: usize,

    /// Initial UTL state
    pub initial_utl_state: UtlState,

    /// Lifecycle stage set
    pub lifecycle_stage: LifecycleStage,

    /// Source files processed
    pub source_files: Vec<String>,

    /// Warnings during seeding
    pub warnings: Vec<String>,
}

impl SeedNodeGenerator {
    pub fn new(config: SeedingConfig, utl_processor: Arc<UtlProcessor>) -> Self {
        Self { config, utl_processor }
    }

    /// Generate nodes from parsed document
    pub async fn generate(
        &self,
        document: &ParsedDocument,
    ) -> Result<Vec<SeedNode>, SeedingError> {
        let mut nodes = Vec::new();
        let target_per_section = self.config.target_nodes / document.sections.len().max(1);

        for (idx, section) in document.sections.iter().enumerate() {
            let section_nodes = self.generate_section_nodes(section, target_per_section)?;

            for node in section_nodes {
                nodes.push(node);
            }
        }

        // Trim or pad to target
        if nodes.len() > self.config.target_nodes {
            nodes.truncate(self.config.target_nodes);
        }

        Ok(nodes)
    }

    fn generate_section_nodes(
        &self,
        section: &DocumentSection,
        target_count: usize,
    ) -> Result<Vec<SeedNode>, SeedingError> {
        let chunks = self.chunk_content(&section.content);

        chunks.iter().take(target_count)
            .map(|chunk| {
                Ok(SeedNode {
                    id: Uuid::new_v4(),
                    content: chunk.clone(),
                    heading: section.heading.clone(),
                    source_path: self.config.source.to_string_lossy().to_string(),
                    agent_id: self.config.agent_id.clone(),
                    domain: self.config.domain.clone(),
                    initial_salience: 0.5,  // Neutral initial salience
                    initial_johari: JohariQuadrant::Unknown,  // New knowledge
                })
            })
            .collect()
    }

    fn chunk_content(&self, content: &str) -> Vec<String> {
        let words: Vec<&str> = content.split_whitespace().collect();
        let mut chunks = Vec::new();
        let mut start = 0;

        while start < words.len() {
            let end = (start + self.config.chunk_size).min(words.len());
            let chunk = words[start..end].join(" ");
            chunks.push(chunk);

            start = end.saturating_sub(self.config.chunk_overlap);
            if start >= words.len() || end == words.len() {
                break;
            }
        }

        chunks
    }

    /// Generate edges between related nodes
    pub fn generate_edges(&self, nodes: &[SeedNode]) -> Vec<SeedEdge> {
        if !self.config.generate_edges {
            return Vec::new();
        }

        let mut edges = Vec::new();

        // Sequential edges (document order)
        for i in 0..nodes.len().saturating_sub(1) {
            edges.push(SeedEdge {
                source: nodes[i].id,
                target: nodes[i + 1].id,
                edge_type: EdgeType::Temporal,
                weight: 0.8,
            });
        }

        // Hierarchical edges (same heading)
        let mut heading_groups: HashMap<Option<String>, Vec<Uuid>> = HashMap::new();
        for node in nodes {
            heading_groups.entry(node.heading.clone())
                .or_default()
                .push(node.id);
        }

        for (_, group_ids) in heading_groups {
            if group_ids.len() > 1 {
                for i in 0..group_ids.len() {
                    for j in (i + 1)..group_ids.len() {
                        edges.push(SeedEdge {
                            source: group_ids[i],
                            target: group_ids[j],
                            edge_type: EdgeType::Hierarchical,
                            weight: 0.6,
                        });
                    }
                }
            }
        }

        edges
    }
}

/// A node to be seeded
#[derive(Debug, Clone)]
pub struct SeedNode {
    pub id: Uuid,
    pub content: String,
    pub heading: Option<String>,
    pub source_path: String,
    pub agent_id: String,
    pub domain: Option<String>,
    pub initial_salience: f32,
    pub initial_johari: JohariQuadrant,
}

/// An edge to be seeded
#[derive(Debug, Clone)]
pub struct SeedEdge {
    pub source: Uuid,
    pub target: Uuid,
    pub edge_type: EdgeType,
    pub weight: f32,
}
```

### CLI Subcommand

```rust
// crates/context-graph-mcp/src/bin/reasoning.rs

use clap::{Parser, Subcommand};
use context_graph_utl::seeding::{SeedingConfig, DocumentParser, SeedNodeGenerator};

#[derive(Parser)]
#[command(name = "reasoning")]
#[command(about = "Context Graph reasoning tools")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Seed graph with synthetic data from documents
    Seed {
        /// Source file or directory
        #[arg(short, long)]
        source: PathBuf,

        /// Target number of nodes
        #[arg(short, long, default_value = "200")]
        target_nodes: usize,

        /// Chunk size in tokens
        #[arg(long, default_value = "512")]
        chunk_size: usize,

        /// Domain for seeded nodes
        #[arg(long)]
        domain: Option<String>,

        /// Dry run (don't actually create nodes)
        #[arg(long)]
        dry_run: bool,
    },
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let cli = Cli::parse();

    match cli.command {
        Commands::Seed {
            source,
            target_nodes,
            chunk_size,
            domain,
            dry_run,
        } => {
            let config = SeedingConfig {
                source,
                target_nodes,
                chunk_size,
                domain,
                ..Default::default()
            };

            let parser = DocumentParser::new(config.clone());
            let document = parser.parse(&config.source)?;

            println!("Parsed document: {}", document.title);
            println!("Sections: {}", document.sections.len());
            println!("Total tokens: {}", document.total_tokens);

            if dry_run {
                println!("\n[Dry run] Would create {} nodes", target_nodes);
                return Ok(());
            }

            let utl_processor = Arc::new(UtlProcessor::new(UtlConfig::default())?);
            let generator = SeedNodeGenerator::new(config, utl_processor);

            let nodes = generator.generate(&document).await?;
            let edges = generator.generate_edges(&nodes);

            println!("\nGenerated {} nodes", nodes.len());
            println!("Generated {} edges", edges.len());

            // TODO: Actually store nodes and edges in graph
            println!("\nSeeding complete!");

            Ok(())
        }
    }
}
```

## Testing Requirements

1. **Parser Tests**
   - Parse markdown document correctly
   - Parse plaintext document
   - Parse code files
   - Extract headings and hierarchy

2. **Generator Tests**
   - Generate target number of nodes
   - Chunk content correctly
   - Overlap preserved
   - Edges generated for related nodes

3. **CLI Tests**
   - `reasoning seed --source README.md --target-nodes 100`
   - `reasoning seed --dry-run` shows preview
   - Invalid source path handled

4. **Integration Tests**
   - Full seeding flow works
   - Lifecycle set to Infancy after seeding
   - UTL state initialized correctly

---

*Task specification generated for Module 05 - UTL Integration*
