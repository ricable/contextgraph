# TASK-M02-017: Implement Node CRUD Operations

```xml
<task_spec id="TASK-M02-017" version="1.0">
<metadata>
  <title>Implement Node CRUD Operations</title>
  <status>blocked</status>
  <layer>logic</layer>
  <module>module-02</module>
  <sequence>17</sequence>
  <priority>critical</priority>
  <estimated_hours>4</estimated_hours>
  <implements>
    <item>REQ-CORE-006: CRUD requirements</item>
    <item>TECH-CORE-002 Section 3.2: RocksDB backend specification</item>
  </implements>
  <depends_on>
    <task_ref>TASK-M02-014</task_ref>
    <task_ref>TASK-M02-016</task_ref>
  </depends_on>
  <estimated_complexity>high</estimated_complexity>
</metadata>

<context>
This task implements the core CRUD (Create, Read, Update, Delete) operations for MemoryNode storage in the RocksDB backend. These operations are the foundation of all memory storage functionality and must maintain consistency across multiple column families (nodes, embeddings, Johari indexes, temporal indexes, tag indexes, source indexes) using atomic WriteBatch operations.
</context>

<input_context_files>
  <file purpose="RocksDB backend structure">crates/context-graph-storage/src/rocksdb_backend.rs</file>
  <file purpose="Serialization functions">crates/context-graph-storage/src/serialization.rs</file>
  <file purpose="Column family definitions">crates/context-graph-storage/src/column_families.rs</file>
  <file purpose="MemoryNode struct definition">crates/context-graph-core/src/memory_node.rs</file>
  <file purpose="Constitution for performance targets">docs2/constitution.yaml</file>
</input_context_files>

<prerequisites>
  <check>TASK-M02-014 (Bincode Serialization) completed</check>
  <check>TASK-M02-016 (RocksDB Backend Open/Close) completed</check>
  <check>All column families are defined and accessible</check>
  <check>Serialization functions work for MemoryNode</check>
</prerequisites>

<scope>
  <in_scope>
    - store_node() method - atomic write to nodes, embeddings, Johari, temporal, tags, sources CFs
    - get_node() method - retrieve and deserialize MemoryNode by ID
    - update_node() method - handle index updates when quadrant/tags change
    - delete_node() method - remove from all indexes (soft delete support)
    - WriteBatch usage for atomic multi-CF operations
    - Index maintenance for all secondary indexes
    - Async-safe implementation with RwLock
    - Performance target: <1ms store, <500μs get
  </in_scope>
  <out_of_scope>
    - Edge CRUD operations (TASK-M02-018)
    - Secondary index query methods (TASK-M02-023)
    - Embedding-specific methods (TASK-M02-024)
    - Memex trait abstraction (TASK-M02-026)
  </out_of_scope>
</scope>

<definition_of_done>
  <signatures>
    <signature file="crates/context-graph-storage/src/rocksdb_backend.rs">
impl RocksDbMemex {
    /// Stores a MemoryNode atomically across all relevant column families.
    ///
    /// Writes to: nodes, embeddings, johari_{quadrant}, temporal, tags (per tag), sources
    /// Uses WriteBatch for atomicity.
    ///
    /// # Performance
    /// Target: &lt;1ms p95 latency
    ///
    /// # Errors
    /// Returns StorageError on write failure or serialization error
    pub async fn store_node(&amp;self, node: &amp;MemoryNode) -> Result&lt;(), StorageError&gt;;

    /// Retrieves a MemoryNode by its ID.
    ///
    /// # Performance
    /// Target: &lt;500μs p95 latency
    ///
    /// # Errors
    /// Returns StorageError::NotFound if node doesn't exist
    pub async fn get_node(&amp;self, id: &amp;NodeId) -> Result&lt;MemoryNode, StorageError&gt;;

    /// Updates an existing MemoryNode, maintaining index consistency.
    ///
    /// Handles index updates when quadrant or tags change.
    ///
    /// # Errors
    /// Returns StorageError::NotFound if node doesn't exist
    pub async fn update_node(&amp;self, node: &amp;MemoryNode) -> Result&lt;(), StorageError&gt;;

    /// Deletes a MemoryNode and removes it from all indexes.
    ///
    /// # Arguments
    /// * `id` - Node ID to delete
    /// * `soft_delete` - If true, marks as deleted; if false, permanently removes
    ///
    /// # Errors
    /// Returns StorageError::NotFound if node doesn't exist
    pub async fn delete_node(&amp;self, id: &amp;NodeId, soft_delete: bool) -> Result&lt;(), StorageError&gt;;
}
    </signature>
  </signatures>

  <constraints>
    - All writes must use WriteBatch for atomicity
    - Index updates must be consistent (old entries removed, new added)
    - Soft delete must preserve data for 30-day recovery
    - Must not hold locks during I/O operations
    - UUID serialization must be consistent (bytes or string)
    - Performance: store &lt;1ms, get &lt;500μs at p95
    - Follow constitution.yaml coding standards
  </constraints>

  <verification>
    - cargo build --package context-graph-storage compiles without errors
    - cargo test --package context-graph-storage crud passes all tests
    - Store/get round-trip preserves all MemoryNode fields
    - Update correctly maintains index consistency
    - Delete removes from all column families
    - Soft delete marks but preserves data
    - Concurrent store/get operations work correctly
  </verification>
</definition_of_done>

<pseudo_code>
rocksdb_backend.rs (additions):

impl RocksDbMemex:
  async fn store_node(node: &amp;MemoryNode):
    // Acquire read lock for CF handles
    let inner = self.inner.read().await

    // Create WriteBatch for atomic operation
    let mut batch = WriteBatch::default()

    // 1. Serialize and write to nodes CF
    let node_key = node.id.as_bytes()
    let node_value = serialize_node(node)?
    batch.put_cf(cf_nodes, node_key, node_value)

    // 2. Write embedding to embeddings CF
    let embedding_value = serialize_embedding(&amp;node.embedding)?
    batch.put_cf(cf_embeddings, node_key, embedding_value)

    // 3. Add to Johari quadrant index
    let johari_cf = get_johari_cf(node.quadrant)
    batch.put_cf(johari_cf, node_key, &amp;[])  // Value is empty, key is index

    // 4. Add to temporal index (timestamp:id format)
    let temporal_key = format_temporal_key(node.created_at, node.id)
    batch.put_cf(cf_temporal, temporal_key, &amp;[])

    // 5. Add to tag indexes (one entry per tag)
    for tag in &amp;node.metadata.tags:
      let tag_key = format_tag_key(tag, node.id)
      batch.put_cf(cf_tags, tag_key, &amp;[])

    // 6. Add to sources index if source present
    if let Some(source) = &amp;node.metadata.source:
      let source_key = format_source_key(source, node.id)
      batch.put_cf(cf_sources, source_key, &amp;[])

    // Execute atomic batch write
    inner.db.write(batch)?
    Ok(())

  async fn get_node(id: &amp;NodeId):
    let inner = self.inner.read().await

    // Read from nodes CF
    let node_key = id.as_bytes()
    let node_bytes = inner.db.get_cf(cf_nodes, node_key)?
      .ok_or(StorageError::NotFound)?

    // Deserialize and return
    deserialize_node(&amp;node_bytes)

  async fn update_node(node: &amp;MemoryNode):
    // First, get the old node to compare indexes
    let old_node = self.get_node(&amp;node.id).await?

    let inner = self.inner.read().await
    let mut batch = WriteBatch::default()

    // Update node data
    let node_key = node.id.as_bytes()
    batch.put_cf(cf_nodes, node_key, serialize_node(node)?)
    batch.put_cf(cf_embeddings, node_key, serialize_embedding(&amp;node.embedding)?)

    // Handle Johari quadrant change
    if old_node.quadrant != node.quadrant:
      batch.delete_cf(get_johari_cf(old_node.quadrant), node_key)
      batch.put_cf(get_johari_cf(node.quadrant), node_key, &amp;[])

    // Handle tag changes
    let old_tags: HashSet = old_node.metadata.tags.into_iter().collect()
    let new_tags: HashSet = node.metadata.tags.iter().cloned().collect()

    for removed_tag in old_tags.difference(&amp;new_tags):
      batch.delete_cf(cf_tags, format_tag_key(removed_tag, node.id))
    for added_tag in new_tags.difference(&amp;old_tags):
      batch.put_cf(cf_tags, format_tag_key(added_tag, node.id), &amp;[])

    inner.db.write(batch)?
    Ok(())

  async fn delete_node(id: &amp;NodeId, soft_delete: bool):
    let node = self.get_node(id).await?
    let inner = self.inner.read().await
    let mut batch = WriteBatch::default()
    let node_key = id.as_bytes()

    if soft_delete:
      // Mark as deleted in node metadata, keep data
      let mut updated_node = node.clone()
      updated_node.metadata.mark_deleted()
      batch.put_cf(cf_nodes, node_key, serialize_node(&amp;updated_node)?)
    else:
      // Remove from all column families
      batch.delete_cf(cf_nodes, node_key)
      batch.delete_cf(cf_embeddings, node_key)
      batch.delete_cf(get_johari_cf(node.quadrant), node_key)
      batch.delete_cf(cf_temporal, format_temporal_key(node.created_at, id))

      for tag in &amp;node.metadata.tags:
        batch.delete_cf(cf_tags, format_tag_key(tag, id))

      if let Some(source) = &amp;node.metadata.source:
        batch.delete_cf(cf_sources, format_source_key(source, id))

    inner.db.write(batch)?
    Ok(())

// Helper functions
fn format_temporal_key(timestamp: DateTime&lt;Utc&gt;, id: NodeId) -> Vec&lt;u8&gt;
fn format_tag_key(tag: &amp;str, id: NodeId) -> Vec&lt;u8&gt;
fn format_source_key(source: &amp;str, id: NodeId) -> Vec&lt;u8&gt;
fn get_johari_cf(quadrant: JohariQuadrant) -> &amp;ColumnFamily

#[cfg(test)]
mod tests:
  test_store_and_get_node()
  test_update_node_quadrant_change()
  test_update_node_tags_change()
  test_delete_node_soft()
  test_delete_node_hard()
  test_store_latency_under_1ms()
  test_get_latency_under_500us()
  test_concurrent_operations()
</pseudo_code>

<files_to_create>
  <!-- No new files - additions to existing -->
</files_to_create>

<files_to_modify>
  <file path="crates/context-graph-storage/src/rocksdb_backend.rs">Add store_node, get_node, update_node, delete_node methods</file>
</files_to_modify>

<validation_criteria>
  <criterion>store_node() writes to nodes, embeddings, johari, temporal, tags, sources CFs atomically</criterion>
  <criterion>get_node() retrieves and deserializes MemoryNode correctly</criterion>
  <criterion>update_node() handles index updates when quadrant/tags change</criterion>
  <criterion>delete_node() removes from all indexes (hard delete)</criterion>
  <criterion>delete_node() marks as deleted but preserves data (soft delete)</criterion>
  <criterion>Latency target: &lt;1ms for store, &lt;500μs for get at p95</criterion>
  <criterion>All operations are async-safe with RwLock</criterion>
  <criterion>WriteBatch ensures atomicity across column families</criterion>
</validation_criteria>

<test_commands>
  <command>cargo build --package context-graph-storage</command>
  <command>cargo test --package context-graph-storage crud -- --nocapture</command>
  <command>cargo test --package context-graph-storage node -- --nocapture</command>
  <command>cargo clippy --package context-graph-storage -- -D warnings</command>
</test_commands>
</task_spec>
```

## Implementation Notes

### WriteBatch Atomicity
RocksDB WriteBatch ensures all-or-nothing semantics. If any write fails, the entire batch is rolled back.

### Index Key Formats
- **Temporal**: `{timestamp_millis}:{uuid_bytes}` - enables range scans by time
- **Tag**: `{tag_string}:{uuid_bytes}` - enables prefix scans by tag
- **Source**: `{source_string}:{uuid_bytes}` - enables prefix scans by source

### Soft Delete (Constitution SEC-06)
Per the constitution, soft delete with 30-day recovery is the default. Permanent deletion requires explicit `reason='user_requested'` and `soft_delete=false`.

---

*Task ID: TASK-M02-017*
*Module: 02 - Core Infrastructure*
*Layer: Logic*
