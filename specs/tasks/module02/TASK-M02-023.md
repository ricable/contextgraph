# TASK-M02-023: Implement Secondary Index Operations

```xml
<task_spec id="TASK-M02-023" version="1.0">
<metadata>
  <title>Implement Secondary Index Operations</title>
  <status>blocked</status>
  <layer>surface</layer>
  <module>module-02</module>
  <sequence>23</sequence>
  <priority>high</priority>
  <estimated_hours>3</estimated_hours>
  <implements>
    <item>TECH-CORE-002 Section 3: Storage specification</item>
    <item>REQ-CORE-005: Storage requirements</item>
  </implements>
  <depends_on>
    <task_ref>TASK-M02-017</task_ref>
  </depends_on>
  <estimated_complexity>medium</estimated_complexity>
</metadata>

<context>
This task implements query methods that leverage the secondary indexes created during node storage. These include querying by Johari quadrant, tags, sources, and time ranges. All queries use RocksDB prefix iterators for efficient scanning without loading entire column families.
</context>

<input_context_files>
  <file purpose="Node CRUD with index creation">crates/context-graph-storage/src/rocksdb_backend.rs</file>
  <file purpose="Column family definitions">crates/context-graph-storage/src/column_families.rs</file>
  <file purpose="JohariQuadrant enum">crates/context-graph-core/src/johari.rs</file>
</input_context_files>

<prerequisites>
  <check>TASK-M02-017 (Node CRUD Operations) completed</check>
  <check>All index column families are populated during node storage</check>
</prerequisites>

<scope>
  <in_scope>
    - get_nodes_by_quadrant() - query nodes by Johari quadrant
    - get_nodes_by_tag() - query nodes by tag
    - get_nodes_by_source() - query nodes by source
    - get_nodes_in_time_range() - query nodes by creation time
    - Pagination support via limit/offset parameters
    - Results returned as Vec&lt;NodeId&gt; (not full nodes)
    - Efficient prefix iterator usage
  </in_scope>
  <out_of_scope>
    - Full node retrieval (use get_node after getting IDs)
    - Full-text search (future module)
    - Embedding similarity search (future module - FAISS)
    - Memex trait abstraction (TASK-M02-026)
  </out_of_scope>
</scope>

<definition_of_done>
  <signatures>
    <signature file="crates/context-graph-storage/src/indexes.rs">
use crate::{RocksDbMemex, StorageError};
use context_graph_core::{JohariQuadrant, NodeId};
use chrono::{DateTime, Utc};

impl RocksDbMemex {
    /// Gets all node IDs in a specific Johari quadrant.
    ///
    /// Uses the johari_{quadrant} column family prefix scan.
    ///
    /// # Arguments
    /// * `quadrant` - The Johari quadrant to query
    /// * `limit` - Maximum number of results (None = unlimited)
    /// * `offset` - Number of results to skip
    pub async fn get_nodes_by_quadrant(
        &amp;self,
        quadrant: JohariQuadrant,
        limit: Option&lt;usize&gt;,
        offset: usize,
    ) -> Result&lt;Vec&lt;NodeId&gt;, StorageError&gt;;

    /// Gets all node IDs with a specific tag.
    ///
    /// Uses the tags column family with tag prefix scan.
    pub async fn get_nodes_by_tag(
        &amp;self,
        tag: &amp;str,
        limit: Option&lt;usize&gt;,
        offset: usize,
    ) -> Result&lt;Vec&lt;NodeId&gt;, StorageError&gt;;

    /// Gets all node IDs from a specific source.
    ///
    /// Uses the sources column family with source prefix scan.
    pub async fn get_nodes_by_source(
        &amp;self,
        source: &amp;str,
        limit: Option&lt;usize&gt;,
        offset: usize,
    ) -> Result&lt;Vec&lt;NodeId&gt;, StorageError&gt;;

    /// Gets all node IDs created within a time range.
    ///
    /// Uses the temporal column family with range scan.
    ///
    /// # Arguments
    /// * `start` - Start of time range (inclusive)
    /// * `end` - End of time range (exclusive)
    pub async fn get_nodes_in_time_range(
        &amp;self,
        start: DateTime&lt;Utc&gt;,
        end: DateTime&lt;Utc&gt;,
        limit: Option&lt;usize&gt;,
        offset: usize,
    ) -> Result&lt;Vec&lt;NodeId&gt;, StorageError&gt;;
}
    </signature>
  </signatures>

  <constraints>
    - Results must be Vec&lt;NodeId&gt;, not full MemoryNode objects
    - Pagination must work correctly (skip offset, limit results)
    - Prefix scans must terminate correctly when prefix no longer matches
    - Time range must handle timezone-aware DateTime correctly
    - Empty results return empty Vec, not error
  </constraints>

  <verification>
    - cargo build --package context-graph-storage compiles without errors
    - cargo test --package context-graph-storage index passes all tests
    - Pagination returns correct subsets
    - Time range queries respect boundaries
    - Empty queries return empty Vec
  </verification>
</definition_of_done>

<pseudo_code>
indexes.rs:

impl RocksDbMemex:
  async fn get_nodes_by_quadrant(quadrant, limit, offset):
    let inner = self.inner.read().await

    // Get the correct Johari CF
    let cf = get_johari_cf(quadrant)

    // Full scan of the quadrant CF (keys are node IDs)
    let iter = inner.db.iterator_cf(cf, IteratorMode::Start)

    let mut results = Vec::new()
    let mut skipped = 0

    for item in iter:
      let (key, _) = item?

      // Handle offset
      if skipped < offset:
        skipped += 1
        continue

      // Parse node ID from key
      let node_id = parse_node_id(&amp;key)?
      results.push(node_id)

      // Handle limit
      if let Some(max) = limit:
        if results.len() >= max:
          break

    Ok(results)

  async fn get_nodes_by_tag(tag, limit, offset):
    let inner = self.inner.read().await

    // Tag key format: tag_string:node_id_bytes
    let prefix = format_tag_prefix(tag)
    let iter = inner.db.prefix_iterator_cf(cf_tags, &amp;prefix)

    let mut results = Vec::new()
    let mut skipped = 0

    for item in iter:
      let (key, _) = item?

      // Verify prefix still matches
      if !key.starts_with(&amp;prefix):
        break

      if skipped < offset:
        skipped += 1
        continue

      // Extract node ID from key (after tag prefix)
      let node_id = extract_node_id_from_tag_key(&amp;key)?
      results.push(node_id)

      if let Some(max) = limit:
        if results.len() >= max:
          break

    Ok(results)

  async fn get_nodes_by_source(source, limit, offset):
    // Similar to get_nodes_by_tag but using sources CF
    let inner = self.inner.read().await
    let prefix = format_source_prefix(source)
    let iter = inner.db.prefix_iterator_cf(cf_sources, &amp;prefix)

    // Same pagination logic as above
    ...

  async fn get_nodes_in_time_range(start, end, limit, offset):
    let inner = self.inner.read().await

    // Temporal key format: timestamp_millis:node_id
    let start_key = format_temporal_key_start(start)
    let end_key = format_temporal_key_start(end)

    let iter = inner.db.iterator_cf(
      cf_temporal,
      IteratorMode::From(&amp;start_key, Direction::Forward)
    )

    let mut results = Vec::new()
    let mut skipped = 0

    for item in iter:
      let (key, _) = item?

      // Check if past end time
      if key >= end_key:
        break

      if skipped < offset:
        skipped += 1
        continue

      let node_id = extract_node_id_from_temporal_key(&amp;key)?
      results.push(node_id)

      if let Some(max) = limit:
        if results.len() >= max:
          break

    Ok(results)

// Helper functions
fn format_tag_prefix(tag: &amp;str) -> Vec&lt;u8&gt;
fn format_source_prefix(source: &amp;str) -> Vec&lt;u8&gt;
fn format_temporal_key_start(time: DateTime&lt;Utc&gt;) -> Vec&lt;u8&gt;
fn extract_node_id_from_tag_key(key: &amp;[u8]) -> Result&lt;NodeId, StorageError&gt;
fn extract_node_id_from_temporal_key(key: &amp;[u8]) -> Result&lt;NodeId, StorageError&gt;

#[cfg(test)]
mod index_tests:
  test_get_nodes_by_quadrant_all()
  test_get_nodes_by_quadrant_with_limit()
  test_get_nodes_by_quadrant_with_offset()
  test_get_nodes_by_tag()
  test_get_nodes_by_source()
  test_get_nodes_in_time_range()
  test_empty_results()
  test_pagination_correctness()
</pseudo_code>

<files_to_create>
  <file path="crates/context-graph-storage/src/indexes.rs">Secondary index query operations</file>
</files_to_create>

<files_to_modify>
  <file path="crates/context-graph-storage/src/lib.rs">Add pub mod indexes;</file>
  <file path="crates/context-graph-storage/src/rocksdb_backend.rs">Import and re-export index methods</file>
</files_to_modify>

<validation_criteria>
  <criterion>get_nodes_by_quadrant() uses correct Johari CF prefix scan</criterion>
  <criterion>get_nodes_by_tag() uses tags CF prefix scan</criterion>
  <criterion>get_nodes_in_time_range() uses temporal CF range scan</criterion>
  <criterion>Pagination supported via limit/offset</criterion>
  <criterion>Results returned as Vec&lt;NodeId&gt;</criterion>
  <criterion>Empty results return empty Vec, not error</criterion>
</validation_criteria>

<test_commands>
  <command>cargo build --package context-graph-storage</command>
  <command>cargo test --package context-graph-storage index -- --nocapture</command>
  <command>cargo clippy --package context-graph-storage -- -D warnings</command>
</test_commands>
</task_spec>
```

## Implementation Notes

### Key Formats (Reference from TASK-M02-017)
- **Johari CFs**: Keys are raw NodeId bytes
- **Tags CF**: `{tag_string}:{node_id_bytes}`
- **Sources CF**: `{source_string}:{node_id_bytes}`
- **Temporal CF**: `{timestamp_millis_be}:{node_id_bytes}`

### Performance Considerations
- All operations use iterators, not loading entire CFs into memory
- Prefix iterators terminate when prefix no longer matches
- Limit/offset are applied during iteration, not post-filtering

---

*Task ID: TASK-M02-023*
*Module: 02 - Core Infrastructure*
*Layer: Surface*
