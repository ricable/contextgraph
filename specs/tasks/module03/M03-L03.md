<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L03" version="3.0">
<metadata>
  <title>Semantic Model (E1 - intfloat/e5-large-v2)</title>
  <status>complete</status>
  <layer>logic</layer>
  <sequence>3</sequence>
  <implements>PRD: E1 Semantic embedding, 1024D output, constitution.yaml embeddings.E1_Semantic</implements>
  <depends_on>M03-F09 (COMPLETE), M03-L01 (COMPLETE)</depends_on>
  <estimated_hours>4</estimated_hours>
  <last_updated>2026-01-01</last_updated>
</metadata>

<critical_context>
## WHAT THIS TASK ACTUALLY REQUIRES

This task implements the first REAL pretrained model for the 12-model embedding ensemble.
Unlike the stub implementations, this MUST:
1. Load ACTUAL model weights from disk (HuggingFace format)
2. Perform ACTUAL tokenization using the model's tokenizer
3. Run ACTUAL inference to produce 1024D embeddings
4. Use ACTUAL candle-transformers for BERT-style inference

## CODEBASE CURRENT STATE (Audited 2026-01-01)

### Dependencies VERIFIED COMPLETE:
- M03-F09 (EmbeddingModel trait): `crates/context-graph-embeddings/src/traits/embedding_model.rs`
  - Trait signature verified, 29 tests passing
  - Requires: `model_id()`, `supported_input_types()`, `embed()`, `is_initialized()`

- M03-L01 (ModelRegistry): `crates/context-graph-embeddings/src/models/registry.rs`
  - 68 tests passing, uses ModelFactory pattern
  - Already exports: ModelRegistry, ModelRegistryConfig, MemoryTracker

### Current Crate Structure:
```
crates/context-graph-embeddings/
├── Cargo.toml           # candle = [] feature flag exists but NOT implemented
├── src/
│   ├── lib.rs           # Exports: models, traits, types, error, config, storage
│   ├── traits/
│   │   ├── mod.rs
│   │   ├── embedding_model.rs  # EmbeddingModel trait (M03-F09)
│   │   └── model_factory.rs    # ModelFactory trait, SingleModelConfig
│   ├── models/
│   │   ├── mod.rs              # Exports MemoryTracker, ModelRegistry
│   │   ├── registry.rs         # ModelRegistry (M03-L01)
│   │   └── memory_tracker.rs   # MemoryTracker (M03-L01)
│   ├── types/
│   │   ├── model_id.rs         # ModelId enum with 12 variants
│   │   ├── embedding.rs        # ModelEmbedding struct
│   │   └── input.rs            # ModelInput, InputType enums
│   └── error.rs                # EmbeddingError, EmbeddingResult
```

### Files TO BE CREATED by this task:
```
crates/context-graph-embeddings/src/models/pretrained/
├── mod.rs              # NEW: Module exports for pretrained models
└── semantic.rs         # NEW: SemanticModel implementation
```

### Cargo.toml Changes REQUIRED:
The `candle = []` feature exists but dependencies are NOT added yet.
This task MUST add candle dependencies when the feature is enabled.
</critical_context>

<exact_requirements>
## MODEL SPECIFICATIONS (From constitution.yaml)

| Property | Value | Source |
|----------|-------|--------|
| HuggingFace repo | intfloat/e5-large-v2 | contextprd.md |
| Native dimension | 1024D | constitution.yaml embeddings.E1_Semantic |
| Max tokens | 512 | HuggingFace model card |
| Latency target | &lt;5ms P95 | constitution.yaml perf.latency.single_embed |
| Architecture | BERT-style encoder | intfloat/e5-large-v2 model card |
| Instruction prefix | "query: " for queries, "passage: " for documents | e5 model specification |
| Default prefix | "passage: " | For documents/passages |
| Pooling | Mean pooling over token embeddings | Standard e5 behavior |
| Output normalization | L2 normalized to unit vector | Required for cosine similarity |

## INSTRUCTION PREFIX BEHAVIOR

The e5 model family requires instruction prefixes:
- **Query mode**: Input prefixed with "query: " - for search queries
- **Passage mode**: Input prefixed with "passage: " - for documents (DEFAULT)

Example:
```
Input: "How does photosynthesis work?"
Query mode output: "query: How does photosynthesis work?"
Passage mode output: "passage: How does photosynthesis work?"
```

The ModelInput.Text variant should indicate whether this is a query via metadata.
Default behavior MUST be passage mode (for document embedding).
</exact_requirements>

<implementation_signatures>
## EXACT STRUCT AND METHOD SIGNATURES

```rust
// File: crates/context-graph-embeddings/src/models/pretrained/semantic.rs

use std::path::Path;
use std::sync::atomic::{AtomicBool, Ordering};
use async_trait::async_trait;

use crate::error::{EmbeddingError, EmbeddingResult};
use crate::traits::{EmbeddingModel, SingleModelConfig};
use crate::types::{InputType, ModelEmbedding, ModelId, ModelInput};

/// Constants for SemanticModel
pub const SEMANTIC_DIMENSION: usize = 1024;
pub const SEMANTIC_MAX_TOKENS: usize = 512;
pub const SEMANTIC_LATENCY_BUDGET_MS: u32 = 5;

/// Instruction prefix for query mode (search queries)
pub const QUERY_PREFIX: &str = "query: ";

/// Instruction prefix for passage mode (documents) - DEFAULT
pub const PASSAGE_PREFIX: &str = "passage: ";

/// Semantic embedding model using intfloat/e5-large-v2.
///
/// This is the primary semantic understanding model producing 1024D dense vectors.
/// Uses instruction prefixes to distinguish between queries and passages.
///
/// # Thread Safety
/// - `AtomicBool` for `loaded` state (lock-free reads)
/// - Inner model/tokenizer require explicit synchronization if mutable
///
/// # Memory Layout
/// - Total estimated: 1.3GB for FP32 weights
/// - With FP16 quantization: ~650MB
pub struct SemanticModel {
    /// Model weights and inference engine
    /// NOTE: Type depends on candle feature flag
    /// For now, placeholder that compiles without candle
    model_state: ModelState,

    /// Path to model weights directory
    model_path: std::path::PathBuf,

    /// Configuration for this model instance
    config: SingleModelConfig,

    /// Whether model weights are loaded and ready
    loaded: AtomicBool,

    /// Memory used by model weights (bytes)
    memory_size: usize,
}

/// Internal state that varies based on feature flags
enum ModelState {
    /// Unloaded - no weights in memory
    Unloaded,

    /// Loaded with candle model and tokenizer
    /// This variant only exists when `candle` feature is enabled
    #[cfg(feature = "candle")]
    Loaded {
        // TODO: candle::Model type when candle feature implemented
        // model: candle_transformers::models::bert::BertModel,
        // tokenizer: tokenizers::Tokenizer,
        // device: candle::Device,
    },

    /// Stub for testing without real weights
    #[cfg(not(feature = "candle"))]
    Stub,
}

impl SemanticModel {
    /// Create a new SemanticModel instance.
    ///
    /// Model is NOT loaded after construction. Call `load()` before `embed()`.
    ///
    /// # Arguments
    /// * `model_path` - Path to directory containing model weights:
    ///   - `model.safetensors` or `pytorch_model.bin`
    ///   - `tokenizer.json`
    ///   - `config.json`
    /// * `config` - Device placement and quantization settings
    ///
    /// # Errors
    /// - `EmbeddingError::ConfigError` if model_path doesn't exist
    /// - `EmbeddingError::ConfigError` if config validation fails
    ///
    /// # Example
    /// ```rust,ignore
    /// let model = SemanticModel::new(
    ///     Path::new("models/semantic"),
    ///     SingleModelConfig::cuda_fp16(),
    /// )?;
    /// model.load().await?;  // Must load before embed
    /// ```
    pub fn new(model_path: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Get the instruction prefix for this model.
    ///
    /// # Arguments
    /// * `is_query` - true for search queries, false for documents
    ///
    /// # Returns
    /// - `"query: "` if is_query is true
    /// - `"passage: "` if is_query is false (default for documents)
    pub fn instruction_prefix(is_query: bool) -> &'static str {
        if is_query {
            QUERY_PREFIX
        } else {
            PASSAGE_PREFIX
        }
    }

    /// Prepare input text with the appropriate instruction prefix.
    ///
    /// Extracts is_query flag from ModelInput metadata if available,
    /// defaults to passage mode (is_query = false).
    fn prepare_input(&self, input: &ModelInput) -> EmbeddingResult<String>;

    /// Load model weights into memory.
    ///
    /// # Errors
    /// - `EmbeddingError::ModelLoadFailed` if weights file missing
    /// - `EmbeddingError::ModelLoadFailed` if tokenizer.json missing
    /// - `EmbeddingError::ModelLoadFailed` if config.json invalid
    /// - `EmbeddingError::GpuError` if CUDA device unavailable
    /// - `EmbeddingError::MemoryBudgetExceeded` if insufficient memory
    ///
    /// # Panics
    /// Never panics. All errors returned via Result.
    pub async fn load(&self) -> EmbeddingResult<()>;

    /// Unload model weights from memory.
    ///
    /// # Errors
    /// - `EmbeddingError::NotInitialized` if model not loaded
    pub async fn unload(&self) -> EmbeddingResult<()>;

    /// Embed a batch of inputs.
    ///
    /// More efficient than calling embed() multiple times due to batched inference.
    ///
    /// # Arguments
    /// * `inputs` - Slice of ModelInput to embed
    ///
    /// # Errors
    /// - `EmbeddingError::NotInitialized` if model not loaded
    /// - `EmbeddingError::UnsupportedModality` if any input is not Text
    /// - `EmbeddingError::TokenizationError` if tokenization fails
    ///
    /// # Performance
    /// - Sorts inputs by length for optimal padding
    /// - Pads to longest sequence in batch
    pub async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;
}

#[async_trait]
impl EmbeddingModel for SemanticModel {
    fn model_id(&self) -> ModelId {
        ModelId::Semantic
    }

    fn supported_input_types(&self) -> &[InputType] {
        &[InputType::Text]
    }

    fn is_initialized(&self) -> bool {
        self.loaded.load(Ordering::SeqCst)
    }

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding> {
        // 1. Check initialized
        if !self.is_initialized() {
            return Err(EmbeddingError::NotInitialized {
                model_id: self.model_id(),
            });
        }

        // 2. Validate input type
        self.validate_input(input)?;

        // 3. Prepare input with instruction prefix
        let prepared = self.prepare_input(input)?;

        // 4. Tokenize
        // 5. Run inference
        // 6. Mean pooling
        // 7. L2 normalize
        // 8. Return ModelEmbedding

        todo!("Implement actual inference")
    }
}
```

## MODULE EXPORTS (mod.rs)

```rust
// File: crates/context-graph-embeddings/src/models/pretrained/mod.rs

//! Pretrained embedding models for the 12-model ensemble.
//!
//! This module contains implementations for models that require
//! loading pretrained weights from HuggingFace repositories.
//!
//! # Feature Flags
//!
//! Models require the `candle` feature for actual inference:
//! ```toml
//! context-graph-embeddings = { version = "0.1", features = ["candle"] }
//! ```
//!
//! Without the feature, models use stub implementations for testing.

mod semantic;

pub use semantic::{
    SemanticModel,
    SEMANTIC_DIMENSION,
    SEMANTIC_MAX_TOKENS,
    SEMANTIC_LATENCY_BUDGET_MS,
    QUERY_PREFIX,
    PASSAGE_PREFIX,
};
```

## MODELS/MOD.RS UPDATE

```rust
// File: crates/context-graph-embeddings/src/models/mod.rs
// ADD this line after existing module declarations:

pub mod pretrained;

// ADD this to exports:
pub use pretrained::{SemanticModel, QUERY_PREFIX, PASSAGE_PREFIX};
```
</implementation_signatures>

<source_of_truth>
## WHERE TO VERIFY SUCCESS

After implementing this task, the ONLY sources of truth are:

| Truth ID | Location | What to Check |
|----------|----------|---------------|
| SOT-1 | `cargo test -p context-graph-embeddings semantic` | All tests pass |
| SOT-2 | `loaded: AtomicBool` in SemanticModel | Check `is_initialized()` returns correct state |
| SOT-3 | `ModelEmbedding.vector.len()` | Must be exactly 1024 |
| SOT-4 | L2 norm of output vector | Must be approximately 1.0 (plus/minus 0.001) |
| SOT-5 | `model_id()` return value | Must return `ModelId::Semantic` |

## EXECUTE AND INSPECT PROTOCOL

After every change, you MUST verify by EXECUTING AND INSPECTING:

```bash
# 1. Compile check - MUST pass before proceeding
cargo check -p context-graph-embeddings

# 2. Run specific tests - verify test count and pass rate
cargo test -p context-graph-embeddings semantic -- --nocapture

# 3. Check output vector properties (in test)
assert_eq!(embedding.vector.len(), 1024);
let norm: f32 = embedding.vector.iter().map(|x| x * x).sum::&lt;f32&gt;().sqrt();
assert!((norm - 1.0).abs() &lt; 0.001, "Vector must be L2 normalized");

# 4. Verify instruction prefix applied
let prepared = model.prepare_input(&amp;input)?;
assert!(prepared.starts_with("passage: ") || prepared.starts_with("query: "));
```
</source_of_truth>

<edge_case_audit>
## MANDATORY EDGE CASE TESTS

You MUST implement and pass tests for these 3 edge cases:

### EDGE-1: Empty Input Text
```rust
#[tokio::test]
async fn test_edge_1_empty_input_text() {
    let model = create_test_model().await;
    model.load().await.unwrap();

    // BEFORE state
    println!("EDGE-1 BEFORE: model initialized = {}", model.is_initialized());

    let input = ModelInput::text("").unwrap();
    let result = model.embed(&amp;input).await;

    // AFTER state
    println!("EDGE-1 AFTER: result = {:?}", result);

    // Empty input MUST still produce a valid 1024D embedding
    // (e5 model handles this by encoding just the prefix)
    assert!(result.is_ok());
    let embedding = result.unwrap();
    assert_eq!(embedding.vector.len(), 1024);
}
```

### EDGE-2: Maximum Token Length Exceeded
```rust
#[tokio::test]
async fn test_edge_2_max_tokens_exceeded() {
    let model = create_test_model().await;
    model.load().await.unwrap();

    // Create input &gt; 512 tokens
    let long_text = "word ".repeat(1000);  // ~1000 tokens

    // BEFORE state
    println!("EDGE-2 BEFORE: input length = {} chars", long_text.len());

    let input = ModelInput::text(&amp;long_text).unwrap();
    let result = model.embed(&amp;input).await;

    // AFTER state
    println!("EDGE-2 AFTER: result = {:?}", result);

    // MUST truncate to 512 tokens, NOT error
    assert!(result.is_ok());
    let embedding = result.unwrap();
    assert_eq!(embedding.vector.len(), 1024);
}
```

### EDGE-3: Model Not Loaded
```rust
#[tokio::test]
async fn test_edge_3_embed_before_load() {
    let model = create_test_model().await;
    // Deliberately NOT calling load()

    // BEFORE state
    println!("EDGE-3 BEFORE: model initialized = {}", model.is_initialized());

    let input = ModelInput::text("test").unwrap();
    let result = model.embed(&amp;input).await;

    // AFTER state
    println!("EDGE-3 AFTER: result = {:?}", result);

    // MUST return NotInitialized error immediately
    assert!(matches!(
        result,
        Err(EmbeddingError::NotInitialized { model_id: ModelId::Semantic })
    ));
}
```
</edge_case_audit>

<manual_output_verification>
## MANDATORY OUTPUT VERIFICATION PROTOCOL

After implementing the model, you MUST manually verify outputs exist and are correct.

### MOV-1: Verify Vector Dimension
```rust
#[tokio::test]
async fn test_mov_1_vector_dimension() {
    let model = create_and_load_model().await;
    let input = ModelInput::text("The quick brown fox").unwrap();

    let embedding = model.embed(&amp;input).await.unwrap();

    // PHYSICALLY CHECK the output
    println!("MOV-1: Vector length = {}", embedding.vector.len());
    println!("MOV-1: First 5 values = {:?}", &amp;embedding.vector[..5]);
    println!("MOV-1: Last 5 values = {:?}", &amp;embedding.vector[embedding.vector.len()-5..]);

    assert_eq!(embedding.vector.len(), 1024, "Dimension MUST be exactly 1024");
}
```

### MOV-2: Verify L2 Normalization
```rust
#[tokio::test]
async fn test_mov_2_l2_normalization() {
    let model = create_and_load_model().await;
    let input = ModelInput::text("Test sentence for normalization check").unwrap();

    let embedding = model.embed(&amp;input).await.unwrap();

    let norm: f32 = embedding.vector.iter().map(|x| x * x).sum::&lt;f32&gt;().sqrt();

    // PHYSICALLY CHECK the output
    println!("MOV-2: L2 norm = {}", norm);
    println!("MOV-2: Deviation from 1.0 = {}", (norm - 1.0).abs());

    assert!((norm - 1.0).abs() &lt; 0.001, "Vector MUST be L2 normalized to unit length");
}
```

### MOV-3: Verify Instruction Prefix Applied
```rust
#[tokio::test]
async fn test_mov_3_instruction_prefix() {
    // Test query mode
    let query_prepared = SemanticModel::instruction_prefix(true);
    println!("MOV-3: Query prefix = '{}'", query_prepared);
    assert_eq!(query_prepared, "query: ");

    // Test passage mode (default)
    let passage_prepared = SemanticModel::instruction_prefix(false);
    println!("MOV-3: Passage prefix = '{}'", passage_prepared);
    assert_eq!(passage_prepared, "passage: ");
}
```

### MOV-4: Verify Model State Transitions
```rust
#[tokio::test]
async fn test_mov_4_state_transitions() {
    let model = create_test_model().await;

    // State 1: Unloaded
    println!("MOV-4: State after new() = {}", model.is_initialized());
    assert!(!model.is_initialized());

    // State 2: Loaded
    model.load().await.unwrap();
    println!("MOV-4: State after load() = {}", model.is_initialized());
    assert!(model.is_initialized());

    // State 3: Unloaded again
    model.unload().await.unwrap();
    println!("MOV-4: State after unload() = {}", model.is_initialized());
    assert!(!model.is_initialized());
}
```
</manual_output_verification>

<full_state_verification>
## FULL STATE VERIFICATION CHECKLIST

After completing implementation:

1. **Source of Truth Inspection**
   ```bash
   # Check file exists
   ls -la crates/context-graph-embeddings/src/models/pretrained/semantic.rs

   # Check module exports
   grep -n "pub use.*Semantic" crates/context-graph-embeddings/src/models/mod.rs

   # Check lib.rs exports
   grep -n "Semantic" crates/context-graph-embeddings/src/lib.rs
   ```

2. **Execute All Tests**
   ```bash
   cargo test -p context-graph-embeddings -- semantic --nocapture 2>&gt;1 | tee /tmp/semantic_tests.log
   echo "Test count: $(grep -c 'test .* ok' /tmp/semantic_tests.log)"
   ```

3. **Verify No Compilation Warnings**
   ```bash
   cargo clippy -p context-graph-embeddings -- -D warnings 2>&gt;1 | grep -E "warning|error" || echo "Clean!"
   ```

4. **Evidence of Success**
   Provide log output showing:
   - Number of tests run and passed
   - Sample embedding output (first 5 values, last 5 values)
   - L2 norm verification
   - State transition verification
</full_state_verification>

<validation_criteria>
## PASS/FAIL CRITERIA

| Criterion | Check | Expected |
|-----------|-------|----------|
| Compilation | `cargo check -p context-graph-embeddings` | 0 errors |
| Warnings | `cargo clippy -- -D warnings` | 0 warnings |
| Tests | `cargo test semantic` | at least 15 tests pass |
| Edge cases | EDGE-1, EDGE-2, EDGE-3 | All pass |
| Manual verification | MOV-1, MOV-2, MOV-3, MOV-4 | All pass |
| Dimension | `embedding.vector.len()` | Exactly 1024 |
| Normalization | L2 norm of output | 1.0 plus/minus 0.001 |
| Model ID | `model.model_id()` | `ModelId::Semantic` |
| Input types | `model.supported_input_types()` | `[InputType::Text]` |
</validation_criteria>

<cargo_toml_update>
## REQUIRED CARGO.TOML CHANGES

When implementing with the `candle` feature:

```toml
# Add to [dependencies] section:
candle-core = { version = "0.4", optional = true }
candle-nn = { version = "0.4", optional = true }
candle-transformers = { version = "0.4", optional = true }
tokenizers = { version = "0.15", optional = true }

# Update [features] section:
[features]
default = ["stub"]
stub = []
candle = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:tokenizers"]
```

NOTE: For initial implementation, you may use the `stub` feature (default) with a stub
implementation that returns deterministic embeddings. The candle implementation is
required for production but can be a follow-up task if candle integration proves complex.
</cargo_toml_update>

<stub_implementation>
## ACCEPTABLE STUB IMPLEMENTATION (For Phase 0)

If candle integration proves too complex, an acceptable Phase 0 stub is:

```rust
#[cfg(not(feature = "candle"))]
impl SemanticModel {
    pub async fn load(&amp;self) -> EmbeddingResult&lt;()&gt; {
        self.loaded.store(true, Ordering::SeqCst);
        tracing::info!("SemanticModel stub loaded (no real weights)");
        Ok(())
    }

    pub async fn unload(&amp;self) -> EmbeddingResult&lt;()&gt; {
        if !self.is_initialized() {
            return Err(EmbeddingError::NotInitialized {
                model_id: self.model_id(),
            });
        }
        self.loaded.store(false, Ordering::SeqCst);
        tracing::info!("SemanticModel stub unloaded");
        Ok(())
    }
}

#[async_trait]
#[cfg(not(feature = "candle"))]
impl EmbeddingModel for SemanticModel {
    // ... required methods ...

    async fn embed(&amp;self, input: &amp;ModelInput) -> EmbeddingResult&lt;ModelEmbedding&gt; {
        if !self.is_initialized() {
            return Err(EmbeddingError::NotInitialized {
                model_id: self.model_id(),
            });
        }
        self.validate_input(input)?;

        let prepared = self.prepare_input(input)?;
        let start = std::time::Instant::now();

        // Deterministic embedding based on input hash (xxhash64)
        let hash = xxhash_rust::xxh64::xxh64(prepared.as_bytes(), 0);
        let mut rng = StdRng::seed_from_u64(hash);

        let mut vector: Vec&lt;f32&gt; = (0..1024)
            .map(|_| rng.gen_range(-1.0..1.0))
            .collect();

        // L2 normalize
        let norm: f32 = vector.iter().map(|x| x * x).sum::&lt;f32&gt;().sqrt();
        for v in &amp;mut vector {
            *v /= norm;
        }

        let latency_us = start.elapsed().as_micros() as u64;
        Ok(ModelEmbedding::new(ModelId::Semantic, vector, latency_us))
    }
}
```

This stub:
- Produces deterministic 1024D embeddings
- Is L2 normalized
- Tracks state correctly
- Respects instruction prefix
- Has no external dependencies
</stub_implementation>

<sherlock_investigation>
## FINAL VERIFICATION: SHERLOCK INVESTIGATION

After completing ALL implementation and tests, you MUST spawn the `sherlock-holmes`
subagent to perform a forensic investigation of the complete task.

The Sherlock investigation will verify:
1. All files created at correct paths
2. All signatures match specification
3. All tests pass and verify correct behavior
4. All edge cases handled
5. All manual verifications pass
6. No regressions introduced
7. Code follows project conventions

If Sherlock identifies ANY issues, you MUST fix them before marking the task complete.

Command to run after implementation:
```
Use Task tool with subagent_type="sherlock-holmes" to investigate M03-L03 completion
```
</sherlock_investigation>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/pretrained/mod.rs</file>
  <file>crates/context-graph-embeddings/src/models/pretrained/semantic.rs</file>
</files_to_create>

<files_to_modify>
  <file>crates/context-graph-embeddings/src/models/mod.rs</file>
  <file>crates/context-graph-embeddings/Cargo.toml (optional, for candle feature)</file>
</files_to_modify>

<test_commands>
```bash
# Compile check
cargo check -p context-graph-embeddings

# Run semantic tests only
cargo test -p context-graph-embeddings semantic -- --nocapture

# Run all embeddings tests
cargo test -p context-graph-embeddings --lib

# Clippy check
cargo clippy -p context-graph-embeddings -- -D warnings

# Verify file structure
ls -la crates/context-graph-embeddings/src/models/pretrained/
```
</test_commands>

<anti_patterns_forbidden>
## ABSOLUTELY FORBIDDEN

1. **NO MOCK DATA IN TESTS** - All tests use real ModelInput, ModelEmbedding types
2. **NO WORKAROUNDS** - If something doesn't work, error immediately with full context
3. **NO BACKWARDS COMPATIBILITY HACKS** - Code must work or fail, no middle ground
4. **NO UNWRAP IN PRODUCTION** - Use `expect()` with context or return Result
5. **NO IGNORING ERRORS** - Every error MUST propagate
6. **NO PLACEHOLDER IMPLEMENTATIONS** - Either implement fully or use documented stub
7. **NO ASSUMING SUCCESS** - Always verify output after execution
</anti_patterns_forbidden>

<completion_record>
## TASK COMPLETED: 2026-01-01

### Verification Summary
- **Truth Score**: 0.9925 (exceeds 0.95 threshold)
- **Tests**: 21 passing (exceeds 15 minimum)
- **Compilation**: 0 errors
- **Clippy**: 0 warnings

### Files Created/Modified
1. `src/models/pretrained/mod.rs` (24 lines) - Module exports
2. `src/models/pretrained/semantic.rs` (722 lines) - Full SemanticModel implementation
3. `src/models/mod.rs` - Updated to export pretrained module

### Implementation Notes
- Stub implementation using xxhash64-seeded deterministic vectors
- All constants match specification (SEMANTIC_DIMENSION=1024, etc.)
- EmbeddingModel trait fully implemented
- EDGE-1/2/3 and MOV-1/2/3/4 tests all pass
- L2 normalization verified (norm = 1.0000005, deviation < 0.001)
- State transitions verified (new->load->unload)

### Sherlock Verification
VERDICT: INNOCENT - IMPLEMENTATION VERIFIED
The implementation meets all critical requirements with minor deviation (memory_size type).
</completion_record>
</task_spec>
