# Task Specification: M03-F04

## Metadata
- **ID**: M03-F04
- **Title**: ConcatenatedEmbedding Struct
- **Status**: ✅ COMPLETE
- **Layer**: foundation
- **Sequence**: 4
- **Implements**: constitution.yaml → embeddings.models (12-model concatenation)
- **Depends On**: M03-F01 (ModelId), M03-F03 (ModelEmbedding)
- **Estimated Hours**: 2
- **File Created**: `crates/context-graph-embeddings/src/types/concatenated.rs`
- **Updated**: 2026-01-01
- **Verified By**: sherlock-holmes forensic investigation agent

---

## Implementation Status

### ✅ VERIFIED COMPLETE (2026-01-01)

**Forensic Investigation Results:**
- All 4 struct fields implemented correctly
- All 10+ required methods implemented
- xxHash64 dependency added and used correctly
- Fail-fast behavior with panics on invalid state
- 8320 dimension verified in tests
- Module properly exported in mod.rs
- 35 tests for concatenated.rs specifically (108 total for crate)
- All 3 edge cases tested and passing
- Real data used in tests (no mocks)
- Latency tracking with overflow protection (saturating_add/saturating_sub)

**Compilation & Tests:**
```
$ cargo check -p context-graph-embeddings → PASSED
$ cargo test -p context-graph-embeddings → 108 tests passed, 0 failed
```

**Enhancements Beyond Specification:**
- `get_slice(model_id)` method for extracting model slices from concatenated vector
- saturating_add/saturating_sub for overflow protection
- Uses MODEL_COUNT constant instead of magic number 12
- Detailed panic messages with debug information

---

## Context

### What This Task Is
Implement `ConcatenatedEmbedding` struct that aggregates outputs from all 12 embedding models into a single 8320-dimensional vector for FuseMoE input. This is the container that collects individual `ModelEmbedding` outputs and provides the concatenated input to the fusion network.

### Why This Is Needed
- FuseMoE requires a single 8320D input vector (all 12 model outputs concatenated)
- Need to track which models have produced outputs (some may timeout)
- Need to sum latencies for performance monitoring
- Need content hash for caching the concatenated result

### How It Fits
```
Individual Models (E1-E12)
         ↓
    ModelEmbedding (per model)
         ↓
    ConcatenatedEmbedding (this task) ← collects all 12
         ↓
    FuseMoE (8320D → 1536D)
         ↓
    FusedEmbedding (final output)
```

---

## Current Codebase State

### Existing Files (VERIFIED 2026-01-01)
```
crates/context-graph-embeddings/src/
├── lib.rs              # Exports EmbeddingError, EmbeddingResult, ModelId
├── error.rs            # EmbeddingError enum with DimensionMismatch, InvalidInput
├── types/
│   ├── mod.rs          # pub use embedding::ModelEmbedding; pub use model_id::ModelId;
│   ├── model_id.rs     # ModelId enum (12 variants, 0-11)
│   ├── dimensions.rs   # TOTAL_CONCATENATED=8320, OFFSETS array, MODEL_COUNT=12
│   └── embedding.rs    # ModelEmbedding struct (per-model output)
```

### Key Types Already Implemented

**ModelId** (model_id.rs:38-66):
```rust
#[repr(u8)]
pub enum ModelId {
    Semantic = 0,       // E1, 1024D
    TemporalRecent = 1, // E2, 512D
    TemporalPeriodic = 2,  // E3, 512D
    TemporalPositional = 3, // E4, 512D
    Causal = 4,         // E5, 768D
    Sparse = 5,         // E6, 1536D projected
    Code = 6,           // E7, 768D projected
    Graph = 7,          // E8, 384D
    Hdc = 8,            // E9, 1024D projected
    Multimodal = 9,     // E10, 768D
    Entity = 10,        // E11, 384D
    LateInteraction = 11, // E12, 128D
}
// Has: dimension(), projected_dimension(), all(), TryFrom<u8>
```

**ModelEmbedding** (embedding.rs:29-46):
```rust
pub struct ModelEmbedding {
    pub model_id: ModelId,
    pub vector: Vec<f32>,
    pub latency_us: u64,
    pub attention_weights: Option<Vec<f32>>,
    pub is_projected: bool,
}
// Has: new(), validate(), l2_norm(), normalize(), cosine_similarity()
```

**Dimensions** (dimensions.rs):
```rust
pub const TOTAL_CONCATENATED: usize = 8320;
pub const MODEL_COUNT: usize = 12;
pub const OFFSETS: [usize; 12] = [0, 1024, 1536, 2048, 2560, 3328, 4864, 5632, 6016, 7040, 7808, 8192];
pub const PROJECTED_DIMENSIONS: [usize; 12] = [1024, 512, 512, 512, 768, 1536, 768, 384, 1024, 768, 384, 128];
pub fn offset_by_index(index: usize) -> usize;
pub fn projected_dimension_by_index(index: usize) -> usize;
```

**EmbeddingError** (error.rs):
```rust
pub enum EmbeddingError {
    ModelLoadError(String),
    GenerationError(String),
    InvalidInput(String),
    DimensionMismatch { expected: usize, actual: usize },
    IoError(#[from] std::io::Error),
}
```

---

## Implementation Requirements

### Struct Definition
```rust
/// Aggregates outputs from all 12 embedding models.
///
/// This struct collects individual ModelEmbedding outputs and concatenates
/// them into a single 8320D vector for FuseMoE input.
///
/// # Invariants
/// - Embeddings are indexed by ModelId as u8 (0-11)
/// - is_complete() true only when all 12 slots filled
/// - concatenated vector is built in model order (E1-E12)
/// - content_hash is deterministic for same embeddings
#[derive(Debug, Clone)]
pub struct ConcatenatedEmbedding {
    /// Individual model embeddings indexed by ModelId as u8
    /// Array of 12 slots, each Option<ModelEmbedding>
    pub embeddings: [Option<ModelEmbedding>; 12],

    /// The concatenated 8320D vector (built by concatenate())
    pub concatenated: Vec<f32>,

    /// Sum of all individual model latencies in microseconds
    pub total_latency_us: u64,

    /// xxHash64 of concatenated vector bytes for caching
    pub content_hash: u64,
}
```

### Required Methods

```rust
impl ConcatenatedEmbedding {
    /// Creates new ConcatenatedEmbedding with all slots empty.
    /// concatenated = empty Vec, total_latency_us = 0, content_hash = 0
    pub fn new() -> Self;

    /// Sets embedding at index matching model_id.
    /// Updates total_latency_us by adding embedding.latency_us.
    /// Validates embedding dimension matches model's projected_dimension.
    /// Panics: if embedding.model_id dimension doesn't match vector length
    pub fn set(&mut self, embedding: ModelEmbedding);

    /// Gets embedding for model_id (if present).
    pub fn get(&self, model_id: ModelId) -> Option<&ModelEmbedding>;

    /// Returns true only if all 12 slots are Some.
    pub fn is_complete(&self) -> bool;

    /// Returns list of ModelId variants not yet set.
    pub fn missing_models(&self) -> Vec<ModelId>;

    /// Returns count of filled slots (0-12).
    pub fn filled_count(&self) -> usize;

    /// Builds concatenated vector from embeddings in model order (E1-E12).
    /// Requires: is_complete() == true
    /// After: concatenated.len() == 8320, content_hash is computed
    /// Panics: if not complete
    pub fn concatenate(&mut self);

    /// Returns total dimension (8320 when complete).
    pub fn total_dimension(&self) -> usize;

    /// Validates all embeddings against their model requirements.
    pub fn validate(&self) -> EmbeddingResult<()>;

    /// Computes xxHash64 of concatenated vector bytes.
    /// Must be deterministic: same embeddings → same hash.
    fn compute_hash(data: &[f32]) -> u64;
}

impl Default for ConcatenatedEmbedding {
    fn default() -> Self { Self::new() }
}
```

---

## Constraints (MUST Follow)

1. **Array Size**: `embeddings` is `[Option<ModelEmbedding>; 12]` - exactly 12 slots
2. **Indexing**: Use `ModelId as u8` for array index (0-11)
3. **Completeness**: `is_complete()` returns true ONLY when all 12 slots are `Some`
4. **Order**: `concatenate()` builds vector in E1-E12 order (index 0, 1, 2, ... 11)
5. **Dimensions**: Use projected dimensions (Sparse=1536, Code=768, HDC=1024)
6. **Hash**: Use xxHash64 for content_hash (add `xxhash-rust` dependency)
7. **Validation**: All embeddings must pass `validate()` before concatenation
8. **No Fallbacks**: If something is wrong, return error or panic - do not silently continue
9. **Latency**: `total_latency_us` is sum of all individual embedding latencies

---

## File Changes Required

### 1. Update types/mod.rs
```rust
//! Core types for the 12-model embedding pipeline.

mod concatenated;  // ADD THIS
mod embedding;
mod model_id;
pub mod dimensions;

pub use concatenated::ConcatenatedEmbedding;  // ADD THIS
pub use embedding::ModelEmbedding;
pub use model_id::ModelId;
pub use model_id::TokenizerFamily;
```

### 2. Update Cargo.toml (add xxhash dependency)
Add after `rand = "0.8"` line in `crates/context-graph-embeddings/Cargo.toml`:
```toml
# Hashing for content deduplication
xxhash-rust = { version = "0.8", features = ["xxh64"] }
```

Current state of Cargo.toml (relevant section):
```toml
# For stub implementations
rand = "0.8"
# ADD xxhash-rust HERE
```

### 3. Create types/concatenated.rs
Full implementation as specified above.

---

## Test Cases (MUST All Pass)

### Unit Tests
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::{ModelId, ModelEmbedding, dimensions};

    #[test]
    fn test_new_creates_empty_struct() {
        let ce = ConcatenatedEmbedding::new();
        assert!(ce.embeddings.iter().all(|e| e.is_none()));
        assert!(ce.concatenated.is_empty());
        assert_eq!(ce.total_latency_us, 0);
        assert_eq!(ce.content_hash, 0);
        assert!(!ce.is_complete());
        assert_eq!(ce.filled_count(), 0);
    }

    #[test]
    fn test_set_places_at_correct_index() {
        let mut ce = ConcatenatedEmbedding::new();
        let emb = ModelEmbedding::new(ModelId::Semantic, vec![0.1; 1024], 1000);
        ce.set(emb);

        assert!(ce.embeddings[0].is_some()); // Semantic = 0
        assert_eq!(ce.filled_count(), 1);
        assert_eq!(ce.total_latency_us, 1000);
    }

    #[test]
    fn test_get_returns_correct_embedding() {
        let mut ce = ConcatenatedEmbedding::new();
        let emb = ModelEmbedding::new(ModelId::Causal, vec![0.1; 768], 500);
        ce.set(emb.clone());

        let got = ce.get(ModelId::Causal);
        assert!(got.is_some());
        assert_eq!(got.unwrap().model_id, ModelId::Causal);
    }

    #[test]
    fn test_is_complete_only_when_all_12() {
        let mut ce = ConcatenatedEmbedding::new();

        // Fill 11 models
        for model_id in ModelId::all().iter().take(11) {
            let dim = model_id.projected_dimension();
            let emb = ModelEmbedding::new(*model_id, vec![0.1; dim], 100);
            ce.set(emb);
        }
        assert!(!ce.is_complete());
        assert_eq!(ce.filled_count(), 11);

        // Fill last model
        let emb = ModelEmbedding::new(ModelId::LateInteraction, vec![0.1; 128], 100);
        ce.set(emb);
        assert!(ce.is_complete());
        assert_eq!(ce.filled_count(), 12);
    }

    #[test]
    fn test_missing_models_returns_correct_list() {
        let mut ce = ConcatenatedEmbedding::new();
        let missing = ce.missing_models();
        assert_eq!(missing.len(), 12);

        // Set one model
        let emb = ModelEmbedding::new(ModelId::Semantic, vec![0.1; 1024], 100);
        ce.set(emb);

        let missing = ce.missing_models();
        assert_eq!(missing.len(), 11);
        assert!(!missing.contains(&ModelId::Semantic));
    }

    #[test]
    fn test_concatenate_produces_8320_vector() {
        let mut ce = create_complete_embedding();
        ce.concatenate();

        assert_eq!(ce.concatenated.len(), dimensions::TOTAL_CONCATENATED);
        assert_eq!(ce.concatenated.len(), 8320);
    }

    #[test]
    fn test_concatenate_order_matches_model_order() {
        let mut ce = ConcatenatedEmbedding::new();

        // Set each model with unique value
        for (i, model_id) in ModelId::all().iter().enumerate() {
            let dim = model_id.projected_dimension();
            let emb = ModelEmbedding::new(*model_id, vec![i as f32; dim], 100);
            ce.set(emb);
        }

        ce.concatenate();

        // Verify order: first 1024 elements should be 0.0 (Semantic)
        assert_eq!(ce.concatenated[0], 0.0);
        // Next 512 should be 1.0 (TemporalRecent)
        assert_eq!(ce.concatenated[1024], 1.0);
        // Last 128 should be 11.0 (LateInteraction)
        assert_eq!(ce.concatenated[8320 - 1], 11.0);
    }

    #[test]
    fn test_content_hash_deterministic() {
        let mut ce1 = create_complete_embedding();
        let mut ce2 = create_complete_embedding();

        ce1.concatenate();
        ce2.concatenate();

        assert_eq!(ce1.content_hash, ce2.content_hash);
        assert_ne!(ce1.content_hash, 0);
    }

    #[test]
    fn test_total_latency_sums_all() {
        let mut ce = ConcatenatedEmbedding::new();

        for model_id in ModelId::all() {
            let dim = model_id.projected_dimension();
            let emb = ModelEmbedding::new(*model_id, vec![0.1; dim], 100);
            ce.set(emb);
        }

        assert_eq!(ce.total_latency_us, 1200); // 12 * 100
    }

    #[test]
    #[should_panic]
    fn test_concatenate_panics_when_incomplete() {
        let mut ce = ConcatenatedEmbedding::new();
        ce.concatenate(); // Should panic
    }

    #[test]
    fn test_total_dimension() {
        let mut ce = create_complete_embedding();
        ce.concatenate();
        assert_eq!(ce.total_dimension(), 8320);
    }

    // Helper function
    fn create_complete_embedding() -> ConcatenatedEmbedding {
        let mut ce = ConcatenatedEmbedding::new();
        for model_id in ModelId::all() {
            let dim = model_id.projected_dimension();
            let mut emb = ModelEmbedding::new(*model_id, vec![0.5; dim], 100);
            emb.set_projected(true);
            ce.set(emb);
        }
        ce
    }
}
```

---

## Edge Case Testing (REQUIRED)

You MUST test these edge cases with before/after state verification:

### Edge Case 1: Empty Input Handling
```rust
#[test]
fn edge_case_empty_struct() {
    let ce = ConcatenatedEmbedding::new();
    println!("BEFORE: filled={}, complete={}", ce.filled_count(), ce.is_complete());

    let missing = ce.missing_models();

    println!("AFTER: missing_count={}", missing.len());
    assert_eq!(missing.len(), 12);
    println!("Edge Case 1 PASSED: Empty struct returns all 12 models as missing");
}
```

### Edge Case 2: Overwrite Existing Embedding
```rust
#[test]
fn edge_case_overwrite() {
    let mut ce = ConcatenatedEmbedding::new();
    let emb1 = ModelEmbedding::new(ModelId::Semantic, vec![1.0; 1024], 100);
    ce.set(emb1);

    println!("BEFORE: latency={}, first_value={}",
             ce.total_latency_us,
             ce.embeddings[0].as_ref().unwrap().vector[0]);

    let emb2 = ModelEmbedding::new(ModelId::Semantic, vec![2.0; 1024], 200);
    ce.set(emb2);

    println!("AFTER: latency={}, first_value={}",
             ce.total_latency_us,
             ce.embeddings[0].as_ref().unwrap().vector[0]);

    // Note: This tests current behavior - document if latency should replace or add
    assert_eq!(ce.embeddings[0].as_ref().unwrap().vector[0], 2.0);
    println!("Edge Case 2 PASSED: Overwrite replaces embedding");
}
```

### Edge Case 3: Maximum Latency Value
```rust
#[test]
fn edge_case_max_latency() {
    let mut ce = ConcatenatedEmbedding::new();
    let emb = ModelEmbedding::new(ModelId::Semantic, vec![0.1; 1024], u64::MAX);

    println!("BEFORE: total_latency={}", ce.total_latency_us);
    ce.set(emb);
    println!("AFTER: total_latency={}", ce.total_latency_us);

    assert_eq!(ce.total_latency_us, u64::MAX);
    println!("Edge Case 3 PASSED: u64::MAX latency handled correctly");
}
```

---

## Full State Verification (MANDATORY)

After implementation, verify:

### Source of Truth Check
```rust
#[test]
fn verify_source_of_truth() {
    // The concatenated vector IS the source of truth
    let mut ce = create_complete_embedding();
    ce.concatenate();

    // 1. Verify vector exists in memory
    assert!(!ce.concatenated.is_empty());
    println!("SOURCE OF TRUTH: concatenated.len() = {}", ce.concatenated.len());

    // 2. Verify dimensions match specification
    assert_eq!(ce.concatenated.len(), dimensions::TOTAL_CONCATENATED);
    println!("DIMENSION CHECK: {} == {} (expected)",
             ce.concatenated.len(), dimensions::TOTAL_CONCATENATED);

    // 3. Verify hash is non-zero
    assert_ne!(ce.content_hash, 0);
    println!("HASH CHECK: content_hash = {} (non-zero)", ce.content_hash);

    // 4. Read back individual slices
    for (i, model_id) in ModelId::all().iter().enumerate() {
        let offset = dimensions::offset_by_index(i);
        let dim = dimensions::projected_dimension_by_index(i);
        let slice = &ce.concatenated[offset..offset + dim];

        println!("MODEL {}: offset={}, dim={}, slice_len={}",
                 model_id, offset, dim, slice.len());
        assert_eq!(slice.len(), dim);
    }

    println!("VERIFICATION COMPLETE: All checks passed");
}
```

---

## Validation Criteria

- [ ] `cargo check` passes with no errors
- [ ] `cargo clippy` passes with no warnings
- [ ] `cargo test` - all tests pass
- [ ] All 12 models can be set and retrieved
- [ ] Concatenation produces exactly 8320 floats
- [ ] Concatenation order matches E1-E12 specification
- [ ] Content hash is deterministic
- [ ] Edge cases documented and tested
- [ ] Source of truth verification passes

---

## Final Verification with Sherlock Holmes Agent

After completing implementation, the `sherlock-holmes` subagent MUST verify:

1. **File Exists**: `crates/context-graph-embeddings/src/types/concatenated.rs` exists
2. **Module Export**: `types/mod.rs` exports `ConcatenatedEmbedding`
3. **All Tests Pass**: `cargo test --lib -p context-graph-embeddings`
4. **No Compilation Errors**: `cargo check -p context-graph-embeddings`
5. **Struct Has All Fields**: embeddings, concatenated, total_latency_us, content_hash
6. **Methods Implemented**: new, set, get, is_complete, missing_models, concatenate, total_dimension
7. **Hash Dependency**: xxhash-rust added to Cargo.toml or alternative hash used
8. **Concatenation Length**: Verified to produce exactly 8320 elements
9. **Edge Cases**: All 3 edge case tests present and passing

If ANY verification fails, Sherlock must identify the exact issue and how to fix it.
