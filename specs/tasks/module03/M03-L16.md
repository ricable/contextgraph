# M03-L16: BatchQueue and Request Types

```xml
<task_spec id="M03-L16" version="2.3">
<metadata>
  <title>Implement BatchQueue and BatchRequest Types</title>
  <status>complete</status>
  <actual_status>FULLY IMPLEMENTED AND VERIFIED</actual_status>
  <completed_date>2026-01-01</completed_date>
  <layer>logic</layer>
  <sequence>16</sequence>
  <implements>PRD batch processing infrastructure</implements>
  <depends_on>M03-F06, M03-F13</depends_on>
  <estimated_hours>2</estimated_hours>
</metadata>

<context>
Implement batch queue and request types for asynchronous batching.
The batch system allows multiple embedding requests to be collected
and processed together for improved GPU utilization and throughput.

Key components:
- BatchRequest: Individual embedding request with response channel
- BatchQueue: Collection of pending requests per model
- Batch: Assembled batch ready for processing

These types form the foundation for the BatchProcessor (M03-L17).

## CRITICAL CODEBASE CONTEXT

### SHERLOCK VERIFICATION COMPLETED (2026-01-01)

**STATUS: FULLY IMPLEMENTED AND PASSING**

- `src/batch/mod.rs` - 67 lines ✓
- `src/batch/types.rs` - 1273 lines ✓

**VERIFICATION RESULTS:**
- ✓ `cargo check` - PASS (no warnings)
- ✓ `cargo test --lib batch` - 35 tests PASS (91 batch-related total)
- ✓ `cargo clippy -D warnings` - PASS
- ✓ `lib.rs` exports `pub mod batch;`
- ✓ All types properly defined: BatchRequest, BatchQueue, Batch, BatchQueueStats

**DESIGN NOTE:** `Batch::fail()` and `BatchQueue::cancel_all()` take `impl Into<String>`
instead of `EmbeddingError` to avoid requiring Clone on EmbeddingError (which has non-Clone
variants like `Box<dyn Error>` and `std::io::Error`). This is a valid alternative approach.

**DEPENDENCY:** Added `uuid = { version = "1.0", features = ["v4"] }` to Cargo.toml for request tracking.

### Existing Infrastructure (DO NOT DUPLICATE)
- `config.rs:218-288` - BatchConfig ALREADY EXISTS with fields:
  - max_batch_size (32), min_batch_size (1), max_wait_ms (50)
  - dynamic_batching, padding_strategy, sort_by_length
- `storage/batch.rs` - BatchBinaryEncoder for GDS storage (DIFFERENT PURPOSE)
- `error.rs:105-106` - BatchError variant EXISTS
- `batch/mod.rs` - ALREADY EXISTS (67 lines)
- `batch/types.rs` - ALREADY EXISTS (1273 lines) but has errors

### Types You MUST Use (from lib.rs exports)
- `ModelInput` - from types/input.rs (Text/Code/Image/Audio variants)
- `ModelEmbedding` - from types/embedding.rs
- `ModelId` - from types/model_id.rs
- `BatchConfig` - from config.rs
- `EmbeddingResult`, `EmbeddingError` - from error.rs

### Constitution Requirements (docs2/constitution.yaml)
- batch_embed_64: "<50ms" latency budget
- throughput: embed_batch ">1000/sec"
- NO FALLBACKS: Errors propagate, never silently handled
- FAIL FAST: Invalid state triggers immediate error
</context>

<definition_of_done>
  <signatures>
```rust
// FILE: crates/context-graph-embeddings/src/batch/request.rs
use tokio::sync::oneshot;
use uuid::Uuid;
use std::time::Instant;
use crate::types::{ModelInput, ModelId, ModelEmbedding};
use crate::error::{EmbeddingResult, EmbeddingError};

/// Individual embedding request submitted to the batch system.
/// NO FALLBACKS: Request fails fast if channel is closed.
#[derive(Debug)]
pub struct BatchRequest {
    /// Unique request identifier (UUID v4)
    pub id: Uuid,
    /// Input to embed - MUST be validated (non-empty)
    pub input: ModelInput,
    /// Target model for embedding
    pub model_id: ModelId,
    /// Channel for returning result - NO RETRY on send failure
    pub response_tx: oneshot::Sender<EmbeddingResult<ModelEmbedding>>,
    /// Timestamp when request was submitted
    pub submitted_at: Instant,
    /// Priority level (0=normal, higher=more urgent, max 255)
    pub priority: u8,
}

impl BatchRequest {
    /// Create new request with response channel.
    /// Returns (request, receiver) tuple.
    /// FAIL FAST: Panics if input validation fails.
    pub fn new(
        input: ModelInput,
        model_id: ModelId,
    ) -> (Self, oneshot::Receiver<EmbeddingResult<ModelEmbedding>>);

    /// Create request with custom priority.
    pub fn with_priority(
        input: ModelInput,
        model_id: ModelId,
        priority: u8,
    ) -> (Self, oneshot::Receiver<EmbeddingResult<ModelEmbedding>>);

    /// Time elapsed since submission.
    #[inline]
    pub fn elapsed(&self) -> std::time::Duration;

    /// Estimated token count for batching decisions.
    /// Used by sort_by_length when enabled in BatchConfig.
    pub fn estimated_tokens(&self) -> usize;
}
```

```rust
// FILE: crates/context-graph-embeddings/src/batch/queue.rs
use std::collections::VecDeque;
use std::sync::atomic::{AtomicU64, Ordering};
use crate::config::BatchConfig;
use crate::types::ModelId;
use crate::error::EmbeddingError;
use super::request::BatchRequest;
use super::batch::Batch;

/// Queue of pending requests for a single model.
/// NOT thread-safe - wrap in Arc<Mutex<>> for concurrent access.
#[derive(Debug)]
pub struct BatchQueue {
    /// Pending requests ordered by submission time
    requests: VecDeque<BatchRequest>,
    /// Configuration for batching behavior (from config.rs)
    config: BatchConfig,
    /// Model this queue serves
    model_id: ModelId,
    /// Statistics (atomic for lock-free reads)
    stats: BatchQueueStats,
}

impl BatchQueue {
    /// Create new queue for a model.
    /// FAIL FAST: Panics if config.validate() fails.
    pub fn new(model_id: ModelId, config: BatchConfig) -> Self;

    /// Add request to queue.
    /// Updates stats.requests_received atomically.
    pub fn push(&mut self, request: BatchRequest);

    /// Check if queue should be flushed (batch ready).
    /// Returns true when:
    /// - requests.len() >= config.max_batch_size, OR
    /// - oldest request waited >= config.max_wait_ms (AND len >= config.min_batch_size)
    pub fn should_flush(&self) -> bool;

    /// Extract a batch of requests for processing.
    /// Returns None if empty.
    /// If config.sort_by_length, sorts by estimated_tokens.
    pub fn drain_batch(&mut self) -> Option<Batch>;

    /// Number of pending requests.
    #[inline]
    pub fn len(&self) -> usize;

    /// Check if queue is empty.
    #[inline]
    pub fn is_empty(&self) -> bool;

    /// Oldest request wait time.
    /// Returns None if queue is empty.
    pub fn oldest_wait_time(&self) -> Option<std::time::Duration>;

    /// Clear all pending requests with error.
    /// Sends error to ALL response channels (ignores send failures).
    /// Use for graceful shutdown.
    pub fn cancel_all(&mut self, error: EmbeddingError);

    /// Get current statistics (lock-free snapshot).
    pub fn stats(&self) -> BatchQueueSummary;
}
```

```rust
// FILE: crates/context-graph-embeddings/src/batch/batch.rs
use uuid::Uuid;
use std::time::Instant;
use tokio::sync::oneshot;
use crate::types::{ModelInput, ModelId, ModelEmbedding};
use crate::error::{EmbeddingResult, EmbeddingError};

/// Assembled batch ready for processing.
/// Consumes BatchRequests, owns the response channels.
#[derive(Debug)]
pub struct Batch {
    /// Batch identifier (UUID v4)
    pub id: Uuid,
    /// Model to use for all inputs
    pub model_id: ModelId,
    /// Inputs in this batch (same order as response_txs)
    pub inputs: Vec<ModelInput>,
    /// Response channels (same order as inputs)
    pub response_txs: Vec<oneshot::Sender<EmbeddingResult<ModelEmbedding>>>,
    /// Original request IDs for tracing
    pub request_ids: Vec<Uuid>,
    /// When batch was assembled
    pub assembled_at: Instant,
    /// Total estimated tokens in batch (for padding estimation)
    pub total_tokens: usize,
}

impl Batch {
    /// Create new empty batch for a model.
    pub fn new(model_id: ModelId) -> Self;

    /// Add a request to the batch.
    /// Moves ownership of input and response channel.
    pub fn add(&mut self, request: BatchRequest);

    /// Number of items in batch.
    #[inline]
    pub fn len(&self) -> usize;

    /// Check if batch is empty.
    #[inline]
    pub fn is_empty(&self) -> bool;

    /// Send results back to requesters.
    /// CRITICAL: results.len() MUST equal self.len().
    /// Consumes self - batch is no longer usable after this.
    /// Ignores send failures (receiver may have dropped).
    pub fn complete(self, results: Vec<EmbeddingResult<ModelEmbedding>>);

    /// Send error to all requesters.
    /// Consumes self - batch is no longer usable after this.
    /// Requires Clone on EmbeddingError.
    pub fn fail(self, error: EmbeddingError);
}
```

```rust
// FILE: crates/context-graph-embeddings/src/batch/stats.rs
use std::sync::atomic::{AtomicU64, Ordering};

/// Queue statistics for monitoring (thread-safe atomics).
#[derive(Debug, Default)]
pub struct BatchQueueStats {
    pub requests_received: AtomicU64,
    pub batches_processed: AtomicU64,
    pub requests_completed: AtomicU64,
    pub requests_failed: AtomicU64,
    pub total_wait_time_us: AtomicU64,
    pub total_batch_size: AtomicU64,
}

/// Snapshot of queue statistics (non-atomic, for display).
#[derive(Debug, Clone)]
pub struct BatchQueueSummary {
    pub requests_received: u64,
    pub batches_processed: u64,
    pub requests_completed: u64,
    pub requests_failed: u64,
    pub avg_wait_time_us: u64,
    pub avg_batch_size: f64,
}

impl BatchQueueStats {
    pub fn new() -> Self;
    pub fn record_request(&self);
    pub fn record_batch(&self, size: usize, wait_time_us: u64);
    pub fn record_completion(&self, success: bool);
    pub fn summary(&self) -> BatchQueueSummary;
}
```

```rust
// FILE: crates/context-graph-embeddings/src/batch/mod.rs
//! Batch queue and request types for async embedding requests.
//!
//! # Components
//! - BatchRequest: Individual request with oneshot response channel
//! - BatchQueue: Per-model queue with flush logic
//! - Batch: Assembled batch ready for model inference
//! - BatchQueueStats: Thread-safe statistics
//!
//! # Design
//! - NO FALLBACKS: All errors propagate
//! - FAIL FAST: Invalid state panics or errors immediately
//! - Thread-safe stats via atomics

mod batch;
mod queue;
mod request;
mod stats;

pub use batch::Batch;
pub use queue::BatchQueue;
pub use request::BatchRequest;
pub use stats::{BatchQueueStats, BatchQueueSummary};
```
  </signatures>

  <constraints>
    <constraint>BatchRequest uses oneshot channel for async response - NO RETRY</constraint>
    <constraint>UUID v4 for unique request identification</constraint>
    <constraint>Priority support (0-255) for request ordering</constraint>
    <constraint>should_flush() respects BatchConfig.max_batch_size AND max_wait_ms</constraint>
    <constraint>should_flush() requires min_batch_size to be met before timeout flush</constraint>
    <constraint>drain_batch() sorts by estimated_tokens when config.sort_by_length=true</constraint>
    <constraint>Batch::complete() sends results in EXACT order matching inputs</constraint>
    <constraint>Batch::complete() asserts results.len() == inputs.len()</constraint>
    <constraint>cancel_all() properly cleans up on shutdown - sends error to all</constraint>
    <constraint>Thread-safe statistics with AtomicU64 (Ordering::Relaxed for stats)</constraint>
    <constraint>EmbeddingError must implement Clone for Batch::fail()</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/batch/mod.rs</file>
  <file>crates/context-graph-embeddings/src/batch/request.rs</file>
  <file>crates/context-graph-embeddings/src/batch/queue.rs</file>
  <file>crates/context-graph-embeddings/src/batch/batch.rs</file>
  <file>crates/context-graph-embeddings/src/batch/stats.rs</file>
</files_to_create>

<files_to_modify>
  <file action="add_pub_mod">crates/context-graph-embeddings/src/lib.rs</file>
  <file action="add_derive_clone">crates/context-graph-embeddings/src/error.rs</file>
</files_to_modify>

<validation_criteria>
  <criterion>cargo check passes with no warnings</criterion>
  <criterion>cargo test --lib passes all tests</criterion>
  <criterion>cargo clippy -D warnings passes</criterion>
  <criterion>BatchRequest::new() returns (request, receiver) tuple</criterion>
  <criterion>BatchQueue::push() increments stats.requests_received</criterion>
  <criterion>should_flush() returns true at config.max_batch_size</criterion>
  <criterion>should_flush() returns true after max_wait_ms when len >= min_batch_size</criterion>
  <criterion>should_flush() returns false after max_wait_ms when len < min_batch_size</criterion>
  <criterion>drain_batch() returns None when empty</criterion>
  <criterion>drain_batch() sorts by length when config.sort_by_length=true</criterion>
  <criterion>Batch::complete() sends to all receivers in order</criterion>
  <criterion>Batch::complete() panics if results.len() != inputs.len()</criterion>
  <criterion>Statistics are correctly updated (atomic operations)</criterion>
</validation_criteria>
</task_spec>
```

---

## Source of Truth

| Item | Authoritative Location | Version |
|------|------------------------|---------|
| BatchConfig | `src/config.rs:218-288` | Use existing, DO NOT duplicate |
| ModelInput | `src/types/input.rs` | Text/Code/Image/Audio variants |
| ModelEmbedding | `src/types/embedding.rs` | Has validate(), normalize() |
| ModelId | `src/types/model_id.rs` | 12 model enum |
| EmbeddingError | `src/error.rs` | Must add Clone derive |
| EmbeddingResult | `src/error.rs:152` | Type alias |
| BatchBinaryEncoder | `src/storage/batch.rs` | DIFFERENT - for GDS encoding |
| Constitution | `docs2/constitution.yaml` | Performance budgets, anti-patterns |

---

## Full State Verification Requirements

### 1. Execute & Inspect Pattern
For every test, print BEFORE and AFTER state:

```rust
#[test]
fn test_batch_request_new() {
    println!("BEFORE: Creating new BatchRequest");
    let input = ModelInput::text("Hello world".to_string());
    println!("BEFORE: input created, text len = {}",
        match &input { ModelInput::Text { content, .. } => content.len(), _ => 0 });

    let (request, receiver) = BatchRequest::new(input, ModelId::Semantic);

    println!("AFTER: request.id = {}", request.id);
    println!("AFTER: request.model_id = {:?}", request.model_id);
    println!("AFTER: request.priority = {}", request.priority);
    println!("AFTER: receiver is_connected = true (by construction)");

    assert!(!receiver.try_recv().is_ok()); // Not yet sent
    println!("PASSED: receiver has not received anything yet");
}
```

### 2. Edge Case Audits (3 minimum)

**Edge Case 1: Empty Queue Drain**
```rust
#[test]
fn test_drain_empty_queue() {
    let config = BatchConfig::default();
    println!("BEFORE: empty queue, config.max_batch_size = {}", config.max_batch_size);

    let mut queue = BatchQueue::new(ModelId::Semantic, config);
    println!("BEFORE: queue.len() = {}", queue.len());

    let batch = queue.drain_batch();

    println!("AFTER: drain_batch() returned = {:?}", batch.is_some());
    assert!(batch.is_none());
    println!("PASSED: empty queue returns None");
}
```

**Edge Case 2: Timeout with min_batch_size NOT met**
```rust
#[test]
fn test_should_flush_timeout_min_not_met() {
    let config = BatchConfig {
        max_batch_size: 32,
        min_batch_size: 4,
        max_wait_ms: 10,
        ..Default::default()
    };
    println!("BEFORE: config.min_batch_size = {}, max_wait_ms = {}ms",
        config.min_batch_size, config.max_wait_ms);

    let mut queue = BatchQueue::new(ModelId::Semantic, config);
    let (req, _rx) = BatchRequest::new(ModelInput::text("test".to_string()), ModelId::Semantic);
    queue.push(req);
    println!("BEFORE: queue.len() = 1 < min_batch_size");

    std::thread::sleep(std::time::Duration::from_millis(15));
    println!("AFTER: waited 15ms > max_wait_ms");

    let should_flush = queue.should_flush();
    println!("AFTER: should_flush() = {}", should_flush);
    assert!(!should_flush, "Should NOT flush when len < min_batch_size");
    println!("PASSED: timeout does not trigger flush when min_batch_size not met");
}
```

**Edge Case 3: Batch::complete() length mismatch**
```rust
#[test]
#[should_panic(expected = "length mismatch")]
fn test_batch_complete_length_mismatch() {
    println!("BEFORE: creating batch with 2 items");
    let mut batch = Batch::new(ModelId::Semantic);

    let (req1, _rx1) = BatchRequest::new(ModelInput::text("a".to_string()), ModelId::Semantic);
    let (req2, _rx2) = BatchRequest::new(ModelInput::text("b".to_string()), ModelId::Semantic);
    batch.add(req1);
    batch.add(req2);
    println!("BEFORE: batch.len() = {}", batch.len());

    let results = vec![Ok(ModelEmbedding::stub(ModelId::Semantic))]; // Only 1 result!
    println!("AFTER: calling complete() with {} results (expected 2)", results.len());

    batch.complete(results); // Should panic
}
```

### 3. Manual Output Verification

**REQUIRED OUTPUTS TO VERIFY:**
1. `crates/context-graph-embeddings/src/batch/mod.rs` - EXISTS and compiles
2. `crates/context-graph-embeddings/src/batch/request.rs` - EXISTS and compiles
3. `crates/context-graph-embeddings/src/batch/queue.rs` - EXISTS and compiles
4. `crates/context-graph-embeddings/src/batch/batch.rs` - EXISTS and compiles
5. `crates/context-graph-embeddings/src/batch/stats.rs` - EXISTS and compiles
6. `src/lib.rs` contains `pub mod batch;` export
7. `src/error.rs` has `#[derive(Clone)]` on EmbeddingError
8. `cargo test --lib batch` passes ALL tests

---

## Implementation Notes

### Request Creation Pattern
```rust
impl BatchRequest {
    pub fn new(
        input: ModelInput,
        model_id: ModelId,
    ) -> (Self, oneshot::Receiver<EmbeddingResult<ModelEmbedding>>) {
        let (tx, rx) = oneshot::channel();
        let request = Self {
            id: Uuid::new_v4(),
            input,
            model_id,
            response_tx: tx,
            submitted_at: Instant::now(),
            priority: 0,
        };
        (request, rx)
    }

    pub fn estimated_tokens(&self) -> usize {
        match &self.input {
            ModelInput::Text { content, .. } => content.len() / 4 + 1,
            ModelInput::Code { content, .. } => content.len() / 3 + 1,
            ModelInput::Image { .. } => 256, // Fixed token estimate for images
            ModelInput::Audio { .. } => 512, // Fixed token estimate for audio
        }
    }
}
```

### Batch Flush Logic (respects min_batch_size)
```rust
impl BatchQueue {
    pub fn should_flush(&self) -> bool {
        if self.requests.is_empty() {
            return false;
        }

        // Immediate flush if reached max batch size
        if self.requests.len() >= self.config.max_batch_size {
            return true;
        }

        // Timeout flush only if min_batch_size is met
        if self.requests.len() >= self.config.min_batch_size {
            if let Some(oldest) = self.requests.front() {
                if oldest.elapsed().as_millis() as u64 >= self.config.max_wait_ms {
                    return true;
                }
            }
        }

        false
    }
}
```

### Batch Assembly with Length Sorting
```rust
impl BatchQueue {
    pub fn drain_batch(&mut self) -> Option<Batch> {
        if self.requests.is_empty() {
            return None;
        }

        let batch_size = self.requests.len().min(self.config.max_batch_size);
        let mut batch = Batch::new(self.model_id);

        let mut requests: Vec<BatchRequest> = self.requests
            .drain(..batch_size)
            .collect();

        // Sort by length for padding efficiency (config-driven)
        if self.config.sort_by_length {
            requests.sort_by_key(|r| r.estimated_tokens());
        }

        let avg_wait_us: u64 = requests.iter()
            .map(|r| r.elapsed().as_micros() as u64)
            .sum::<u64>() / requests.len() as u64;

        for request in requests {
            batch.add(request);
        }

        // Update statistics
        self.stats.record_batch(batch.len(), avg_wait_us);

        Some(batch)
    }
}
```

### Result Distribution (with length check)
```rust
impl Batch {
    pub fn complete(self, results: Vec<EmbeddingResult<ModelEmbedding>>) {
        assert_eq!(
            self.response_txs.len(),
            results.len(),
            "length mismatch: {} channels, {} results",
            self.response_txs.len(),
            results.len()
        );

        for (tx, result) in self.response_txs.into_iter().zip(results.into_iter()) {
            // Ignore send errors (receiver may have dropped)
            let _ = tx.send(result);
        }
    }

    pub fn fail(self, error: EmbeddingError) {
        for tx in self.response_txs {
            let _ = tx.send(Err(error.clone()));
        }
    }
}
```

---

## Required Modifications to Existing Files

### 1. Add Clone derive to EmbeddingError (error.rs)
```rust
// BEFORE:
#[derive(Debug, Error)]
pub enum EmbeddingError {

// AFTER:
#[derive(Debug, Clone, Error)]
pub enum EmbeddingError {
```

### 2. Add batch module to lib.rs
```rust
// Add after line 32:
pub mod batch;

// Add to re-exports (after line 77):
pub use batch::{Batch, BatchQueue, BatchRequest, BatchQueueStats, BatchQueueSummary};
```

---

## Sherlock Verification Checklist - COMPLETED

Final verification completed on 2026-01-01:

- [x] `cargo check` passes with no warnings
- [x] `cargo test --lib batch` - 35 tests PASS (91 batch-related total)
- [x] `cargo clippy -D warnings` - no clippy warnings
- [x] Files exist: batch/mod.rs (67 lines), batch/types.rs (1273 lines)
- [x] lib.rs exports batch module (`pub mod batch;` at line 27)
- [x] Batch::fail() uses String clone instead of EmbeddingError clone (valid alternative)
- [x] No duplicate BatchConfig definition
- [x] BatchQueue uses existing BatchConfig from config.rs
- [x] Tests use REAL ModelInput/ModelEmbedding types (NO MOCKS)
- [x] All batch types properly exported and functional
