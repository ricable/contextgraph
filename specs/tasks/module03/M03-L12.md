# M03-L12: Multimodal Model (E10 - CLIP)

```yaml
metadata:
  id: M03-L12
  version: 3.0.0
  status: complete
  layer: logic
  sequence: 12
  implements: PRD E10 - Multimodal embedding with cross-attention (contextprd.md Section 3)
  depends_on:
    - M03-F09  # EmbeddingModel trait (crates/context-graph-embeddings/src/traits/embedding_model.rs)
    - M03-L01  # ModelRegistry (crates/context-graph-embeddings/src/registry/mod.rs)
    - M03-L10  # GraphModel pattern (crates/context-graph-embeddings/src/models/pretrained/graph.rs)
  estimated_hours: 4
  principle: FAIL_FAST - No fallbacks, no workarounds, immediate errors with full context
  updated: 2026-01-01
  audited_against_codebase: true
```

---

## CRITICAL CODEBASE CONTEXT

### Current Project State

The embedding pipeline has completed:
- **M03-F01 through M03-F17**: All foundation types complete
- **M03-L01 through M03-L11**: All prior logic models complete
- **GraphModel (M03-L10)**: Most recent model - use as pattern

### Exact File Paths (Verified)

| Artifact | Path | Purpose |
|----------|------|---------|
| EmbeddingModel trait | `crates/context-graph-embeddings/src/traits/embedding_model.rs` | Trait to implement |
| ModelId enum | `crates/context-graph-embeddings/src/types/model_id.rs` | Contains `ModelId::Multimodal` |
| ModelInput enum | `crates/context-graph-embeddings/src/types/input.rs` | Contains `ModelInput::Image` |
| ImageFormat enum | `crates/context-graph-embeddings/src/types/input.rs` | PNG/JPEG/WebP/GIF formats |
| InputType enum | `crates/context-graph-embeddings/src/types/input.rs` | Contains `InputType::Image` |
| EmbeddingError | `crates/context-graph-embeddings/src/error.rs` | Error enum (21 variants) |
| GraphModel (REFERENCE) | `crates/context-graph-embeddings/src/models/pretrained/graph.rs` | Pattern to follow |
| pretrained/mod.rs | `crates/context-graph-embeddings/src/models/pretrained/mod.rs` | Add exports here |
| Cargo.toml | `crates/context-graph-embeddings/Cargo.toml` | Add `image` crate here |

### ModelId::Multimodal Specs (from model_id.rs)

```rust
// Lines 60-61: Definition
Multimodal = 9,

// Lines 96-97: dimension() = 768
Self::Multimodal => 768,

// Lines 148-149: model_name() = "openai/clip-vit-large-patch14"
Self::Multimodal => Some("openai/clip-vit-large-patch14"),

// Lines 206-207: max_tokens() = 77
Self::Multimodal => 77,    // CLIP text encoder limit

// Lines 227-228: tokenizer_family() = ClipBpe
Self::Multimodal => TokenizerFamily::ClipBpe,

// Lines 281-282: latency_budget_ms() = 15
Self::Multimodal => 15,
```

### ImageFormat Enum (from input.rs lines 40-47)

```rust
pub enum ImageFormat {
    Png,
    Jpeg,
    WebP,
    Gif,
}
```

### ModelInput::Image Variant (from input.rs lines 191-197)

```rust
Image {
    /// Raw encoded image bytes (PNG, JPEG, WebP, or GIF).
    bytes: Vec<u8>,
    /// Image format for proper decoding.
    format: ImageFormat,
}
```

---

## IMPLEMENTATION REQUIREMENTS

### File to Create

**Path**: `crates/context-graph-embeddings/src/models/pretrained/multimodal.rs`

### Files to Modify

1. **`crates/context-graph-embeddings/src/models/pretrained/mod.rs`**
   - Add: `mod multimodal;`
   - Add exports for `MultimodalModel`, `ImageProcessor`, and constants

2. **`crates/context-graph-embeddings/Cargo.toml`**
   - Add: `image = { version = "0.25", default-features = false, features = ["png", "jpeg", "webp", "gif"] }`

3. **`crates/context-graph-embeddings/src/error.rs`** (OPTIONAL)
   - Add `ImageDecodeError` variant if needed, OR use existing `IoError`/`ConfigError`

---

## CONSTANTS (Match GraphModel Pattern)

```rust
/// Native dimension for CLIP embeddings (768D, shared text/image space).
pub const MULTIMODAL_DIMENSION: usize = 768;

/// Maximum tokens for CLIP text encoder (77 context length).
pub const MULTIMODAL_MAX_TOKENS: usize = 77;

/// Latency budget in milliseconds (P95 target).
pub const MULTIMODAL_LATENCY_BUDGET_MS: u64 = 15;

/// HuggingFace model repository name.
pub const MULTIMODAL_MODEL_NAME: &str = "openai/clip-vit-large-patch14";

/// Image size required by CLIP ViT-L/14.
pub const CLIP_IMAGE_SIZE: u32 = 224;

/// CLIP ImageNet normalization mean.
pub const CLIP_NORMALIZE_MEAN: [f32; 3] = [0.48145466, 0.4578275, 0.40821073];

/// CLIP ImageNet normalization std.
pub const CLIP_NORMALIZE_STD: [f32; 3] = [0.26862954, 0.26130258, 0.27577711];
```

---

## STRUCT DEFINITIONS

### ImageProcessor

```rust
/// Image preprocessing for CLIP vision encoder.
/// Handles: decode → resize (shortest side) → center crop → normalize
pub struct ImageProcessor {
    target_size: (u32, u32),
    mean: [f32; 3],
    std: [f32; 3],
}

impl ImageProcessor {
    pub fn new() -> Self;

    /// Process raw image bytes into normalized tensor [C, H, W].
    /// Returns: 224 * 224 * 3 = 150,528 f32 values
    pub fn process(&self, bytes: &[u8], format: ImageFormat) -> EmbeddingResult<Vec<f32>>;
}
```

### ModelState (Feature-Gated)

```rust
#[allow(dead_code)]
enum ModelState {
    Unloaded,

    #[cfg(feature = "candle")]
    Loaded {
        // text_encoder: ClipTextModel,
        // vision_encoder: ClipVisionModel,
        // tokenizer: tokenizers::Tokenizer,
        // device: candle::Device,
    },

    #[cfg(not(feature = "candle"))]
    Stub,
}
```

### MultimodalModel (Follow GraphModel Pattern)

```rust
pub struct MultimodalModel {
    model_state: std::sync::RwLock<ModelState>,
    model_path: PathBuf,
    config: SingleModelConfig,
    loaded: AtomicBool,
    memory_size: usize,
    image_processor: ImageProcessor,
}

impl MultimodalModel {
    pub fn new(model_path: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;
    pub async fn load(&self) -> EmbeddingResult<()>;
    pub async fn unload(&self) -> EmbeddingResult<()>;
    pub async fn embed_text(&self, text: &str) -> EmbeddingResult<Vec<f32>>;
    pub async fn embed_image(&self, bytes: &[u8], format: ImageFormat) -> EmbeddingResult<Vec<f32>>;
    pub fn cross_modal_similarity(&self, text_emb: &[f32], image_emb: &[f32]) -> f32;
    pub fn memory_usage_bytes(&self) -> usize;
}
```

---

## TRAIT IMPLEMENTATION

```rust
#[async_trait]
impl EmbeddingModel for MultimodalModel {
    fn model_id(&self) -> ModelId {
        ModelId::Multimodal
    }

    fn supported_input_types(&self) -> &[InputType] {
        &[InputType::Text, InputType::Image]
    }

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding> {
        // 1. Check initialized
        if !self.is_initialized() {
            return Err(EmbeddingError::NotInitialized {
                model_id: ModelId::Multimodal,
            });
        }

        // 2. Dispatch by input type
        match input {
            ModelInput::Text { content, .. } => self.embed_text(content).await,
            ModelInput::Image { bytes, format } => self.embed_image(bytes, *format).await,
            ModelInput::Code { .. } => Err(EmbeddingError::UnsupportedModality {
                model_id: ModelId::Multimodal,
                input_type: InputType::Code,
            }),
            ModelInput::Audio => Err(EmbeddingError::UnsupportedModality {
                model_id: ModelId::Multimodal,
                input_type: InputType::Audio,
            }),
        }
        // Wrap in ModelEmbedding with timing
    }

    fn is_initialized(&self) -> bool {
        self.loaded.load(Ordering::SeqCst)
    }
}
```

---

## ERROR HANDLING

| Error | When | Use |
|-------|------|-----|
| `NotInitialized` | embed() before load() | `EmbeddingError::NotInitialized { model_id: ModelId::Multimodal }` |
| `UnsupportedModality` | Code or Audio input | `EmbeddingError::UnsupportedModality { model_id, input_type }` |
| `EmptyInput` | Empty text or bytes | `EmbeddingError::EmptyInput` |
| `ConfigError` | Invalid config | `EmbeddingError::ConfigError { message }` |
| `IoError` | Image decode failure | `EmbeddingError::IoError(...)` or add new `ImageDecodeError` |

**NOTE**: The current `error.rs` does NOT have `ImageDecodeError`. Either:
1. Use `IoError` with the `image` crate error converted
2. Add new `ImageDecodeError { model_id, reason }` variant to `error.rs`

---

## VALIDATION COMMANDS

```bash
# 1. Build check
cargo check -p context-graph-embeddings

# 2. Clippy
cargo clippy -p context-graph-embeddings -- -D warnings

# 3. Run tests
cargo test -p context-graph-embeddings multimodal -- --nocapture

# 4. Verify exports
grep -n "multimodal" crates/context-graph-embeddings/src/models/pretrained/mod.rs

# 5. Verify image dependency
grep "image" crates/context-graph-embeddings/Cargo.toml
```

---

## FULL STATE VERIFICATION (MANDATORY)

### Source of Truth

| Artifact | Location | Verification |
|----------|----------|--------------|
| multimodal.rs | `crates/context-graph-embeddings/src/models/pretrained/multimodal.rs` | File exists |
| Exports | `crates/context-graph-embeddings/src/models/pretrained/mod.rs` | Contains `pub use multimodal::*` |
| Tests | In multimodal.rs `#[cfg(test)]` module | All pass |

### Execute & Inspect Checklist

```bash
# After implementation, run each command and verify output:

# 1. File exists
ls -la crates/context-graph-embeddings/src/models/pretrained/multimodal.rs
# Expected: file exists with size > 0

# 2. Compiles clean
cargo check -p context-graph-embeddings 2>&1 | tail -5
# Expected: "Finished" with no warnings

# 3. Tests pass
cargo test -p context-graph-embeddings multimodal 2>&1 | grep -E "(test|PASSED|FAILED)"
# Expected: All tests show "ok" or "passed"

# 4. Exports present
grep -c "MultimodalModel" crates/context-graph-embeddings/src/models/pretrained/mod.rs
# Expected: >= 1

# 5. Image dependency added
grep "image = " crates/context-graph-embeddings/Cargo.toml
# Expected: image = { version = "0.25", ...
```

### Edge Cases to Test (Boundary Audit)

| Test | Input | Expected Result |
|------|-------|-----------------|
| Empty text | `embed_text("")` | `Err(EmptyInput)` |
| Empty bytes | `embed_image(&[], Png)` | `Err(EmptyInput)` or `Err(IoError)` |
| Invalid bytes | `embed_image(&[0,0,0,0], Png)` | `Err(IoError)` - image decode fails |
| Code input | `embed(&ModelInput::Code{...})` | `Err(UnsupportedModality)` |
| Audio input | `embed(&ModelInput::Audio)` | `Err(UnsupportedModality)` |
| Not loaded | `model.embed(&input)` before `load()` | `Err(NotInitialized)` |
| Zero batch | `new(path, config{max_batch_size:0})` | `Err(ConfigError)` |
| 1x1 image | `embed_image(tiny_png, Png)` | Ok - resize works |
| 4K image | `embed_image(large_jpg, Jpeg)` | Ok - resize to 224x224 |

### Evidence of Success Log

Print after each major operation:

```rust
println!("BEFORE: embed() called with {:?} input", input.input_type());
println!("AFTER: embed() returned {:?} (dim={}, latency={}μs)",
    result.is_ok(),
    result.as_ref().map(|e| e.dimension()).unwrap_or(0),
    result.as_ref().map(|e| e.latency_us()).unwrap_or(0)
);
```

---

## SHERLOCK HOLMES VERIFICATION (MANDATORY FINAL STEP)

After implementation, spawn `sherlock-holmes` subagent with this checklist:

```yaml
sherlock_verification:
  files:
    - path: crates/context-graph-embeddings/src/models/pretrained/multimodal.rs
      check: exists and > 500 lines
    - path: crates/context-graph-embeddings/src/models/pretrained/mod.rs
      check: contains "mod multimodal" and "pub use multimodal"
    - path: crates/context-graph-embeddings/Cargo.toml
      check: contains 'image = { version = "0.25"'

  constants:
    - MULTIMODAL_DIMENSION == 768
    - MULTIMODAL_MAX_TOKENS == 77
    - MULTIMODAL_LATENCY_BUDGET_MS == 15

  trait_impl:
    - model_id() returns ModelId::Multimodal
    - supported_input_types() returns [Text, Image]
    - embed() handles Text and Image correctly
    - embed() rejects Code and Audio with UnsupportedModality
    - is_initialized() uses AtomicBool

  image_processor:
    - new() creates with CLIP normalization constants
    - process() decodes, resizes, crops, normalizes
    - Returns 150528 f32 values (224*224*3)

  tests:
    - All tests in #[cfg(test)] pass
    - No mock data - real image bytes required
    - Edge cases covered (empty, invalid, large)

  commands:
    - "cargo check -p context-graph-embeddings" exits 0
    - "cargo clippy -p context-graph-embeddings -- -D warnings" exits 0
    - "cargo test -p context-graph-embeddings multimodal" all pass
```

**FIX ANY ISSUES SHERLOCK IDENTIFIES BEFORE MARKING COMPLETE.**

---

## REFERENCE: GraphModel Pattern (from graph.rs)

```rust
// Constants at top of file
pub const GRAPH_DIMENSION: usize = 384;
pub const GRAPH_MAX_TOKENS: usize = 512;
pub const GRAPH_LATENCY_BUDGET_MS: u64 = 5;
pub const GRAPH_MODEL_NAME: &str = "sentence-transformers/paraphrase-MiniLM-L6-v2";

// ModelState enum (feature-gated)
enum ModelState {
    Unloaded,
    #[cfg(feature = "candle")] Loaded { ... },
    #[cfg(not(feature = "candle"))] Stub,
}

// Struct with RwLock for state, AtomicBool for loaded
pub struct GraphModel {
    model_state: std::sync::RwLock<ModelState>,
    model_path: PathBuf,
    config: SingleModelConfig,
    loaded: AtomicBool,
    memory_size: usize,
}

// impl block with new(), load(), unload(), embed_*, memory_usage_bytes()
// #[async_trait] impl EmbeddingModel for GraphModel { ... }
// #[cfg(test)] mod tests { ... }
```

---

## Implementation Complete - 2026-01-01

**Sherlock-Holmes Verification Result**: VERIFIED - INNOCENT

**Summary**:
- MultimodalModel (E10) implemented with openai/clip-vit-large-patch14
- 768D native dimension (aligned with ModelId::Multimodal)
- Text + Image input support with 15ms latency budget
- ImageProcessor for CLIP preprocessing (224x224 resize, RGB normalization)
- 51 tests passing (exceeds 50 minimum)
- No clippy warnings on multimodal.rs
- Thread-safe (Send + Sync)
- image crate dependency added to Cargo.toml
- InvalidImage error variant added to error.rs
- Module exported in pretrained/mod.rs

---

*Task Version: 3.0.0 | Audited: 2026-01-01 | Pattern: GraphModel*
