# M03-L14: Late-Interaction Model (E12 - ColBERT)

```xml
<task_spec id="M03-L14" version="2.1">
<metadata>
  <title>Implement Late-Interaction Model using ColBERT</title>
  <status>complete</status>
  <layer>logic</layer>
  <sequence>14</sequence>
  <implements>PRD E12 - Late-Interaction embedding with ColBERT MaxSim</implements>
  <depends_on>M03-F09, M03-L01</depends_on>
  <estimated_hours>4</estimated_hours>
  <reference_implementation>crates/context-graph-embeddings/src/models/pretrained/entity.rs</reference_implementation>
</metadata>

<critical_context>
<!-- ESSENTIAL: Read this section FIRST before implementing -->

LOAD/UNLOAD ARE NOT TRAIT METHODS:
- The EmbeddingModel trait has only 4 REQUIRED methods:
  1. model_id() -> ModelId
  2. supported_input_types() -> &[InputType]
  3. is_initialized() -> bool
  4. embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>
- load() and unload() are implemented DIRECTLY on the model struct
- See entity.rs lines 306-351 for the pattern

STUB MODE PATTERN (from entity.rs lines 373-392):
```rust
#[cfg(not(feature = "candle"))]
fn generate_deterministic_embedding(text: &str) -> Vec<f32> {
    let hash = xxhash_rust::xxh64::xxh64(text.as_bytes(), 0);
    let mut rng = StdRng::seed_from_u64(hash);
    let mut vector: Vec<f32> = (0..DIMENSION)
        .map(|_| rng.gen_range(-1.0..1.0))
        .collect();
    // L2 normalize
    let norm: f32 = vector.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > f32::EPSILON {
        for v in &mut vector { *v /= norm; }
    }
    vector
}
```

TOKEN GENERATION FOR STUB MODE:
For ColBERT, generate variable-length token embeddings based on input:
```rust
fn generate_stub_token_embeddings(text: &str) -> TokenEmbeddings {
    // Approximate tokenization: split on whitespace + special chars
    let tokens: Vec<String> = text.split_whitespace()
        .take(LATE_INTERACTION_MAX_TOKENS)
        .map(|s| s.to_string())
        .collect();

    // Generate deterministic 128D vector per token
    let vectors: Vec<Vec<f32>> = tokens.iter()
        .map(|tok| generate_deterministic_embedding(tok))
        .collect();

    let mask = vec![true; tokens.len()];
    TokenEmbeddings { vectors, tokens, mask }
}
```

THREAD SAFETY (from entity.rs lines 469-471):
```rust
// SAFETY: RwLock provides interior mutability with proper synchronization
unsafe impl Send for LateInteractionModel {}
unsafe impl Sync for LateInteractionModel {}
```
</critical_context>

<context>
Implement late-interaction embedding using colbert-ir/colbertv2.0.
ColBERT produces per-token embeddings (128D each) enabling fine-grained
matching via MaxSim scoring. Unlike single-vector models, ColBERT preserves
token-level information for more expressive retrieval.

Output for fusion: 128D pooled vector.
Storage: Full per-token embeddings for MaxSim retrieval.
Latency target: less than 8ms.

CRITICAL DESIGN PRINCIPLES:
- NO FALLBACKS: All errors must propagate via EmbeddingError
- FAIL FAST: Invalid state triggers immediate error with context
- NO MOCK DATA: Tests use deterministic embeddings, never random/mock data
- All outputs MUST be verified to physically exist
</context>

<codebase_state>
  <!-- VERIFIED against actual codebase 2026-01-01 -->
  <file_exists path="crates/context-graph-embeddings/src/models/pretrained/mod.rs">
    Current exports: causal, code, entity, graph, multimodal, semantic, sparse
    MISSING: late_interaction module (task creates this)
  </file_exists>
  <file_exists path="crates/context-graph-embeddings/src/types/model_id.rs">
    ModelId::LateInteraction variant EXISTS (line 65)
    dimension() returns 128 (line 99)
    directory_name() returns "late-interaction" (line 174)
    latency_budget_ms() returns 8 (line 284)
    tokenizer_family() returns BertWordpiece (line 230)
  </file_exists>
  <file_exists path="crates/context-graph-embeddings/src/error.rs">
    EmbeddingError enum with 17 variants for fail-fast behavior
    Key variants: NotInitialized, InvalidDimension, EmptyInput, UnsupportedModality, InputTooLong
    EmbeddingResult<T> type alias available
  </file_exists>
  <file_exists path="crates/context-graph-embeddings/src/traits/embedding_model.rs">
    EmbeddingModel trait with 4 REQUIRED methods (see critical_context)
    DEFAULT implementations for: dimension(), projected_dimension(), max_tokens(),
      latency_budget_ms(), is_pretrained(), embed_batch()
    NOTE: load()/unload() are NOT trait methods - implement on struct directly
  </file_exists>
  <file_exists path="crates/context-graph-embeddings/src/traits/mod.rs">
    Exports: EmbeddingModel, ModelFactory, DevicePlacement, QuantizationMode,
      SingleModelConfig, MEMORY_ESTIMATES, TOTAL_MEMORY_ESTIMATE, get_memory_estimate
  </file_exists>
</codebase_state>

<definition_of_done>
  <signatures>
```rust
// File: crates/context-graph-embeddings/src/models/pretrained/late_interaction.rs

use std::path::{Path, PathBuf};
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::RwLock;
use async_trait::async_trait;
use crate::error::{EmbeddingError, EmbeddingResult};
use crate::traits::{EmbeddingModel, SingleModelConfig};
use crate::types::{InputType, ModelEmbedding, ModelId, ModelInput};

/// Native dimension for ColBERT per-token embeddings.
pub const LATE_INTERACTION_DIMENSION: usize = 128;

/// Maximum tokens for ColBERT (standard BERT-family limit).
pub const LATE_INTERACTION_MAX_TOKENS: usize = 512;

/// Latency budget in milliseconds (P95 target).
pub const LATE_INTERACTION_LATENCY_BUDGET_MS: u64 = 8;

/// HuggingFace model repository name.
pub const LATE_INTERACTION_MODEL_NAME: &str = "colbert-ir/colbertv2.0";

/// Per-token embeddings from ColBERT.
///
/// Each token in the input produces a 128D embedding vector.
/// The mask indicates which tokens are valid (non-padding).
#[derive(Debug, Clone)]
pub struct TokenEmbeddings {
    /// Token vectors [num_tokens, 128] - each inner Vec is 128D
    pub vectors: Vec<Vec<f32>>,
    /// Token strings for debugging/analysis
    pub tokens: Vec<String>,
    /// Mask for valid tokens (excludes padding)
    pub mask: Vec<bool>,
}

impl TokenEmbeddings {
    /// Create new token embeddings with validation.
    ///
    /// # Errors
    /// - `EmbeddingError::InvalidDimension` if any vector is not 128D
    /// - `EmbeddingError::EmptyInput` if vectors is empty
    pub fn new(
        vectors: Vec<Vec<f32>>,
        tokens: Vec<String>,
        mask: Vec<bool>,
    ) -> EmbeddingResult<Self>;

    /// Count of valid (non-padding) tokens.
    pub fn valid_token_count(&self) -> usize;
}

/// Internal state that varies based on feature flags.
#[allow(dead_code)]
enum ModelState {
    Unloaded,
    #[cfg(feature = "candle")]
    Loaded { /* candle fields */ },
    #[cfg(not(feature = "candle"))]
    Stub,
}

/// Late-interaction embedding model using colbert-ir/colbertv2.0.
pub struct LateInteractionModel {
    model_state: RwLock<ModelState>,
    model_path: PathBuf,
    config: SingleModelConfig,
    loaded: AtomicBool,
    memory_size: usize,
}

impl LateInteractionModel {
    /// Create a new LateInteractionModel instance.
    /// Model is NOT loaded after construction. Call `load()` before `embed()`.
    pub fn new(model_path: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Load model weights into memory. (NOT a trait method)
    pub async fn load(&self) -> EmbeddingResult<()>;

    /// Unload model weights from memory. (NOT a trait method)
    pub async fn unload(&self) -> EmbeddingResult<()>;

    /// Get full per-token embeddings for MaxSim scoring.
    pub async fn embed_tokens(&self, text: &str) -> EmbeddingResult<TokenEmbeddings>;

    /// Pool token embeddings to single 128D vector for fusion.
    pub fn pool_tokens(&self, token_embs: &TokenEmbeddings) -> Vec<f32>;

    /// ColBERT MaxSim scoring: score = sum_i max_j cos(q_i, d_j)
    pub fn maxsim_score(
        query_tokens: &TokenEmbeddings,
        doc_tokens: &TokenEmbeddings,
    ) -> f32;

    /// Batch MaxSim for efficient retrieval.
    pub fn batch_maxsim(
        query_tokens: &TokenEmbeddings,
        doc_batch: &[TokenEmbeddings],
    ) -> Vec<f32>;
}

// SAFETY: RwLock provides interior mutability with proper synchronization
unsafe impl Send for LateInteractionModel {}
unsafe impl Sync for LateInteractionModel {}

#[async_trait]
impl EmbeddingModel for LateInteractionModel {
    // REQUIRED (4 methods):
    fn model_id(&self) -> ModelId { ModelId::LateInteraction }
    fn supported_input_types(&self) -> &[InputType] { &[InputType::Text] }
    fn is_initialized(&self) -> bool;
    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;

    // OVERRIDE DEFAULTS:
    fn dimension(&self) -> usize { 128 }
    fn projected_dimension(&self) -> usize { 128 }
    fn max_tokens(&self) -> usize { 512 }
    fn latency_budget_ms(&self) -> u32 { 8 }
    fn is_pretrained(&self) -> bool { true }
}
```
  </signatures>

  <constraints>
    <constraint>Uses candle-transformers for inference (feature = "candle")</constraint>
    <constraint>Model loaded from models/late-interaction/ directory (hyphenated)</constraint>
    <constraint>Produces per-token embeddings (128D each)</constraint>
    <constraint>dimension() returns 128 (per-token native dimension)</constraint>
    <constraint>projected_dimension() returns 128 (pooled for fusion)</constraint>
    <constraint>max_tokens() returns 512</constraint>
    <constraint>latency_budget_ms() returns 8</constraint>
    <constraint>MaxSim scoring for late interaction retrieval</constraint>
    <constraint>Stores full token embeddings for retrieval use</constraint>
    <constraint>Pools to single 128D vector for FuseMoE input</constraint>
    <constraint>Thread-safe with Send + Sync bounds</constraint>
    <constraint>All token vectors MUST be L2 normalized</constraint>
    <constraint>load()/unload() are struct methods, NOT trait methods</constraint>
    <constraint>NO fallback behavior - all errors propagate via EmbeddingError</constraint>
    <constraint>NO mock data in tests - use deterministic hash-based generation</constraint>
  </constraints>
</definition_of_done>

<files_to_modify>
  <file action="create">crates/context-graph-embeddings/src/models/pretrained/late_interaction.rs</file>
  <file action="modify">crates/context-graph-embeddings/src/models/pretrained/mod.rs</file>
</files_to_modify>

<mod_rs_update>
Add to crates/context-graph-embeddings/src/models/pretrained/mod.rs:

```rust
mod late_interaction;

pub use late_interaction::{
    LateInteractionModel,
    TokenEmbeddings,
    LATE_INTERACTION_DIMENSION,
    LATE_INTERACTION_LATENCY_BUDGET_MS,
    LATE_INTERACTION_MAX_TOKENS,
    LATE_INTERACTION_MODEL_NAME,
};
```
</mod_rs_update>

<validation_criteria>
  <criterion>cargo check passes with no errors</criterion>
  <criterion>cargo test --lib passes (all tests green)</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>LateInteractionModel::new() creates valid instance</criterion>
  <criterion>Model is_initialized() returns false before load()</criterion>
  <criterion>Model is_initialized() returns true after load()</criterion>
  <criterion>embed() before load() returns EmbeddingError::NotInitialized</criterion>
  <criterion>embed_tokens() produces variable-length token embeddings</criterion>
  <criterion>Each token embedding is exactly 128D</criterion>
  <criterion>pool_tokens() produces single 128D L2-normalized vector</criterion>
  <criterion>maxsim_score() computes correct late-interaction score</criterion>
  <criterion>All token vectors are L2 normalized (norm approx 1.0)</criterion>
  <criterion>Same input produces identical embeddings (deterministic)</criterion>
  <criterion>Different inputs produce different embeddings</criterion>
  <criterion>Latency under 8ms for typical inputs (stub mode: under 50ms)</criterion>
  <criterion>model_id() returns ModelId::LateInteraction</criterion>
  <criterion>pretrained/mod.rs exports LateInteractionModel</criterion>
</validation_criteria>

<full_state_verification>
  <source_of_truth>
    <!-- These files define the contract - implementation MUST match -->
    <file path="crates/context-graph-embeddings/src/types/model_id.rs">
      ModelId::LateInteraction at line 65
      dimension() returns 128 at line 99
      latency_budget_ms() returns 8 at line 284
      tokenizer_family() returns BertWordpiece at line 230
      directory_name() returns "late-interaction" at line 174
    </file>
    <file path="specs/constitution.yaml">
      E12_LateInteraction: { dim: "128D/tok", math: ColBERT_MaxSim, hw: CUDA_Tile, lat: "less than 8ms" }
    </file>
    <file path="specs/contextprd.md">
      E12 Late-Interaction: colbert-ir/colbertv2.0 for token-level matching
    </file>
  </source_of_truth>

  <edge_case_audits>
    <!-- ALL 8 tests MUST print BEFORE/AFTER state for verification -->
    <test name="test_edge_case_1_empty_text_content">
      BEFORE: Create ModelInput with empty string
      OPERATION: ModelInput::text("")
      AFTER: Must return Err(EmbeddingError::EmptyInput) - print the error

      BEFORE: Create ModelInput with whitespace-only " \t\n "
      OPERATION: ModelInput::text("   ") then model.embed()
      AFTER: Must return Ok(embedding) - print vector.len() and first 3 values
    </test>
    <test name="test_edge_case_2_long_input">
      BEFORE: Create text with exactly 512 tokens (print token count)
      OPERATION: model.embed_tokens(long_text)
      AFTER: Must succeed - print TokenEmbeddings.vectors.len()

      BEFORE: Create text with 600 tokens (exceeds limit, print count)
      OPERATION: model.embed_tokens(too_long_text)
      AFTER: Must return Err(EmbeddingError::InputTooLong) - print error
    </test>
    <test name="test_edge_case_3_unsupported_modality_code">
      BEFORE: Create ModelInput::code("fn main() {}")
      OPERATION: model.embed(&code_input)
      AFTER: Must return Err(EmbeddingError::UnsupportedModality) - print error
    </test>
    <test name="test_edge_case_4_unsupported_modality_image">
      BEFORE: Create ModelInput::Image with dummy bytes
      OPERATION: model.embed(&image_input)
      AFTER: Must return Err(EmbeddingError::UnsupportedModality) - print error
    </test>
    <test name="test_edge_case_5_special_characters">
      BEFORE: text = "Unicode: test cafe emoji fire Chinese Japanese"
      OPERATION: model.embed_tokens(text)
      AFTER: Print each token, verify no NaN/Infinity in any vector
    </test>
    <test name="test_edge_case_6_single_token">
      BEFORE: text = "hello"
      OPERATION: model.embed_tokens(text)
      AFTER: Print vectors.len() (must be 1), print token, verify 128D
    </test>
    <test name="test_edge_case_7_maxsim_identical_docs">
      BEFORE: Create TokenEmbeddings for "test query"
      OPERATION: LateInteractionModel::maxsim_score(&query, &query)
      AFTER: Print score (must be maximum possible = sum of 1.0 per token)
    </test>
    <test name="test_edge_case_8_pool_single_token">
      BEFORE: Create TokenEmbeddings with single token
      OPERATION: model.pool_tokens(&single_token_embs)
      AFTER: Print pooled.len() (128), verify L2 norm approx 1.0
    </test>
  </edge_case_audits>

  <evidence_of_success>
    <!-- Test MUST print these values for human/AI verification -->
    <evidence>model_id = ModelId::LateInteraction (E12)</evidence>
    <evidence>dimension = 128</evidence>
    <evidence>projected_dimension = 128</evidence>
    <evidence>max_tokens = 512</evidence>
    <evidence>latency_budget_ms = 8</evidence>
    <evidence>is_pretrained = true</evidence>
    <evidence>tokenizer_family = BertWordpiece</evidence>
    <evidence>TokenEmbeddings.vectors.len() = [actual token count]</evidence>
    <evidence>Each vector.len() = 128</evidence>
    <evidence>L2 norm of each token approx 1.0</evidence>
    <evidence>pool_tokens() output L2 norm approx 1.0</evidence>
    <evidence>maxsim_score() > 0 for related query/doc</evidence>
    <evidence>Latency measurement in microseconds</evidence>
  </evidence_of_success>

  <physical_verification>
    <!-- After implementation, VERIFY these files exist -->
    <file_must_exist>crates/context-graph-embeddings/src/models/pretrained/late_interaction.rs</file_must_exist>
    <export_must_exist module="pretrained/mod.rs">LateInteractionModel</export_must_exist>
    <export_must_exist module="pretrained/mod.rs">TokenEmbeddings</export_must_exist>
    <export_must_exist module="pretrained/mod.rs">LATE_INTERACTION_DIMENSION</export_must_exist>
    <test_must_pass>cargo test late_interaction --lib</test_must_pass>
  </physical_verification>
</full_state_verification>

<test_implementation_pattern>
Follow entity.rs test structure (lines 473-1119):

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    fn create_test_model() -> LateInteractionModel {
        LateInteractionModel::new(
            Path::new("models/late-interaction"),
            SingleModelConfig::default(),
        ).expect("Failed to create LateInteractionModel")
    }

    async fn create_and_load_model() -> LateInteractionModel {
        let model = create_test_model();
        model.load().await.expect("Failed to load model");
        model
    }

    // ==================== Construction Tests ====================
    #[test]
    fn test_new_creates_unloaded_model() {
        let model = create_test_model();
        println!("BEFORE: model created");
        println!("AFTER: is_initialized = {}", model.is_initialized());
        assert!(!model.is_initialized());
    }

    // ==================== Trait Implementation Tests ====================
    #[test]
    fn test_model_id() {
        let model = create_test_model();
        println!("model_id = {:?}", model.model_id());
        assert_eq!(model.model_id(), ModelId::LateInteraction);
    }

    #[test]
    fn test_dimension_is_128() {
        let model = create_test_model();
        println!("dimension = {}", model.dimension());
        assert_eq!(model.dimension(), 128);
    }

    // ==================== State Transition Tests ====================
    #[tokio::test]
    async fn test_load_sets_initialized() {
        let model = create_test_model();
        println!("BEFORE load: is_initialized = {}", model.is_initialized());
        model.load().await.expect("load failed");
        println!("AFTER load: is_initialized = {}", model.is_initialized());
        assert!(model.is_initialized());
    }

    #[tokio::test]
    async fn test_embed_before_load_fails() {
        let model = create_test_model();
        let input = ModelInput::text("test").expect("Input");
        println!("BEFORE: calling embed without load");
        let result = model.embed(&input).await;
        println!("AFTER: result = {:?}", result);
        assert!(matches!(result, Err(EmbeddingError::NotInitialized { .. })));
    }

    // ==================== Token Embedding Tests ====================
    #[tokio::test]
    async fn test_embed_tokens_produces_per_token_vectors() {
        let model = create_and_load_model().await;
        let tokens = model.embed_tokens("hello world test").await.expect("embed_tokens");
        println!("Input: 'hello world test'");
        println!("TokenEmbeddings.vectors.len() = {}", tokens.vectors.len());
        println!("TokenEmbeddings.tokens = {:?}", tokens.tokens);
        assert!(tokens.vectors.len() >= 3);
    }

    #[tokio::test]
    async fn test_each_token_is_128d() {
        let model = create_and_load_model().await;
        let tokens = model.embed_tokens("hello world").await.expect("embed_tokens");
        for (i, vec) in tokens.vectors.iter().enumerate() {
            println!("Token {} '{}': dimension = {}", i, tokens.tokens[i], vec.len());
            assert_eq!(vec.len(), 128);
        }
    }

    // ==================== MaxSim Tests ====================
    #[tokio::test]
    async fn test_maxsim_score_identical_returns_max() {
        let model = create_and_load_model().await;
        let tokens = model.embed_tokens("test query").await.expect("embed_tokens");
        let score = LateInteractionModel::maxsim_score(&tokens, &tokens);
        let expected_max = tokens.valid_token_count() as f32;
        println!("MaxSim(query, query) = {} (expected max: {})", score, expected_max);
        assert!((score - expected_max).abs() < 0.01);
    }

    // ==================== Pooling Tests ====================
    #[tokio::test]
    async fn test_pool_tokens_produces_128d() {
        let model = create_and_load_model().await;
        let tokens = model.embed_tokens("test input").await.expect("embed_tokens");
        let pooled = model.pool_tokens(&tokens);
        println!("pool_tokens output len = {}", pooled.len());
        assert_eq!(pooled.len(), 128);
    }

    #[tokio::test]
    async fn test_pool_tokens_l2_normalized() {
        let model = create_and_load_model().await;
        let tokens = model.embed_tokens("test input").await.expect("embed_tokens");
        let pooled = model.pool_tokens(&tokens);
        let norm: f32 = pooled.iter().map(|x| x * x).sum::<f32>().sqrt();
        println!("pool_tokens L2 norm = {}", norm);
        assert!((norm - 1.0).abs() < 0.001);
    }

    // ==================== Edge Case Tests ====================
    // See edge_case_audits above - implement all 8

    // ==================== Evidence of Success ====================
    #[tokio::test]
    async fn test_evidence_of_success() {
        println!("\n========================================");
        println!("M03-L14 EVIDENCE OF SUCCESS");
        println!("========================================\n");

        let model = create_and_load_model().await;

        println!("1. MODEL METADATA:");
        println!("   model_id = {:?}", model.model_id());
        println!("   dimension = {}", model.dimension());
        println!("   projected_dimension = {}", model.projected_dimension());
        println!("   max_tokens = {}", model.max_tokens());
        println!("   latency_budget_ms = {}", model.latency_budget_ms());
        println!("   is_pretrained = {}", model.is_pretrained());
        println!("   supported_input_types = {:?}", model.supported_input_types());

        let start = std::time::Instant::now();
        let tokens = model.embed_tokens("ColBERT late interaction test").await.expect("embed");
        let latency = start.elapsed();

        println!("\n2. TOKEN EMBEDDINGS:");
        println!("   tokens.vectors.len() = {}", tokens.vectors.len());
        println!("   tokens.tokens = {:?}", tokens.tokens);
        for (i, vec) in tokens.vectors.iter().enumerate() {
            let norm: f32 = vec.iter().map(|x| x * x).sum::<f32>().sqrt();
            println!("   Token {}: dim={}, L2_norm={:.6}", i, vec.len(), norm);
        }

        println!("\n3. POOLING:");
        let pooled = model.pool_tokens(&tokens);
        let pooled_norm: f32 = pooled.iter().map(|x| x * x).sum::<f32>().sqrt();
        println!("   pooled.len() = {}", pooled.len());
        println!("   pooled L2 norm = {:.6}", pooled_norm);

        println!("\n4. MAXSIM:");
        let score = LateInteractionModel::maxsim_score(&tokens, &tokens);
        println!("   MaxSim(self, self) = {:.6}", score);

        println!("\n5. LATENCY:");
        println!("   embed_tokens latency = {:?}", latency);

        println!("\n========================================");
        println!("ALL CHECKS PASSED");
        println!("========================================\n");
    }
}
```
</test_implementation_pattern>

<sherlock_verification>
After implementation is complete, run sherlock-holmes subagent to verify:

1. **File Existence**: late_interaction.rs exists at correct path
2. **Module Export**: pretrained/mod.rs exports all required symbols
3. **Trait Implementation**: EmbeddingModel properly implemented (4 required methods)
4. **load()/unload()**: Verified as struct methods, NOT trait methods
5. **Test Coverage**: All 8 edge cases have passing tests with BEFORE/AFTER printing
6. **Source of Truth Alignment**: Dimensions, latency match model_id.rs
7. **No Mock Data**: Verify no random/mock patterns - only xxhash64 deterministic
8. **No Fallbacks**: Verify all errors propagate, no silent failures
9. **Thread Safety**: unsafe impl Send + Sync present with safety comment
10. **Build Success**: cargo check, cargo test, cargo clippy all pass
</sherlock_verification>
</task_spec>
```

---

## Implementation Notes

### Model Details
- **HuggingFace Repo**: `colbert-ir/colbertv2.0`
- **Base Model**: BERT-based with linear projection
- **Token Embedding Dimension**: 128 (projected from BERT's 768)
- **Max Sequence Length**: 512 tokens
- **Tokenizer**: BertWordpiece (shared family with Semantic, Sparse, etc.)

### Directory Structure
```
models/late-interaction/    # Note: hyphenated per model_id.rs:174
├── config.json
├── tokenizer.json
├── model.safetensors
└── tokenizer_config.json
```

### ColBERT MaxSim Scoring
The key innovation of ColBERT is the MaxSim scoring function:

```rust
impl LateInteractionModel {
    pub fn maxsim_score(
        query_tokens: &TokenEmbeddings,
        doc_tokens: &TokenEmbeddings,
    ) -> f32 {
        let mut total_score = 0.0;

        for (i, q_vec) in query_tokens.vectors.iter().enumerate() {
            if !query_tokens.mask[i] {
                continue;  // Skip padding
            }

            // Find maximum similarity to any document token
            let max_sim = doc_tokens.vectors.iter()
                .enumerate()
                .filter(|(j, _)| doc_tokens.mask[*j])
                .map(|(_, d_vec)| cosine_similarity(q_vec, d_vec))
                .fold(f32::NEG_INFINITY, f32::max);

            total_score += max_sim;
        }

        total_score
    }
}
```

### Token Pooling for Fusion
For the FuseMoE fusion layer, we need a single 128D vector:

```rust
impl LateInteractionModel {
    pub fn pool_tokens(&self, token_embs: &TokenEmbeddings) -> Vec<f32> {
        // Mean pooling over valid tokens
        let valid_vectors: Vec<&Vec<f32>> = token_embs.vectors.iter()
            .zip(token_embs.mask.iter())
            .filter(|(_, &valid)| valid)
            .map(|(v, _)| v)
            .collect();

        if valid_vectors.is_empty() {
            return vec![0.0f32; LATE_INTERACTION_DIMENSION];
        }

        let n = valid_vectors.len() as f32;
        let mut pooled = vec![0.0f32; LATE_INTERACTION_DIMENSION];

        for v in valid_vectors {
            for (i, val) in v.iter().enumerate() {
                pooled[i] += val / n;
            }
        }

        // L2 normalize
        let norm: f32 = pooled.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > f32::EPSILON {
            pooled.iter_mut().for_each(|x| *x /= norm);
        }

        pooled
    }
}
```

### Stub Implementation Pattern (from entity.rs lines 373-392)
```rust
#[cfg(not(feature = "candle"))]
fn generate_deterministic_embedding(text: &str) -> Vec<f32> {
    // Use xxhash64 for deterministic seeding
    let hash = xxhash_rust::xxh64::xxh64(text.as_bytes(), 0);
    let mut rng = StdRng::seed_from_u64(hash);

    // Generate random values at native dimension (128D)
    let mut vector: Vec<f32> = (0..LATE_INTERACTION_DIMENSION)
        .map(|_| rng.gen_range(-1.0..1.0))
        .collect();

    // L2 normalize to unit vector
    let norm: f32 = vector.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > f32::EPSILON {
        for v in &mut vector {
            *v /= norm;
        }
    }

    vector
}
```

### Storage Considerations
Per-token embeddings require more storage than single-vector models:
- **Single text (100 tokens)**: 100 * 128 * 4 bytes = 51.2 KB
- **Compression**: Can use int8 quantization to reduce to 12.8 KB
- **Trade-off**: Better retrieval quality vs. storage cost

---

## Completion Checklist

- [x] `late_interaction.rs` created at correct path
- [x] `TokenEmbeddings` struct with validation
- [x] `LateInteractionModel` struct matching entity.rs pattern
- [x] `load()` and `unload()` as struct methods (NOT trait methods)
- [x] `EmbeddingModel` trait: 4 required methods implemented
- [x] All 8 edge case tests passing with BEFORE/AFTER output
- [x] `test_evidence_of_success` logs all evidence
- [x] `pretrained/mod.rs` exports all symbols
- [x] `cargo check` passes
- [x] `cargo test late_interaction --lib` passes (52 tests)
- [x] `cargo clippy` passes with no warnings
- [x] sherlock-holmes verification complete - VERDICT: INNOCENT
