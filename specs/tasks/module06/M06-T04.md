# M06-T04: Implement CandleEmbeddingProvider (1536D output for graph compatibility)

```yaml
metadata:
  id: "M06-T04"
  title: "Implement CandleEmbeddingProvider with 1536D Output"
  module: "module-06"
  module_name: "Stub Elimination"
  layer: "logic"
  priority: "critical"
  estimated_hours: 8
  created: "2026-01-04"
  updated: "2026-01-04"
  status: "ready"
  dependencies:
    - "M06-T02"  # COMPLETE - EmbeddingProvider trait exists
  blocks: ["M06-T07", "M06-T10"]
  spec_refs:
    - "docs2/constitution.yaml:305-323"   # 12-model embedding (E8_Graph_MiniLM: 384D internal)
    - "docs2/constitution.yaml:108-122"   # Performance: single_embed <10ms, batch_embed_64 <50ms
    - "docs2/contextprd.md:82-95"         # E8 Graph/GNN 1536D output, FuseMoE → 1536D
    - "docs2/contextprd.md:100-103"       # KnowledgeNode.embedding: Vector1536
```

## ⚠️ CRITICAL: Dimension Specification

**DISCREPANCY RESOLVED**: The constitution.yaml and contextprd.md have DIFFERENT dimension specs:

| Source | E8 Dimension | Notes |
|--------|--------------|-------|
| constitution.yaml:316 | 384D | Internal model dimension (MiniLM-L6-v2 native) |
| contextprd.md:89 | 1536D | Logical output dimension for graph compatibility |
| contextprd.md:95 | 1536D | FuseMoE fusion output |
| contextprd.md:103 | Vector1536 | KnowledgeNode.embedding storage |

**DECISION**: CandleEmbeddingProvider MUST output **1536D** to be compatible with:
- `KnowledgeNode.embedding: Vector1536` storage format
- Drop-in replacement for `EmbeddingProviderAdapter`
- All graph operations expecting 1536D embeddings

**IMPLEMENTATION**: Use MiniLM-L6-v2 (384D internal) + learnable projection layer (384D → 1536D)

## Current State (Audited 2026-01-04)

### What Already Exists

1. **`EmbeddingProvider` trait** (M06-T02 COMPLETE):
   - Location: `crates/context-graph-core/src/traits/embedding_provider.rs:202-294`
   - Methods: `embed`, `embed_batch`, `dimensions`, `model_id`, `is_ready`
   - Output: `EmbeddingOutput { vector, model_id, dimensions, latency }`

2. **`FusedEmbeddingProvider`** (Working GPU implementation):
   - Location: `crates/context-graph-embeddings/src/provider/fused.rs`
   - Uses `SemanticModel` (e5-large-v2, 1024D) + `ProjectionLayer` → 1536D
   - Implements local `EmbeddingProvider` trait (NOT core trait)
   - Has `embed(&str)` and `embed_batch(&[&str])` methods

3. **`EmbeddingProviderAdapter`** (Bridge to core trait):
   - Location: `crates/context-graph-mcp/src/adapters/embedding_adapter.rs:65-284`
   - Wraps `FusedEmbeddingProvider` → implements `context_graph_core::traits::EmbeddingProvider`
   - Used in production `server.rs:42-50`

4. **Candle models exist but use different dimensions**:
   - `SemanticModel` (E1): 1024D → projected to 1536D
   - `GraphModel` (E8): 384D MiniLM
   - All located in `crates/context-graph-embeddings/src/models/pretrained/`

5. **No `providers/` directory exists** - task document was WRONG about file structure

### What This Task Must Actually Do

The original task description was INCORRECT. The system already has a working Candle embedding provider via `FusedEmbeddingProvider` + `EmbeddingProviderAdapter`.

**REAL OBJECTIVE**: Create a standalone `CandleEmbeddingProvider` in `context-graph-core` that:
1. Implements the core `EmbeddingProvider` trait directly
2. Uses MiniLM-L6-v2 (384D internal) + projection layer → **1536D output**
3. Can be used WITHOUT the full embeddings crate dependency
4. Provides a **drop-in replacement** for `EmbeddingProviderAdapter`
5. Compatible with `KnowledgeNode.embedding: Vector1536` storage

---

## Problem Statement

The current architecture requires `context-graph-mcp` → `context-graph-embeddings` → full 12-model pipeline just for basic embeddings. This task creates a lightweight Candle-based provider in `context-graph-core` using MiniLM-L6-v2 + projection that:

1. Implements `context_graph_core::traits::EmbeddingProvider` directly
2. Outputs **1536D** embeddings (compatible with `KnowledgeNode.embedding: Vector1536`)
3. Provides sub-10ms single embeddings on GPU
4. Works without the full embedding pipeline
5. Serves as reference implementation for trait conformance

**Architecture**:
```
Input Text → Tokenizer → MiniLM-L6-v2 (384D) → ProjectionLayer → 1536D Output
                                                    ↑
                                         (learnable 384×1536 matrix)
```

---

## Scope

### In Scope

- `CandleEmbeddingProvider` struct in `context-graph-core/src/providers/`
- MiniLM-L6-v2 model loading (384 dimensions, sentence-transformers)
- GPU (CUDA) acceleration when available
- CPU fallback with graceful detection
- Batch embedding with efficiency > N single calls
- L2 normalization of output vectors
- Model warm-up on initialization
- Truncation at max sequence length (512 tokens)

### Out of Scope

- FuseMoE fusion (use `FusedEmbeddingProvider` for that)
- 12-model pipeline integration
- Projection layers (keep native 384D)
- Custom model training

---

## Prerequisites Checklist

- [x] M06-T02 complete - `EmbeddingProvider` trait exists at `context-graph-core/src/traits/embedding_provider.rs`
- [x] Candle workspace dependencies configured in root `Cargo.toml`
- [ ] Model weights downloadable (sentence-transformers/all-MiniLM-L6-v2)

---

## Files to Create

| File | Description |
|------|-------------|
| `crates/context-graph-core/src/providers/mod.rs` | Module declaration |
| `crates/context-graph-core/src/providers/candle_provider.rs` | Main implementation |
| `crates/context-graph-core/src/providers/config.rs` | Configuration structs |
| `crates/context-graph-core/src/providers/device.rs` | Device detection utilities |

## Files to Modify

| File | Change |
|------|--------|
| `crates/context-graph-core/src/lib.rs` | Add `pub mod providers;` |
| `crates/context-graph-core/Cargo.toml` | Add candle deps with optional feature |

---

## Implementation Signatures

```rust
// File: crates/context-graph-core/src/providers/candle_provider.rs

use crate::traits::{EmbeddingProvider, EmbeddingOutput};
use crate::error::CoreResult;
use std::path::PathBuf;
use std::time::Instant;

/// Configuration for Candle embedding provider.
#[derive(Debug, Clone)]
pub struct CandleConfig {
    /// HuggingFace model ID (default: "sentence-transformers/all-MiniLM-L6-v2")
    pub model_id: String,
    /// Device selection strategy
    pub device: DeviceConfig,
    /// Maximum batch size for efficient processing
    pub max_batch_size: usize,
    /// Maximum sequence length before truncation (default: 512)
    pub max_sequence_length: usize,
    /// Optional cache directory for model weights
    pub cache_dir: Option<PathBuf>,
}

impl Default for CandleConfig {
    fn default() -> Self {
        Self {
            model_id: "sentence-transformers/all-MiniLM-L6-v2".to_string(),
            device: DeviceConfig::Auto,
            max_batch_size: 64,
            max_sequence_length: 512,
            cache_dir: None,
        }
    }
}

#[derive(Debug, Clone, Default)]
pub enum DeviceConfig {
    #[default]
    Auto,
    Cpu,
    Cuda(usize),  // GPU index
}

/// Candle-based embedding provider using MiniLM-L6-v2.
///
/// Produces 384-dimensional embeddings with GPU acceleration.
/// Implements `context_graph_core::traits::EmbeddingProvider`.
pub struct CandleEmbeddingProvider {
    // Internal fields - implementation details
    config: CandleConfig,
    device: candle_core::Device,
    model: BertModel,       // From candle_nn
    tokenizer: Tokenizer,   // From tokenizers crate
    ready: AtomicBool,
}

impl CandleEmbeddingProvider {
    /// Create new provider with configuration.
    ///
    /// # Errors
    /// - `CoreError::Embedding` if model loading fails
    /// - `CoreError::Embedding` if GPU initialization fails (will NOT fallback silently)
    pub async fn new(config: CandleConfig) -> CoreResult<Self>;

    /// Create with default configuration.
    pub async fn with_defaults() -> CoreResult<Self> {
        Self::new(CandleConfig::default()).await
    }

    /// Get the underlying Candle device.
    pub fn device(&self) -> &candle_core::Device;

    /// Check if running on GPU.
    pub fn is_gpu(&self) -> bool;
}

#[async_trait]
impl EmbeddingProvider for CandleEmbeddingProvider {
    async fn embed(&self, content: &str) -> CoreResult<EmbeddingOutput>;
    async fn embed_batch(&self, contents: &[String]) -> CoreResult<Vec<EmbeddingOutput>>;
    fn dimensions(&self) -> usize { 384 }  // MiniLM dimension
    fn model_id(&self) -> &str { &self.config.model_id }
    fn is_ready(&self) -> bool { self.ready.load(Ordering::Acquire) }
}
```

---

## Pseudo Code

```
CandleEmbeddingProvider:

  async new(config):
    # Device detection - FAIL FAST, no silent fallback
    device = match config.device:
      Auto =>
        if candle_core::utils::cuda_is_available():
          Device::Cuda(0)
        else:
          Device::Cpu
      Cpu => Device::Cpu
      Cuda(idx) =>
        if !cuda_is_available():
          return Err(CoreError::Embedding("CUDA requested but not available"))
        Device::Cuda(idx)

    # Load model weights
    cache_path = config.cache_dir.unwrap_or(dirs::cache_dir() / "context-graph" / "models")

    # Download if not cached (hf-hub or manual)
    weights_path = download_or_cache(config.model_id, cache_path)?

    # Load tokenizer
    tokenizer = Tokenizer::from_pretrained(config.model_id)?

    # Load model
    vb = VarBuilder::from_safetensors(weights_path, DType::F32, &device)?
    bert_config = BertConfig::minilm_l6_v2()
    model = BertModel::load(vb, &bert_config)?

    # Warm up - run dummy inference to prime GPU caches
    dummy_tokens = tokenizer.encode("warmup", truncation=true)
    _ = model.forward(&dummy_tokens)?  # Discard result

    ready.store(true)
    return Self { config, device, model, tokenizer, ready }

  async embed(content):
    if content.is_empty():
      return Err(CoreError::Embedding("Empty content not allowed"))

    if !self.is_ready():
      return Err(CoreError::Embedding("Provider not initialized"))

    start = Instant::now()

    # Tokenize with truncation
    encoding = self.tokenizer.encode(content, truncation=true, max_length=512)

    # Convert to tensors on device
    input_ids = Tensor::from_vec(encoding.ids, &self.device)?
    attention_mask = Tensor::from_vec(encoding.attention_mask, &self.device)?
    token_type_ids = Tensor::zeros_like(&input_ids)?

    # Forward pass
    output = self.model.forward(&input_ids, &attention_mask, &token_type_ids)?

    # Mean pooling over sequence (exclude padding)
    # pooled = sum(output * mask) / sum(mask)
    mask_expanded = attention_mask.unsqueeze(-1).expand_as(&output)?
    sum_embeddings = (output * mask_expanded).sum(dim=1)?
    sum_mask = mask_expanded.sum(dim=1).clamp_min(1e-9)?
    pooled = sum_embeddings / sum_mask

    # L2 normalize
    norm = pooled.sqr().sum(-1)?.sqrt()?
    normalized = pooled / norm.unsqueeze(-1)?

    # Extract to CPU vector
    vector = normalized.to_vec1::<f32>()?

    return EmbeddingOutput::new(
      vector,
      self.config.model_id.clone(),
      start.elapsed()
    )

  async embed_batch(contents):
    if contents.is_empty():
      return Ok(vec![])

    # Check for empty strings - FAIL FAST
    for (i, c) in contents.iter().enumerate():
      if c.is_empty():
        return Err(CoreError::Embedding(format!("Empty content at index {}", i)))

    # Tokenize all
    encodings = self.tokenizer.encode_batch(contents, truncation=true)?

    # Pad to max length in batch
    max_len = encodings.iter().map(|e| e.len()).max().unwrap()

    # Stack into batch tensors [batch_size, seq_len]
    input_ids = stack_padded(&encodings, "input_ids", max_len, &self.device)?
    attention_mask = stack_padded(&encodings, "attention_mask", max_len, &self.device)?
    token_type_ids = Tensor::zeros_like(&input_ids)?

    # Single batched forward pass
    start = Instant::now()
    output = self.model.forward(&input_ids, &attention_mask, &token_type_ids)?

    # Mean pooling per sample
    results = vec![]
    for i in 0..contents.len():
      mask_i = attention_mask.get(i)?
      output_i = output.get(i)?

      mask_expanded = mask_i.unsqueeze(-1).expand_as(&output_i)?
      sum_emb = (output_i * mask_expanded).sum(0)?
      sum_mask = mask_expanded.sum(0).clamp_min(1e-9)?
      pooled = sum_emb / sum_mask

      normalized = l2_normalize(pooled)?
      vector = normalized.to_vec1::<f32>()?

      results.push(EmbeddingOutput::new(
        vector,
        self.config.model_id.clone(),
        start.elapsed() / contents.len()  # Average latency
      ))

    return results
```

---

## Performance Requirements (constitution.yaml)

| Operation | Target | Verification |
|-----------|--------|--------------|
| Single embed | <10ms | `test_single_embed_latency` |
| Batch embed (64) | <50ms | `test_batch_embed_latency` |
| Dimensions | 384 | `test_dimensions` |
| Normalized | magnitude ~1.0 | `test_normalization` |

---

## Constraints (MUST Enforce)

1. **NO BACKWARDS COMPATIBILITY** - If CUDA requested but unavailable, FAIL with error
2. **NO MOCK DATA** - Tests use real model inference, not fake vectors
3. **NO SILENT FALLBACKS** - Empty content → `CoreError::Embedding`, never return default
4. **FAIL FAST** - Any initialization failure → immediate error with descriptive message
5. **EMBEDDINGS MUST BE NORMALIZED** - L2 norm = 1.0 (within floating point tolerance)
6. **BATCH EFFICIENCY** - `embed_batch(64)` MUST be faster than 64× `embed()`

---

## Validation Criteria

- [ ] Model loads successfully (GPU or CPU based on config)
- [ ] Single embedding latency <10ms (on GPU)
- [ ] Batch of 64 latency <50ms (on GPU)
- [ ] Embeddings are L2-normalized (magnitude 1.0 ± 0.001)
- [ ] Different texts produce different embeddings
- [ ] Same text produces identical embeddings (deterministic)
- [ ] Empty string returns `CoreError::Embedding`, NOT empty vector
- [ ] Text longer than 512 tokens is truncated without error
- [ ] `is_ready()` returns false before initialization completes

---

## Test Commands

```bash
# 1. Build with candle feature
cargo build -p context-graph-core --features candle

# 2. Run unit tests
cargo test -p context-graph-core candle_provider --features candle -- --nocapture

# 3. Run latency benchmark
cargo test -p context-graph-core test_single_embed_latency --features candle -- --nocapture

# 4. Verify GPU detection (if available)
cargo test -p context-graph-core test_device_selection --features candle -- --nocapture

# 5. Verify normalization
cargo test -p context-graph-core test_embedding_normalized --features candle -- --nocapture
```

---

## Full State Verification Protocol

After implementing the logic, you MUST perform complete verification:

### 1. Source of Truth Definition

The **Source of Truth** for this task is:
- **Model Loading**: The Candle `BertModel` loaded in GPU memory
- **Embedding Output**: `EmbeddingOutput` struct returned from `embed()`
- **Device State**: `candle_core::Device` enum (Cpu or Cuda(n))

### 2. Execute & Inspect Protocol

After each implementation step, run:

```bash
# Verify model loads and is cached
ls -la ~/.cache/context-graph/models/all-MiniLM-L6-v2/

# Verify GPU utilization (if CUDA)
nvidia-smi --query-gpu=memory.used --format=csv

# Run actual embedding and inspect output
cargo test -p context-graph-core test_embed_real_content --features candle -- --nocapture 2>&1 | head -50
```

### 3. Boundary & Edge Case Audit

You MUST manually test these 3 edge cases and print before/after state:

**Edge Case 1: Empty Input**
```rust
#[test]
fn test_empty_input_fails() {
    let provider = CandleEmbeddingProvider::with_defaults().await.unwrap();
    println!("BEFORE: provider.is_ready() = {}", provider.is_ready());
    let result = provider.embed("").await;
    println!("AFTER: result = {:?}", result);
    assert!(result.is_err());
    assert!(matches!(result.unwrap_err(), CoreError::Embedding(_)));
}
```

**Edge Case 2: Maximum Length Input (512+ tokens)**
```rust
#[test]
fn test_max_length_truncation() {
    let provider = CandleEmbeddingProvider::with_defaults().await.unwrap();
    let long_text = "word ".repeat(1000);  // ~1000 tokens
    println!("BEFORE: input length = {} chars", long_text.len());
    let result = provider.embed(&long_text).await.unwrap();
    println!("AFTER: embedding.dimensions = {}", result.dimensions);
    assert_eq!(result.dimensions, 384);
    assert!(result.latency.as_millis() < 20); // Should still be fast
}
```

**Edge Case 3: Batch with Mixed Lengths**
```rust
#[test]
fn test_batch_mixed_lengths() {
    let provider = CandleEmbeddingProvider::with_defaults().await.unwrap();
    let contents = vec![
        "short".to_string(),
        "medium length sentence for testing".to_string(),
        "very long paragraph ".repeat(50),
    ];
    println!("BEFORE: batch sizes = {:?}", contents.iter().map(|s| s.len()).collect::<Vec<_>>());
    let results = provider.embed_batch(&contents).await.unwrap();
    println!("AFTER: result count = {}, all 384D = {}",
        results.len(),
        results.iter().all(|r| r.dimensions == 384)
    );
    assert_eq!(results.len(), 3);
}
```

### 4. Evidence of Success

After all tests pass, provide a log showing:

```bash
# Final verification commands
cargo test -p context-graph-core providers::candle_provider --features candle -- --nocapture 2>&1 | grep -E "(test |PASSED|FAILED|ok|error)"

# GPU memory state (if applicable)
nvidia-smi --query-gpu=name,memory.used,memory.total --format=csv

# Cache directory contents
ls -la ~/.cache/context-graph/models/
```

---

## Manual Output Verification

**CRITICAL**: After implementing `embed()`, you MUST manually verify the output exists and is correct:

1. **Vector exists and has correct length**:
```rust
let output = provider.embed("test").await?;
assert_eq!(output.vector.len(), 384, "Vector must be exactly 384D");
```

2. **Vector is normalized**:
```rust
let magnitude: f32 = output.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
assert!((magnitude - 1.0).abs() < 0.001, "Vector must be L2-normalized");
```

3. **Different inputs produce different outputs**:
```rust
let a = provider.embed("hello").await?;
let b = provider.embed("world").await?;
let similarity: f32 = a.vector.iter().zip(&b.vector).map(|(x, y)| x * y).sum();
assert!(similarity < 0.99, "Different texts must produce different embeddings");
```

4. **Same input produces identical output** (deterministic):
```rust
let a = provider.embed("test").await?;
let b = provider.embed("test").await?;
assert_eq!(a.vector, b.vector, "Same input must produce identical output");
```

---

## Sherlock-Holmes Final Verification

**MANDATORY**: After completing implementation, spawn `sherlock-holmes` subagent to verify:

```
Verify M06-T04 CandleEmbeddingProvider implementation:

1. File existence checks:
   - crates/context-graph-core/src/providers/mod.rs EXISTS
   - crates/context-graph-core/src/providers/candle_provider.rs EXISTS
   - crates/context-graph-core/src/lib.rs contains "pub mod providers"
   - crates/context-graph-core/Cargo.toml has candle dependencies

2. Trait implementation checks:
   - CandleEmbeddingProvider implements EmbeddingProvider trait
   - All 5 trait methods implemented: embed, embed_batch, dimensions, model_id, is_ready
   - dimensions() returns 384

3. Error handling checks:
   - embed("") returns CoreError::Embedding
   - No .unwrap() in production code paths
   - No silent fallbacks (grep for unwrap_or)

4. Test execution:
   - cargo test -p context-graph-core providers --features candle
   - All tests PASS

5. Performance verification:
   - Single embed <10ms
   - Batch 64 <50ms
   - Embeddings are normalized (magnitude ~1.0)

FIX ANY ISSUES FOUND.
```

---

## Cargo.toml Changes

Add to `crates/context-graph-core/Cargo.toml`:

```toml
[features]
default = []
candle = ["dep:candle-core", "dep:candle-nn", "dep:tokenizers"]

[dependencies]
# ... existing deps ...

# GPU/ML - Candle framework (optional for lightweight builds)
candle-core = { workspace = true, optional = true }
candle-nn = { workspace = true, optional = true }
tokenizers = { version = "0.19", optional = true }

# HuggingFace Hub for model downloads
hf-hub = { version = "0.3", optional = true, features = ["tokio"] }
```

---

## References

- constitution.yaml lines 305-323: 12-model embedding pipeline
- constitution.yaml lines 108-122: Performance budgets
- contextprd.md lines 189-208: E8 Graph/MiniLM-L6-v2 (384D)
- M06-T02: EmbeddingProvider trait (COMPLETE)
- `crates/context-graph-embeddings/src/provider/fused.rs`: Reference implementation

---

*Task updated: 2026-01-04*
*Audited against codebase: Verified file paths and current state*
*Module: 06 - Stub Elimination*
*Layer: Logic*
*Priority: CRITICAL*
