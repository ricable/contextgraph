# TASK-S006: End-to-End Integration Tests with Real Embeddings

```yaml
metadata:
  id: "TASK-S006"
  title: "End-to-End Integration Tests with Real Embeddings"
  layer: "surface"
  priority: "P0"
  estimated_hours: 12
  created: "2026-01-04"
  status: "pending"
  dependencies:
    - "TASK-F001"  # SemanticFingerprint
    - "TASK-F002"  # TeleologicalFingerprint
    - "TASK-F003"  # JohariFingerprint
    - "TASK-F004"  # Storage Schema
    - "TASK-F005"  # HNSW Indexes
    - "TASK-F006"  # Remove Fusion Files
    - "TASK-F007"  # EmbeddingProvider Trait
    - "TASK-F008"  # MemoryStore Trait
    - "TASK-L001"  # Query Executor
    - "TASK-L002"  # Purpose Vector
    - "TASK-L003"  # Goal Alignment
    - "TASK-L004"  # Johari Transition
    - "TASK-L005"  # HNSW Builder
    - "TASK-L006"  # Purpose Index
    - "TASK-L007"  # Similarity Engine
    - "TASK-L008"  # Retrieval Pipeline
  traces_to:
    - "FR-603"  # Fail Fast with Robust Error Logging
    - "FR-604"  # No Mock Data in Tests
```

## Problem Statement

Create comprehensive end-to-end integration tests that verify the complete Multi-Array Teleological Fingerprint system works correctly with REAL embedding data from actual embedding models. NO MOCK VECTORS.

## Context

Integration tests must:
1. Use REAL embeddings generated by actual embedding models
2. Test all 12 embedder types with appropriate content
3. Verify complete flow from content input to search results
4. Test purpose vector computation and alignment
5. Test Johari classification and transitions
6. Verify weighted multi-space search correctness
7. Test goal hierarchy operations
8. Validate Meta-UTL predictions

**CRITICAL: NO mock vectors, NO random data, NO simplified test vectors.**

## Technical Specification

### Test Fixture Requirements

```rust
/// Test fixtures for real embedding data
pub mod test_fixtures {
    use std::path::PathBuf;

    /// Directory containing real embedding fixtures
    pub const FIXTURES_DIR: &str = "tests/fixtures/embeddings";

    /// Load real embeddings for a text sample
    /// These are ACTUAL outputs from the 12 embedding models
    pub fn load_real_semantic_fingerprint(name: &str) -> SemanticFingerprint {
        let path = PathBuf::from(FIXTURES_DIR).join(format!("{}.json", name));
        let data = std::fs::read_to_string(&path)
            .expect(&format!("Failed to load fixture: {}", path.display()));
        serde_json::from_str(&data)
            .expect(&format!("Failed to parse fixture: {}", path.display()))
    }

    /// Load real purpose vector computed from actual North Star
    pub fn load_real_purpose_vector(name: &str) -> PurposeVector {
        let path = PathBuf::from(FIXTURES_DIR).join(format!("{}_purpose.json", name));
        serde_json::from_str(&std::fs::read_to_string(&path).unwrap()).unwrap()
    }

    /// Available test content categories
    pub const TEST_CONTENTS: &[(&str, &str)] = &[
        ("ml_concept", "Machine learning is a subset of artificial intelligence..."),
        ("rust_code", "fn main() { println!(\"Hello, world!\"); }"),
        ("historical_event", "The Battle of Hastings occurred in 1066..."),
        ("causal_chain", "Because the temperature dropped, the water froze..."),
        ("entity_rich", "Apple Inc. was founded by Steve Jobs and Steve Wozniak..."),
    ];
}
```

### Required Test Fixtures (to be generated)

All fixtures MUST be generated using actual embedding models:

```yaml
fixtures:
  # Core content fixtures with all 12 embeddings
  ml_concept.json:
    content: "Machine learning explanation text..."
    e1_text_general: [<1024 REAL floats from text-embedding-3-large>]
    e2_text_small: [<512 REAL floats from text-embedding-3-small>]
    e3_multilingual: [<512 REAL floats from multilingual-e5-base>]
    e4_code: [<512 REAL floats from codet5p-110m>]
    e5_query_doc:
      query: [<768 REAL floats>]
      doc: [<768 REAL floats>]
    e6_sparse:
      indices: [<~1500 REAL indices from splade-v3>]
      values: [<~1500 REAL floats>]
    e7_openai_ada: [<1536 REAL floats from text-embedding-ada-002>]
    e8_minilm: [<384 REAL floats from all-MiniLM-L6>]
    e9_simhash: [<1024 REAL floats from SimHash>]
    e10_instructor: [<768 REAL floats from instructor-xl>]
    e11_fast: [<384 REAL floats from e5-small-v2>]
    e12_token_level: [<N x 128 REAL floats from ColBERT>]

  rust_code.json: <same structure>
  historical_event.json: <same structure>
  causal_chain.json: <same structure>
  entity_rich.json: <same structure>

  # Purpose vector fixtures (computed against North Star)
  ml_concept_purpose.json:
    alignments: [<12 REAL alignment scores>]
    dominant_embedder: <actual dominant>
    coherence: <actual coherence>

  # Johari fixtures (computed from entropy/coherence)
  ml_concept_johari.json:
    quadrants: [[open, hidden, blind, unknown weights x 4] x 12]
    confidence: [<12 REAL confidence scores>]
```

### Integration Test Scenarios

```rust
/// Integration test module for Multi-Array Teleological Fingerprint
#[cfg(test)]
mod integration_tests {
    use super::*;
    use crate::test_fixtures::*;

    /// Full pipeline: content -> embeddings -> store -> search -> retrieve
    #[tokio::test]
    async fn test_complete_memory_lifecycle() {
        // Setup
        let embedding_provider = RealEmbeddingProvider::new();
        let store = TeleologicalMemoryStore::new_test_instance();
        let query_executor = MultiEmbeddingQueryExecutor::new(store.clone());

        // 1. Generate embeddings from REAL content
        let content = "Machine learning uses algorithms to learn from data.";
        let embeddings = embedding_provider.embed_all(content).await.unwrap();

        // Verify all 12 embeddings have correct dimensions
        assert_eq!(embeddings.fingerprint.e1_text_general.len(), 1024);
        assert_eq!(embeddings.fingerprint.e2_text_small.len(), 512);
        assert_eq!(embeddings.fingerprint.e7_openai_ada.len(), 1536);
        // ... verify all 12

        // 2. Compute purpose vector
        let north_star = load_north_star_goal();
        let purpose = compute_purpose_vector(&embeddings.fingerprint, &north_star);
        assert_eq!(purpose.alignments.len(), 12);
        assert!(purpose.dominant_embedder >= 1 && purpose.dominant_embedder <= 12);

        // 3. Compute Johari fingerprint
        let johari = compute_johari_fingerprint(&embeddings.fingerprint);
        for i in 0..12 {
            let quadrant = johari.dominant_quadrant(i);
            assert!(matches!(quadrant, JohariQuadrant::Open | JohariQuadrant::Hidden |
                                       JohariQuadrant::Blind | JohariQuadrant::Unknown));
        }

        // 4. Create and store TeleologicalFingerprint
        let fingerprint = TeleologicalFingerprint::new(
            embeddings.fingerprint,
            purpose,
            johari,
            compute_content_hash(content),
        );
        let id = store.store(fingerprint).await.unwrap();

        // 5. Retrieve and verify
        let retrieved = store.retrieve(id).await.unwrap().unwrap();
        assert_eq!(retrieved.semantic.e1_text_general.len(), 1024);
        assert_eq!(retrieved.purpose_vector.alignments.len(), 12);

        // 6. Search and verify
        let query = "What is machine learning?";
        let query_embeddings = embedding_provider.embed_all(query).await.unwrap();
        let results = query_executor.execute(MultiEmbeddingQuery {
            query: QueryInput::Embeddings(query_embeddings.fingerprint),
            top_k: 5,
            ..Default::default()
        }).await.unwrap();

        // Our stored memory should be in results
        assert!(results.results.iter().any(|r| r.memory_id == id));
    }

    /// Verify weighted search produces correct rankings
    #[tokio::test]
    async fn test_weighted_search_correctness() {
        let store = setup_store_with_diverse_content().await;
        let executor = MultiEmbeddingQueryExecutor::new(store.clone());

        // Search with semantic focus
        let semantic_results = executor.execute(MultiEmbeddingQuery {
            query: QueryInput::Text("machine learning algorithms".into()),
            query_type: Some("semantic_search".into()),
            include_per_embedder_scores: true,
            ..Default::default()
        }).await.unwrap();

        // Search with code focus (same query)
        let code_results = executor.execute(MultiEmbeddingQuery {
            query: QueryInput::Text("machine learning algorithms".into()),
            query_type: Some("code_search".into()),
            include_per_embedder_scores: true,
            ..Default::default()
        }).await.unwrap();

        // Rankings should differ based on weights
        // Code-focused should rank code content higher
        let semantic_top = &semantic_results.results[0];
        let code_top = &code_results.results[0];

        // E7 (code) contribution should be higher in code search
        let semantic_e7 = semantic_top.per_embedder_scores.as_ref().unwrap().e7_code;
        let code_e7 = code_top.per_embedder_scores.as_ref().unwrap().e7_code;
        // This is a relative comparison - actual values depend on content

        // Verify weights were applied
        assert_eq!(semantic_results.query_metadata.query_type_used, "semantic_search");
        assert_eq!(code_results.query_metadata.query_type_used, "code_search");
    }

    /// Test purpose-aligned retrieval
    #[tokio::test]
    async fn test_purpose_aligned_retrieval() {
        let store = setup_store_with_varying_alignment().await;

        // Find memories aligned to learning goal
        let aligned = store.find_aligned_to_goal(
            learning_goal_id(),
            0.7, // min_alignment
            10,  // top_k
        ).await.unwrap();

        // All results should have alignment >= 0.7
        for (id, alignment) in &aligned {
            assert!(*alignment >= 0.7, "Alignment {} < 0.7", alignment);

            // Verify by retrieving
            let mem = store.retrieve(*id).await.unwrap().unwrap();
            assert!(mem.theta_to_north_star >= 0.7);
        }
    }

    /// Test Johari quadrant filtering
    #[tokio::test]
    async fn test_johari_quadrant_filtering() {
        let store = setup_store_with_johari_variety().await;

        // Find memories that are Open in semantic (E1) but Blind in causal (E5)
        let semantic_open = store.find_by_johari(0, JohariQuadrant::Open, 100).await.unwrap();

        let mut blind_spots = Vec::new();
        for id in semantic_open {
            let mem = store.retrieve(id).await.unwrap().unwrap();
            if mem.johari.dominant_quadrant(4) == JohariQuadrant::Blind {
                blind_spots.push(id);
            }
        }

        // These are learning opportunities - aware in semantic, blind in causal
        assert!(!blind_spots.is_empty(), "Should find at least one blind spot");
    }

    /// Test cross-space similarity engine
    #[tokio::test]
    async fn test_cross_space_similarity() {
        let fp1 = load_real_semantic_fingerprint("ml_concept");
        let fp2 = load_real_semantic_fingerprint("rust_code");
        let fp3 = load_real_semantic_fingerprint("ml_concept_v2"); // Similar to ml_concept

        let weights = SimilarityWeights::for_query_type(QueryType::Balanced);

        let sim_same = multi_embedding_similarity(&fp1, &fp3, &weights.weights);
        let sim_diff = multi_embedding_similarity(&fp1, &fp2, &weights.weights);

        // Similar content should have higher similarity
        assert!(sim_same.aggregate > sim_diff.aggregate,
            "Similar content ({}) should score higher than different ({})",
            sim_same.aggregate, sim_diff.aggregate);

        // Verify per-space breakdown
        assert_eq!(sim_same.per_embedder.len(), 12);
        for s in &sim_same.per_embedder {
            assert!(*s >= -1.0 && *s <= 1.0);
        }
    }

    /// Test goal hierarchy navigation
    #[tokio::test]
    async fn test_goal_hierarchy() {
        let store = setup_store_with_goal_hierarchy().await;

        // Get children of North Star
        let north_star = store.get_north_star().await.unwrap();
        let children = store.get_goal_children(north_star.id).await.unwrap();

        assert!(!children.is_empty());
        for child in &children {
            assert_eq!(child.level, 1); // Mid-level goals
            assert_eq!(child.parent_id, Some(north_star.id));
        }

        // Get ancestors of a local goal
        let local_goal = store.get_local_goals().await.unwrap()[0].clone();
        let ancestors = store.get_goal_ancestors(local_goal.id).await.unwrap();

        assert_eq!(ancestors.len(), 2); // Mid-level and North Star
        assert_eq!(ancestors[0].level, 1); // Mid-level
        assert_eq!(ancestors[1].level, 0); // North Star
    }

    /// Test alignment drift detection
    #[tokio::test]
    async fn test_alignment_drift_detection() {
        let store = setup_store_with_drifting_memories().await;

        // Check for memories with alignment drift
        let drifting = store.find_drifting_memories(-0.15, 24).await.unwrap();

        assert!(!drifting.is_empty());
        for (id, delta) in &drifting {
            assert!(*delta < -0.15, "Delta {} should be < -0.15", delta);

            // Verify evolution history shows drift
            let mem = store.retrieve(*id).await.unwrap().unwrap();
            assert!(mem.purpose_evolution.len() >= 2);
            let current = mem.theta_to_north_star;
            let previous = mem.purpose_evolution[mem.purpose_evolution.len() - 2]
                .purpose.aggregate_alignment();
            assert!((current - previous - delta).abs() < 0.001);
        }
    }

    /// Test Meta-UTL prediction accuracy
    #[tokio::test]
    async fn test_meta_utl_predictions() {
        let meta_utl = MetaUTL::new();
        let store = setup_store_with_history().await;

        // Make predictions and validate
        let mut correct = 0;
        let mut total = 0;

        for _ in 0..100 {
            let fp = generate_random_real_fingerprint().await;

            // Predict
            let prediction = meta_utl.predict_storage_impact(&fp);

            // Actually store
            let id = store.store(fp.clone()).await.unwrap();

            // Measure actual impact
            let actual_coherence = store.measure_coherence_delta().await;

            // Validate
            let error = (prediction.coherence_delta - actual_coherence).abs();
            if error < 0.2 {
                correct += 1;
            }
            total += 1;

            meta_utl.validate_prediction(&prediction, actual_coherence);
        }

        let accuracy = correct as f32 / total as f32;
        assert!(accuracy >= 0.85, "Storage prediction accuracy {} < 0.85 target", accuracy);
    }

    /// Verify no fusion code paths exist
    #[tokio::test]
    fn test_no_fusion_code() {
        // Search for fusion-related code patterns
        let src_path = std::path::Path::new("crates");
        let patterns = ["fuse_moe", "FuseMoE", "fusion", "gating", "expert_select"];

        for pattern in patterns {
            let output = std::process::Command::new("rg")
                .args(&["-l", pattern, src_path.to_str().unwrap()])
                .output()
                .expect("Failed to run rg");

            let files = String::from_utf8_lossy(&output.stdout);
            assert!(files.trim().is_empty(),
                "Found fusion-related pattern '{}' in files:\n{}", pattern, files);
        }
    }
}
```

### Fixture Generation Script

```rust
/// Script to generate real embedding fixtures
/// Run with: cargo run --bin generate_fixtures
fn main() {
    let contents = vec![
        ("ml_concept", "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed."),
        ("rust_code", r#"fn main() {
            let numbers = vec![1, 2, 3, 4, 5];
            let sum: i32 = numbers.iter().sum();
            println!("Sum: {}", sum);
        }"#),
        ("historical_event", "The French Revolution began in 1789 with the storming of the Bastille prison in Paris, marking the end of absolute monarchy in France."),
        ("causal_chain", "Because the drought reduced crop yields, food prices increased, which led to widespread protests in the capital city."),
        ("entity_rich", "Tesla Inc., founded by Elon Musk, is headquartered in Austin, Texas and manufactures electric vehicles including the Model 3 and Model Y."),
    ];

    for (name, content) in contents {
        let fingerprint = generate_real_fingerprint(content).await;
        save_fixture(name, &fingerprint);
    }
}
```

## Implementation Requirements

### Prerequisites

ALL Foundation and Logic Layer tasks must be complete.

### Scope

#### In Scope

- End-to-end integration tests for all features
- Real embedding fixture generation
- Multi-space search correctness tests
- Purpose alignment tests
- Johari classification tests
- Goal hierarchy tests
- Meta-UTL prediction tests
- Fusion code absence verification

#### Out of Scope

- Unit tests (covered by individual tasks)
- Performance benchmarks (separate task)
- Load testing (separate task)

### Constraints

- NO mock vectors - all embeddings must be from real models
- Tests must be deterministic with fixtures
- Test suite must complete in < 5 minutes
- Coverage must be >= 80% integration

## Definition of Done

### Implementation Checklist

- [ ] Fixture generation script implemented
- [ ] All 5 content category fixtures generated
- [ ] Complete memory lifecycle test
- [ ] Weighted search correctness test
- [ ] Purpose alignment test
- [ ] Johari filtering test
- [ ] Cross-space similarity test
- [ ] Goal hierarchy test
- [ ] Alignment drift test
- [ ] Meta-UTL prediction test
- [ ] No fusion code verification test

### Verification Commands

```bash
# Generate fixtures (one-time)
cargo run --bin generate_fixtures

# Run integration tests
cargo test --test integration_tests

# Verify no fusion code
rg -l "fuse|fusion|gating" crates/

# Check coverage
cargo llvm-cov --test integration_tests --html
```

## Files to Create

| File | Description |
|------|-------------|
| `tests/integration_tests.rs` | Main integration test file |
| `tests/fixtures/` | Directory for embedding fixtures |
| `tests/fixtures/ml_concept.json` | ML concept embeddings |
| `tests/fixtures/rust_code.json` | Rust code embeddings |
| `tests/fixtures/historical_event.json` | Historical event embeddings |
| `tests/fixtures/causal_chain.json` | Causal chain embeddings |
| `tests/fixtures/entity_rich.json` | Entity-rich embeddings |
| `src/bin/generate_fixtures.rs` | Fixture generation script |

## Traceability

| Requirement | Source | Coverage |
|-------------|--------|----------|
| FR-603 | FUNC-SPEC-001 | Error handling tests |
| FR-604 | FUNC-SPEC-001 | Real embedding fixtures |
| SAC-01-08 | FUNC-SPEC-001 | All acceptance criteria |

---

*Task created: 2026-01-04*
*Layer: Surface*
*Priority: P0 - Validation critical*
