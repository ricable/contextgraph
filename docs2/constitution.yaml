# Context Graph Constitution v6.0.0 (Topic-Based Architecture)
# Multi-Space Embeddings | Emergent Topics | Weighted Clustering | Native Hooks
# ===============================================================================

meta:
  v: "6.0.0"
  name: "Ultimate Context Graph"
  desc: "13-embedding multi-space retrieval with emergent topic discovery via HDBSCAN+BIRCH clustering, weighted category agreement"
  paradigm: "Emergent Topics from Weighted Multi-Space Clustering"
  hook_architecture: "NATIVE Claude Code hooks ONLY (via .claude/settings.json)"

# ABBREVIATIONS
# TF=Teleological Fingerprint, TP=Topic Profile (13D)
# TS=Topic Stability, TPF=Topic Portfolio
# weighted_agreement=Sum of category-weighted cluster membership
# E1-E13=Embedder spaces, HDBSCAN=Density clustering, BIRCH=Online clustering

# ===============================================================================
# TECH STACK
# ===============================================================================
stack:
  lang: { rust: "1.75+", edition: "2021", cuda: "13.1" }
  gpu: { target: "RTX 5090", vram: "32GB", compute: "12.0" }
  deps: [tokio@1.35+, serde@1.0+, uuid@1.6+, chrono@0.4+, rmcp@0.1+, cudarc@0.10+, faiss@0.12+gpu, rocksdb@0.21+, scylladb@1.0+, hdbscan@0.5+, birch@0.3+]
  db:
    primary: { dev: rocksdb, prod: scylladb }
    indexes: { per_embedder: "13x HNSW", matryoshka_128d: HNSW, splade_inverted: inverted, topic: "13D topic_profile HNSW" }
    temporal: timescaledb
    cache: redis7+

# ===============================================================================
# DIRECTORY STRUCTURE
# ===============================================================================
dirs:
  crates/:
    context-graph-mcp/: "tools/, resources/, handlers/, adapters/"
    context-graph-core/: "graph/, search/, utl/, session/, curation/, teleological/, topic/, clustering/, dream/"
    context-graph-cuda/: "kernels/, hnsw/, neuromod/"
    context-graph-embeddings/: "models/, fingerprint/"
    context-graph-storage/: "rocksdb/, scylla/, indexes/, temporal/"
    context-graph-cli/: "commands/, hooks/"
  specs/: [functional/, technical/, tasks/]
  tests/: [integration/, benchmarks/, fixtures/, chaos/, validation/]
  hooks/: [session-start.sh, pre-tool-use.sh, post-tool-use.sh, user-prompt-submit.sh, stop.sh, session-end.sh]

# ===============================================================================
# ARCHITECTURAL RULES (Critical - must not violate)
# ===============================================================================
arch_rules:
  ARCH-01: "TeleologicalArray is atomic - store all 13 embeddings or nothing"
  ARCH-02: "Apples-to-apples only - compare E1<->E1, E4<->E4, never E1<->E5"
  ARCH-03: "Autonomous operation - goals emerge from topic clustering, no manual goal setting"
  ARCH-04: "Temporal embedders (E2-E4) NEVER count toward topic detection - metadata only"
  ARCH-05: "All 13 embedders required - missing embedder is fatal error"
  ARCH-06: "All memory ops through MCP tools - no direct DB access"
  ARCH-07: "NATIVE Claude Code hooks (.claude/settings.json) control memory lifecycle - NOT internal/built-in hooks"
  ARCH-08: "CUDA GPU required for production - no CPU fallbacks"
  ARCH-09: "Topic threshold is weighted_agreement >= 2.5 (not raw space count)"
  ARCH-10: "Divergence detection uses SEMANTIC embedders only (E1, E5, E6, E7, E10, E12, E13)"
  ARCH-11: "Memory sources: HookDescription, ClaudeResponse, MDFileChunk"

  # Multi-Space Search Architecture (TASK-MULTISPACE)
  ARCH-12: "Search strategies: E1Only (default), MultiSpace (weighted fusion), Pipeline (3-stage)"
  ARCH-13: "Weight profiles MUST have E2-E4 = 0.0 for semantic search"
  ARCH-14: "Recency boost is applied POST-retrieval as: final = semantic*(1-boost) + temporal*boost"

# ===============================================================================
# CODING STANDARDS
# ===============================================================================
naming:
  files: { rust: snake_case.rs, cuda: snake_case.cu, tests: "{mod}_test.rs" }
  types: PascalCase
  funcs: { rust: snake_case_verb_first }
  vars: { local: snake_case, const: SCREAMING_SNAKE }

rust_standards:
  error_handling: ["thiserror for library", "anyhow for app", "Never panic in lib", "Propagate with ?"]
  async_patterns: ["tokio runtime", "spawn for parallel", "spawn_blocking for CPU-bound"]
  type_safety: ["newtype for domain IDs", "NonZeroU* for counts", "enums over booleans"]

rules:
  - "One type/module, max 500 lines"
  - "Result<T,E>, thiserror derivation"
  - "Never unwrap() in prod; use expect() with context"
  - "tokio async, Arc<RwLock<T>> for shared state"
  - "Max 5 unsafe blocks/module"
  - "CUDA FFI only in context-graph-cuda"

# ===============================================================================
# ANTI-PATTERNS (FORBIDDEN)
# ===============================================================================
forbidden:
  # Critical - Architecture
  AP-02: "No cross-embedder comparison (E1<->E5)"
  AP-03: "No dimension projection (1024D->512D)"
  AP-04: "No partial TeleologicalArray storage"
  AP-05: "No embedding fusion into single vector"
  AP-06: "No direct DB access - MCP tools only"
  AP-07: "No CPU fallback in production"
  AP-08: "No sync I/O in async context"
  AP-09: "No unbounded caches"
  AP-10: "No NaN/Infinity in similarity scores"

  # Critical - Topic System
  AP-60: "Temporal embedders (E2-E4) MUST NOT count toward topic detection"
  AP-61: "Topic threshold MUST be weighted_agreement >= 2.5, not raw count"
  AP-62: "Divergence alerts MUST only use SEMANTIC embedders"
  AP-63: "NEVER trigger divergence from temporal proximity differences"
  AP-64: "Relational/Structural embedders count at 0.5x weight ONLY"
  AP-65: "No manual topic/goal setting - topics emerge from clustering"

  # Critical - Dream Triggers
  AP-70: "Dream triggers MUST use entropy > 0.7 AND churn > 0.5"
  AP-71: "Dream NREM/REM returning stubs forbidden"
  AP-72: "nrem.rs/rem.rs TODO stubs MUST be implemented"

  # Critical - Multi-Space Search (TASK-MULTISPACE)
  # References:
  # - Pinecone Cascading Retrieval: https://www.pinecone.io/blog/cascading-retrieval/
  # - ACM TOIS Fusion: https://dl.acm.org/doi/10.1145/3596512
  # - ColBERT Late Interaction: https://weaviate.io/blog/late-interaction-overview
  # - SPLADE Sparse Expansion: https://www.pinecone.io/learn/splade/
  AP-73: "Temporal embedders (E2-E4) MUST NOT be used in similarity scoring fusion - temporal proximity != topical similarity"
  AP-74: "E12 ColBERT MUST only be used for re-ranking, NOT initial retrieval"
  AP-75: "E13 SPLADE MUST be used for Stage 1 recall, NOT final ranking"
  AP-76: "Multi-stage retrieval pipeline MUST use: Stage 1 (sparse recall) → Stage 2 (dense scoring) → Stage 3 (optional rerank)"

  # High
  AP-11: "Check existing utils before creating helpers"
  AP-12: "No magic numbers - use named constants"
  AP-14: "No .unwrap() in library code"

  # Hook Architecture (Critical)
  AP-50: "NO internal/built-in Claude Code hooks - NATIVE hooks via .claude/settings.json ONLY"
  AP-51: "NO Universal LLM Adapter - Claude Code native MCP only"
  AP-52: "NO custom Claude Code source modifications for hook integration"
  AP-53: "Hook logic MUST be in shell scripts calling context-graph-cli"

# ===============================================================================
# SECURITY
# ===============================================================================
security:
  SEC-01: "Validate/sanitize all input"
  SEC-02: { rule: "Scrub PII pre-embed", patterns: [api_key, password, bearer_token, ssn, credit_card] }
  SEC-03: { anomaly_threshold: "3.0 std", content_align_min: 0.4 }
  SEC-04: { rule: "Detect prompt injection", patterns: ["ignore previous", "disregard system", "you are now"] }
  SEC-06: "Soft delete 30-day recovery"
  SEC-07: "Secrets from env vars only"

# ===============================================================================
# PERFORMANCE BUDGETS
# ===============================================================================
perf:
  # NOTE: These are REALISTIC budgets based on actual RTX 5090 single-GPU measurements
  # Running 13 embedding models sequentially on single GPU is compute-bound
  latency:
    inject_context: "<2000ms p95"   # Includes embedding + search + retrieval
    single_embed: "<1000ms"         # All 13 models on single GPU (sequential)
    batch_embed_64: "<3000ms"       # Memory-bound, 64 items × 13 models
    faiss_1M_k100: "<5ms"           # Pure FAISS is fast, but includes overhead
    dream_wake: "<100ms"            # Wake from sleep state
    cli_startup: "<400ms"           # Binary load + initialization
    mcp_search: "<2000ms p95"       # Full search_graph operation
    mcp_store: "<2500ms p95"        # Full store_memory operation
  throughput: { embed_batch: ">20/sec", search_batch_100: "<50ms" }  # Realistic for single GPU
  memory: { gpu: "<24GB", graph_cap: ">10M nodes" }
  quality: { topic_stability: ">0.6", attack_detection: ">95%", info_loss: "<15%" }

# ===============================================================================
# TESTING
# ===============================================================================
testing:
  coverage: { unit: "90%", integration: "80%", docs: "80%" }
  gates:
    pre-commit: [fmt --check, clippy -D warnings, test --lib]
    pre-merge: [test --all, bench --no-run, "coverage>=90%"]
    pre-deploy: [integration pass, "bench regression<5%", chaos pass]

# ===============================================================================
# TOPIC SYSTEM
# ===============================================================================
topic_system:
  overview: "Topics emerge from multi-space clustering, no manual goal setting"

  # -------------------------------------------------------------------------
  # EMBEDDER CATEGORIES
  # -------------------------------------------------------------------------
  embedder_categories:
    SEMANTIC:
      embedders: [E1, E5, E6, E7, E10, E12, E13]
      topic_weight: 1.0
      role: "Primary topic triggers - capture meaning, concepts, intent, code"
      divergence_detection: true
      count: 7

    TEMPORAL:
      embedders: [E2, E3, E4]
      topic_weight: 0.0
      role: "Metadata only - NEVER for topic detection"
      divergence_detection: false
      count: 3
      rationale: "Temporal proximity != semantic relationship"
      usage:
        - "E2 (V_freshness): Recency weighting"
        - "E3 (V_periodicity): Cyclical patterns (daily, weekly)"
        - "E4 (V_ordering): Sequence/order relationships"
        - "Same-session badges on injected context"
        - "Timeline enrichment metadata"

    RELATIONAL:
      embedders: [E8, E11]
      topic_weight: 0.5
      role: "Supporting - can reinforce but not define topics alone"
      divergence_detection: false
      count: 2
      usage:
        - "E8 (V_connectivity): Graph structure relationships"
        - "E11 (V_factuality): Named entity relationships"
        - "Entity-based retrieval enrichment"

    STRUCTURAL:
      embedders: [E9]
      topic_weight: 0.5
      role: "Supporting - structural/format patterns"
      divergence_detection: false
      count: 1
      usage:
        - "E9 (V_robustness): Hyperdimensional structure"
        - "Format-aware retrieval"
        - "Noise-robust pattern matching"

  # -------------------------------------------------------------------------
  # WEIGHTED AGREEMENT FORMULA
  # -------------------------------------------------------------------------
  weighted_agreement:
    formula: "weighted_agreement = Sum(topic_weight_i x is_clustered_i)"
    max_value: 8.5  # 7x1.0 (semantic) + 2x0.5 (relational) + 1x0.5 (structural)
    breakdown:
      semantic_max: 7.0  # 7 embedders x 1.0 weight
      relational_max: 1.0  # 2 embedders x 0.5 weight
      structural_max: 0.5  # 1 embedder x 0.5 weight
      temporal_contribution: 0.0  # ALWAYS zero

  # -------------------------------------------------------------------------
  # TOPIC DETECTION
  # -------------------------------------------------------------------------
  topic_detection:
    threshold: 2.5
    examples:
      - "3 semantic spaces agreeing = 3.0 -> TOPIC"
      - "2 semantic + 1 relational = 2.5 -> TOPIC"
      - "2 semantic spaces only = 2.0 -> NOT TOPIC"
      - "5 temporal spaces = 0.0 -> NOT TOPIC (excluded)"
      - "1 semantic + 3 relational = 2.5 -> TOPIC"
    confidence: "topic_confidence = weighted_agreement / 8.5"

  # -------------------------------------------------------------------------
  # TOPIC PORTFOLIO
  # -------------------------------------------------------------------------
  topic_portfolio:
    description: "Emergent topics discovered via clustering, no manual setting"
    structure:
      topics: "HashMap<TopicId, TopicMetrics>"
      history: "VecDeque<TopicSnapshot>"
    profile_dimensions: 13  # All embedders stored, only semantic/relational/structural for detection

  # -------------------------------------------------------------------------
  # TOPIC STABILITY
  # -------------------------------------------------------------------------
  topic_stability:
    metrics:
      churn_rate: "[0.0-1.0] where 0.0=stable, 1.0=completely new topics"
      entropy: "[0.0-1.0] topic distribution entropy"
      membership_stability: "How stable topic memberships are"
      centroid_stability: "How stable topic centroids are"
    phases:
      Emerging: "New topic forming"
      Stable: "Established topic"
      Declining: "Topic losing members"
      Merging: "Topics consolidating"
    thresholds:
      healthy: "churn < 0.3"
      warning: "churn in [0.3, 0.5)"
      unstable: "churn >= 0.5"

  # -------------------------------------------------------------------------
  # DIVERGENCE DETECTION
  # -------------------------------------------------------------------------
  divergence_detection:
    description: "Detect when current activity differs from recent work"
    scope: "SEMANTIC embedders ONLY (E1, E5, E6, E7, E10, E12, E13)"
    excluded:
      - "Temporal (E2-E4): Working at different times is not divergence"
      - "Relational (E8, E11): Different entities is not inherently divergent"
      - "Structural (E9): Different structure is not semantic divergence"
    thresholds:
      E1_semantic: { high: "> 0.75", low: "< 0.3" }
      E5_causal: { high: "> 0.70", low: "< 0.25" }
      E6_sparse: { high: "> 0.60", low: "< 0.2" }
      E7_code: { high: "> 0.80", low: "< 0.35" }
      E10_multimodal: { high: "> 0.70", low: "< 0.3" }
      E12_late_interaction: { high: "> 0.70", low: "< 0.3" }
      E13_splade: { high: "> 0.60", low: "< 0.2" }
    injection_format: |
      DIVERGENCE DETECTED
      Recent activity in [semantic_space]: "[memory content summary]"
      Current appears different - similarity: {score}

  # -------------------------------------------------------------------------
  # TEMPORAL CONTEXT ENRICHMENT
  # -------------------------------------------------------------------------
  temporal_enrichment:
    description: "Temporal embedders provide context badges, not topic triggers"
    badges:
      same_session: "E2 similarity > 0.8 -> 'From same session'"
      same_day: "E3 similarity > 0.7 -> 'From today'"
      same_period: "E4 similarity > 0.6 -> 'Around same time'"
    usage: "Metadata on injected memories, NOT similarity/divergence triggers"

# ===============================================================================
# MEMORY SOURCES
# ===============================================================================
memory_sources:
  HookDescription:
    trigger: "Every hook event"
    content: "Claude's description of what it's doing"
    embedding: "All 13 embedders"

  ClaudeResponse:
    trigger: "SessionEnd, Stop hooks"
    content: "End-of-session answers, significant responses"
    embedding: "All 13 embedders"

  MDFileChunk:
    trigger: "File system events (create/modify .md files)"
    content: "Chunks from markdown files"
    embedding: "All 13 embedders"
    chunking:
      chunk_size: 200  # words
      overlap: 50  # words (25%)
      boundary: "Preserve sentence boundaries when possible"

# ===============================================================================
# CLUSTERING
# ===============================================================================
clustering:
  algorithms:
    batch: "HDBSCAN per embedding space"
    online: "BIRCH CF-trees for incremental updates"
  parameters:
    min_cluster_size: 3
    silhouette_threshold: 0.3
  cross_space_synthesis:
    method: "Weighted agreement across spaces"
    topic_threshold: 2.5
    confidence: "weighted_agreement / 8.5"

# ===============================================================================
# TELEOLOGICAL ARCHITECTURE
# ===============================================================================
teleological:
  core: "13-embedding array IS the teleological vector"
  topic_profile: "TP = [A(E1,V)..A(E13,V)] # 13D alignment scores, searchable"

  embedder_purposes:
    E1: "V_meaning"
    E2: "V_freshness"
    E3: "V_periodicity"
    E4: "V_ordering"
    E5: "V_causality"
    E6: "V_selectivity"
    E7: "V_correctness"
    E8: "V_connectivity"
    E9: "V_robustness"
    E10: "V_multimodality"
    E11: "V_factuality"
    E12: "V_precision"
    E13: "V_keyword_precision"

  # Topics emerge AUTONOMOUSLY from clustering
  # Manual goal/topic setting is FORBIDDEN
  # Valid: TeleologicalFingerprint<->TeleologicalFingerprint, TP<->TP, Ei<->Ei
  # Invalid: Manual topic definition, E1<->E7 comparison

# ===============================================================================
# 4-LAYER BIO-NERVOUS SYSTEM
# ===============================================================================
layers:
  L1_Sensing: { lat: "<5ms", components: [13-model embed, PII scrub, adversarial detect] }
  L3_Memory:  { lat: "<1ms", components: [MHN, FAISS GPU], capacity: "2^768 patterns" }
  L4_Learning: { freq: "100Hz", components: [UTL optimizer, neuromod controller] }
  L5_Coherence: { sync: "10ms", components: [Topic synthesis, context distiller, broadcast] }

# ===============================================================================
# NEUROMODULATION
# ===============================================================================
neuromod:
  Dopamine:     { param: retrieval_sharpness, range: "[1,5]", effect: "higher=sharp retrieval" }
  Serotonin:    { param: similarity.space_weights, range: "[0,1]", effect: "higher=more spaces" }
  Noradrenaline: { param: attention.temp, range: "[0.5,2]", effect: "higher=flat attention" }
  Acetylcholine: { param: utl.lr, range: "[0.001,0.002]", effect: "higher=faster update" }

# ===============================================================================
# DREAM LAYER
# ===============================================================================
dream:
  trigger:
    conditions:
      - "entropy > 0.7 for 5+ min"
      - "churn > 0.5 AND entropy > 0.7"
    constraints:
      activity: "<0.15"
      idle: "10min"
      gpu: "<80%"

  phases:
    nrem:
      duration: "3min"
      purpose: "Hebbian learning replay"
      formula: "Delta_w_ij = eta x phi_i x phi_j for high-importance edges"
      params: { learning_rate: 0.01, weight_decay: 0.001, weight_floor: 0.05, weight_cap: 1.0 }
      recency_bias: 0.8

    rem:
      duration: "2min"
      purpose: "Blind spot discovery via hyperbolic random walk"
      model: "Poincare ball"
      params: { dimensions: 64, curvature: -1.0, step_size: 0.1, max_steps: 100 }
      temperature: 2.0
      blind_spot: { min_semantic_distance: 0.7, require_shared_causal: true, new_edge_weight: 0.3 }

  constraints: { queries: 100, semantic_leap: 0.7, abort_on_query: true, wake: "<100ms", gpu: "<30%" }

  amortized:
    trigger: "3+ hop path traversed >= 5 times"
    min_hops: 3
    min_traversals: 5
    confidence_threshold: 0.7

# ===============================================================================
# UTL (Unified Theory of Learning)
# ===============================================================================
utl:
  canonical: "L = f((Delta_S x Delta_C) . w_e . cos phi) -> L in [0,1]"
  multi_embed: "L_multi = sigmoid(2.0 . (Sum_i tau_i lambda_S . Delta_S_i) . (Sum_j tau_j lambda_C . Delta_C_j) . w_e . cos phi)"
  params: { Delta_S: "[0,1] entropy", Delta_C: "[0,1] coherence", tau: "[0,1] teleological weight", w_e: "[0.5,1.5] emotional", phi: "[0,pi] phase" }

  lifecycle:
    infancy:  { n: "0-50",   lambda_Delta_S: 0.7, lambda_Delta_C: 0.3 }
    growth:   { n: "50-500", lambda_Delta_S: 0.5, lambda_Delta_C: 0.5 }
    maturity: { n: "500+",   lambda_Delta_S: 0.3, lambda_Delta_C: 0.7 }

  delta_methods:
    Delta_S: { E1: "GMM+Mahalanobis", E5: "Asymmetric KNN", E7: "GMM+KNN hybrid", E9: "Hamming", E10: "Cross-modal KNN", E11: "TransE ||h+r-t||", E12: "Token KNN", E13: "Jaccard", default: "KNN" }
    Delta_C: "0.4xConnectivity + 0.4xClusterFit + 0.2xConsistency"

# ===============================================================================
# META-UTL (Self-Aware Learning)
# ===============================================================================
meta_utl:
  awareness: [storage_prediction, retrieval_prediction, parameter_optimization]

  self_correction:
    rule: "prediction_error > 0.2 triggers lambda adjustment"
    formula: "lambda_new = lambda_old + alpha x (target - actual)"
    alpha_source: "acetylcholine_level / ACH_BASELINE"
    constraint: "lambda_S + lambda_C = 1.0, bounds [0.1, 0.9]"
    escalation: "accuracy < 0.7 for 100 ops -> BayesianLambdaOptimizer"

  predictors:
    storage_impact: { input: "fingerprint+context", output: "Delta_L", accuracy: ">0.85" }
    retrieval_quality: { input: "query+top_k", output: "relevance", accuracy: ">0.80" }

  domain_tracking: [Code, Medical, Legal, Creative, Research, General]

# ===============================================================================
# ADAPTIVE THRESHOLD CALIBRATION (ATC)
# ===============================================================================
adaptive_thresholds:
  priors:
    theta_topic: [2.5, "[1.5,4.0]"]  # Topic threshold
    theta_high_sim: [0.75, "[0.60,0.90]"]  # High similarity
    theta_low_sim: [0.30, "[0.20,0.45]"]  # Low similarity (divergence)
    theta_dup: [0.90, "[0.80,0.98]"]  # Duplicate detection
    theta_churn: [0.50, "[0.30,0.70]"]  # Churn threshold
    theta_entropy: [0.70, "[0.50,0.85]"]  # Entropy threshold

  levels:
    L1_EWMA: { freq: "per-query", formula: "theta_ewma=alpha x theta_obs+(1-alpha) x theta_ewma" }
    L2_Temp: { freq: hourly, formula: "sigma(logit(raw)/T)" }
    L3_Bandit: { freq: session, method: "Thompson sampling Beta(alpha,beta)" }
    L4_Bayesian: { freq: weekly, surrogate: "GP", acquisition: "EI" }

  calibration: { ECE: "<0.05", MCE: "<0.10", Brier: "<0.10" }
  alerts: { "ECE>0.10": L2, "drift>3.0": L4 }

# ===============================================================================
# MCP TOOLS
# ===============================================================================
mcp:
  version: "2024-11-05"
  transport: [stdio, sse]

  core_tools:
    topic:
      - get_topic_portfolio
      - get_topic_stability
      - get_topic_metrics
      - get_weighted_agreement
      - search_by_topic
    retrieval:
      - search_graph
      - inject_context
      - get_relevant_memories
      - detect_divergence
    clustering:
      - get_cluster_status
      - get_cross_space_synthesis
    memory:
      - store_memory
      - memory_retrieve
      - get_memetic_status
    adaptive:
      - get_threshold_status
      - get_calibration_metrics
      - trigger_recalibration
    dream:
      - trigger_dream
      - get_dream_status
    maintenance:
      - get_health_status
      - trigger_healing
      - get_pruning_candidates
      - execute_prune
      - trigger_consolidation

# ===============================================================================
# 13-MODEL EMBEDDINGS
# ===============================================================================
embeddings:
  paradigm: "NO FUSION - Store all 13 embeddings; ~17KB quantized"

  models:
    E1_Semantic: { dim: 1024, type: dense, quant: "PQ-8", category: SEMANTIC, topic_weight: 1.0, use: "General meaning" }
    E2_TemporalRecent: { dim: 512, type: dense, quant: Float8, category: TEMPORAL, topic_weight: 0.0, use: "Recency (metadata only)" }
    E3_TemporalPeriodic: { dim: 512, type: dense, quant: Float8, category: TEMPORAL, topic_weight: 0.0, use: "Cycles (metadata only)" }
    E4_TemporalPositional: { dim: 512, type: dense, quant: Float8, category: TEMPORAL, topic_weight: 0.0, use: "Ordering (metadata only)" }
    E5_Causal: { dim: 768, type: dense, quant: "PQ-8", asymmetric: true, category: SEMANTIC, topic_weight: 1.0, use: "Why/because" }
    E6_Sparse: { dim: "~30K 5%", type: sparse, category: SEMANTIC, topic_weight: 1.0, use: "Keywords" }
    E7_Code: { dim: 1536, type: dense, quant: "PQ-8", category: SEMANTIC, topic_weight: 1.0, use: "Code/tech" }
    E8_Graph: { dim: 384, type: dense, quant: Float8, category: RELATIONAL, topic_weight: 0.5, use: "Graph connectivity" }
    E9_HDC: { dim: "10K->1024", type: binary, category: STRUCTURAL, topic_weight: 0.5, use: "Structure" }
    E10_Multimodal: { dim: 768, type: dense, quant: "PQ-8", category: SEMANTIC, topic_weight: 1.0, use: "Intent" }
    E11_Entity: { dim: 384, type: dense, quant: Float8, category: RELATIONAL, topic_weight: 0.5, use: "Named entity relationships" }
    E12_LateInteraction: { dim: "128D/tok", type: dense_per_token, category: SEMANTIC, topic_weight: 1.0, use: "Precise match" }
    E13_SPLADE: { dim: "~30K sparse", type: sparse, category: SEMANTIC, topic_weight: 1.0, use: "Term expansion" }

  fingerprint: { semantic_fingerprint: "[E1..E13]", topic_profile: "[A(E1,V)..A(E13,V)]" }

  retrieval: # 5-Stage (<60ms @ 1M)
    S1: { desc: "BM25+E13 sparse", out: "10K", lat: "<5ms" }
    S2: { desc: "E1[..128] Matryoshka ANN", out: "1K", lat: "<10ms" }
    S3: { desc: "RRF across 13 spaces", out: "100", lat: "<20ms" }
    S4: { desc: "Topic alignment (weighted_agreement >= 2.5)", out: "50", lat: "<10ms" }
    S5: { desc: "E12 MaxSim precision", out: "10", lat: "<15ms" }

# ===============================================================================
# STORAGE
# ===============================================================================
storage:
  primary: { dev: rocksdb, prod: scylladb }
  schema: ["id:UUID", "embeddings:BYTEA", "topic_profile:REAL[13]", "source:ENUM(HookDescription,ClaudeResponse,MDFileChunk)", "coherence:REAL", "created_at:TIMESTAMPTZ", "session_id:VARCHAR", "chunk_metadata:JSONB"]
  indexes:
    L2A_sparse: "E13 SPLADE inverted"
    L2B_matryoshka: "E1[..128] HNSW M:32"
    L2C_per_embedder: "13x HNSW quantized"
    L2D_topic: "13D topic_profile HNSW"

# ===============================================================================
# PROGRESSIVE FEATURE ACTIVATION
# ===============================================================================
progressive_tiers:
  tier_0: { memories: 0, features: ["Storage", "Basic retrieval"], defaults: { cluster: -1, topic_profile: "[0.5;13]", stability: 1.0 } }
  tier_1: { memories: "1-2", features: ["Pairwise similarity"] }
  tier_2: { memories: "3-9", features: ["Basic clustering"] }
  tier_3: { memories: "10-29", features: ["Multiple clusters", "Divergence detection"] }
  tier_4: { memories: "30-99", features: ["Reliable statistics"] }
  tier_5: { memories: "100-499", features: ["Sub-clustering", "Trend analysis"] }
  tier_6: { memories: "500+", features: ["Full personalization"] }

# ===============================================================================
# INJECTION STRATEGY
# ===============================================================================
injection:
  priorities:
    P1: { type: "Divergence Alerts", condition: "Low similarity in SEMANTIC spaces to recent", budget: "~200 tokens" }
    P2: { type: "High-Relevance Topics", condition: "weighted_agreement >= 2.5", budget: "~400 tokens" }
    P3: { type: "Related Memories", condition: "weighted_agreement in [1.0, 2.5)", budget: "~300 tokens" }
    P4: { type: "Recent Context", content: "Last session summary", budget: "~200 tokens" }
    P5: { type: "Temporal Enrichment", content: "Same-session/period badges (metadata only)", budget: "~50 tokens" }

  relevance_score:
    formula: "Sum(category_weight_i x embedder_weight_i x max(0, similarity_i - threshold_i))"
    category_weights:
      SEMANTIC: 1.0
      TEMPORAL: 0.0  # Excluded from relevance
      RELATIONAL: 0.5
      STRUCTURAL: 0.5
    embedder_weights:
      E1: 1.0
      E5: 0.9
      E7: 0.85
      E6: 0.7
      E10: 0.8
      E12: 0.75
      E13: 0.7
      E8: 0.6
      E9: 0.5
      E11: 0.6

  recency_factors:
    "<1h": 1.3
    "<1d": 1.2
    "<7d": 1.1
    "<30d": 1.0
    ">90d": 0.8

  diversity_bonus:
    "weighted_agreement >= 5.0": 1.5  # Strong topic signal
    "weighted_agreement in [2.5, 5.0)": 1.2  # Topic threshold met
    "weighted_agreement in [1.0, 2.5)": 1.0  # Related
    "weighted_agreement < 1.0": 0.8  # Weak

# ===============================================================================
# BIAS MITIGATION
# ===============================================================================
bias_mitigation:
  thompson_sampling:
    exploration_budget: "15%"
    prior: "Beta(1,1) for new clusters"
    update: "accessed -> alpha+=1, not accessed -> beta+=1"

  mmr_diversity:
    lambda: 0.7
    formula: "MMR = lambda x relevance - (1-lambda) x max_similarity_to_selected"

  inverse_propensity:
    weight: "1 / propensity"
    propensity: "exposure_count / total_queries (floor 0.1)"

# ===============================================================================
# CLAUDE CODE CLI INTEGRATION (EXCLUSIVE TARGET)
# ===============================================================================
# This system is designed EXCLUSIVELY for Claude Code CLI
# No support for other LLMs, universal adapters, or generic interfaces
#
# CRITICAL: NATIVE CLAUDE CODE HOOKS ONLY
# ===============================================================================
# Integration uses NATIVE Claude Code hooks configured via .claude/settings.json
# These are NOT internal/built-in hooks - we use the official Claude Code hook system
# Shell script executors call context-graph-cli commands
# NO custom Claude Code modifications required - works with standard Claude Code

claude_code:
  target: "Claude Code CLI ONLY"
  paradigm: "NATIVE hooks + skills + MCP = autonomous topic discovery"

  # -------------------------------------------------------------------------
  # NATIVE HOOK ARCHITECTURE (CRITICAL)
  # -------------------------------------------------------------------------
  native_hooks:
    config_file: ".claude/settings.json"
    script_location: "hooks/*.sh"
    cli_binary: "context-graph-cli"
    rationale: "71% effort reduction vs custom hook infrastructure"

  # -------------------------------------------------------------------------
  # HOOK CONFIGURATION (.claude/settings.json)
  # -------------------------------------------------------------------------
  hooks:
    SessionStart:
      timeout_ms: 5000
      type: command
      command: "./hooks/session-start.sh"
      actions:
        - "Load topic portfolio from persistence"
        - "Initialize topic stability tracker"
        - "Warm FAISS indexes"
        - "Inject portfolio summary + recent divergences"

    UserPromptSubmit:
      timeout_ms: 2000
      type: command
      command: "./hooks/user-prompt-submit.sh"
      actions:
        - "Capture prompt text as memory"
        - "Embed with all 13 embedders"
        - "Search for similar memories (semantic spaces)"
        - "Detect divergence from recent activity"
        - "Inject: similar memories + divergence alerts"

    PreToolUse:
      timeout_ms: 500
      type: command
      command: "./hooks/pre-tool-use.sh"
      matcher: "Edit|Write|Bash"
      actions:
        - "Capture tool description"
        - "Inject brief relevant context"

    PostToolUse:
      timeout_ms: 3000
      type: command
      command: "./hooks/post-tool-use.sh"
      matcher: "*"
      async: true
      actions:
        - "Capture tool description + output summary"
        - "Embed with all 13 embedders"
        - "Store as HookDescription memory"
        - "Update BIRCH clustering online"

    Stop:
      timeout_ms: 3000
      type: command
      command: "./hooks/stop.sh"
      actions:
        - "Capture Claude's response summary"
        - "Store as ClaudeResponse memory"

    SessionEnd:
      timeout_ms: 30000
      type: command
      command: "./hooks/session-end.sh"
      actions:
        - "Capture session summary"
        - "Persist topic portfolio"
        - "Run HDBSCAN batch clustering"
        - "Check consolidation triggers"
        - "Run dream if entropy > 0.7 AND churn > 0.5"
        - "Update adaptive thresholds"

  # -------------------------------------------------------------------------
  # SKILLS (.claude/skills/*/SKILL.md)
  # -------------------------------------------------------------------------
  skills:
    topic-explorer:
      location: ".claude/skills/topic-explorer/SKILL.md"
      model: sonnet
      user_invocable: true
      description: |
        Explore emergent topic portfolio, topic stability metrics,
        and weighted agreement scores. Use when querying what topics
        exist, checking stability, or understanding topic relationships.
        Keywords: topics, portfolio, stability, churn, weighted agreement

    memory-inject:
      location: ".claude/skills/memory-inject/SKILL.md"
      model: haiku
      user_invocable: true
      description: |
        Retrieve and inject contextual memories for the current task.
        Automatically distills content to fit token budget. Use when
        starting tasks, needing background, or restoring context.
        Keywords: memory, context, inject, retrieve, recall, background

    semantic-search:
      location: ".claude/skills/semantic-search/SKILL.md"
      model: haiku
      user_invocable: true
      description: |
        Search the knowledge graph using multi-space retrieval.
        Supports semantic, causal, code, and entity search modes.
        Keywords: search, find, query, lookup, semantic, causal

    dream-consolidation:
      location: ".claude/skills/dream-consolidation/SKILL.md"
      model: sonnet
      user_invocable: true
      description: |
        Trigger memory consolidation via dream phases. NREM replays
        high-importance patterns. REM discovers blind spots via
        hyperbolic random walk. Use when entropy high or churn high.
        Keywords: dream, consolidate, nrem, rem, blind spots, entropy, churn
      triggers:
        - "entropy > 0.7 for 5+ min"
        - "entropy > 0.7 AND churn > 0.5"

    curation:
      location: ".claude/skills/curation/SKILL.md"
      model: sonnet
      user_invocable: true
      description: |
        Curate the knowledge graph by merging, annotating, or forgetting
        concepts. Process curation tasks from get_memetic_status.
        Keywords: curate, merge, forget, annotate, prune, duplicate

  # -------------------------------------------------------------------------
  # CLI COMMANDS (context-graph-cli)
  # -------------------------------------------------------------------------
  cli:
    binary: "context-graph-cli"
    commands:
      session:
        start: "Initialize session, load topic portfolio"
        end: "Persist state, run consolidation if needed"
        params: "--session-id <id>"
      topic:
        portfolio: "Get current topic portfolio"
        stability: "Get topic stability metrics"
        search: "Search by topic"
      inject-context:
        description: "Generate context for injection"
        params: "--format compact|standard|verbose"
      inject-brief:
        description: "Brief context for PreToolUse"
      capture-memory:
        description: "Capture and embed memory"
        params: "--source hook|response|mdfile"
      capture-response:
        description: "Capture Claude response"
      divergence:
        detect: "Detect divergence from recent"
        recent: "Get recent activity"

  # -------------------------------------------------------------------------
  # FILE STRUCTURE (NATIVE HOOKS)
  # -------------------------------------------------------------------------
  file_structure:
    native_hook_config:
      - ".claude/settings.json"
    shell_script_executors:
      - "hooks/session-start.sh"
      - "hooks/pre-tool-use.sh"
      - "hooks/post-tool-use.sh"
      - "hooks/user-prompt-submit.sh"
      - "hooks/stop.sh"
      - "hooks/session-end.sh"
    skills:
      - ".claude/skills/topic-explorer/SKILL.md"
      - ".claude/skills/memory-inject/SKILL.md"
      - ".claude/skills/semantic-search/SKILL.md"
      - ".claude/skills/dream-consolidation/SKILL.md"
      - ".claude/skills/curation/SKILL.md"
    cli:
      - "crates/context-graph-cli/src/main.rs"
      - "crates/context-graph-cli/src/commands/session.rs"
      - "crates/context-graph-cli/src/commands/topic.rs"
      - "crates/context-graph-cli/src/commands/inject.rs"
      - "crates/context-graph-cli/src/commands/capture.rs"

  # -------------------------------------------------------------------------
  # PERFORMANCE REQUIREMENTS
  # -------------------------------------------------------------------------
  performance:
    hooks:
      session_start: "<5s total"
      pre_tool: "<500ms"
      post_tool: "<3s (async OK)"
      user_prompt: "<2s"
      stop: "<3s"
      session_end: "<30s"
    cli:
      brief_output: "<100ms"
      inject_context: "<2s"
      capture_memory: "<500ms"
    topic:
      portfolio_query: "<100ms"
      stability_check: "<50ms"
      weighted_agreement: "<10ms"
      dream_wake: "<100ms"

# ===============================================================================
# SUCCESS CRITERIA
# ===============================================================================
success:
  topic_system:
    weighted_agreement: "Correctly computed per ARCH-09"
    temporal_exclusion: "E2-E4 contribute 0.0 to topics"
    divergence_detection: "Only SEMANTIC spaces trigger alerts"
    topic_stability: ">= 0.6 average"
    emergent_topics: "Discovered via clustering, no manual input"

  performance:
    full_retrieval: "<2000ms p95"       # Realistic with 13-model embedding
    embedding_13: "<1000ms p95"         # Sequential 13 models on single GPU
    pre_tool_hook: "<500ms p95"         # Fast path, no embedding
    memory_per_array: "<17KB quantized"

  memory_capture:
    hook_descriptions: "Captured and embedded"
    claude_responses: "Captured at session end"
    md_chunks: "200 words, 50 overlap"
