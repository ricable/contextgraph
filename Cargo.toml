# Context Graph - Workspace Manifest
# 13-Embedder Context Graph implementing UTL (Unified Theory of Learning)

[workspace]
resolver = "2"
members = [
    "crates/context-graph-mcp",
    "crates/context-graph-core",
    "crates/context-graph-cuda",
    "crates/context-graph-embeddings",
    "crates/context-graph-storage",
    "crates/context-graph-graph",
    "crates/context-graph-utl",
    "crates/context-graph-cli",
    "crates/context-graph-benchmark",
]

[workspace.package]
version = "0.1.0"
edition = "2021"
rust-version = "1.75"
license = "MIT OR Apache-2.0"
repository = "https://github.com/org/context-graph"

[workspace.dependencies]
# Async runtime
tokio = { version = "1.35", features = ["full"] }
async-trait = "0.1"

# Web framework (for SSE transport - TASK-32)
axum = "0.8"
tokio-stream = "0.1"
async-stream = "0.3"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
bincode = "1.3"
toml = "0.8"

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["json", "env-filter"] }

# Configuration
config = "0.14"

# IDs and time
uuid = { version = "1.6", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }

# Utilities
rand = "0.8"
rand_distr = "0.4"

# GPU/ML - Candle framework (HuggingFace)
# https://github.com/huggingface/candle
# Using 0.9.2-alpha for CUDA 13.x support (RTX 5090 Blackwell)
candle-core = { version = "0.9.2-alpha", features = ["cuda"] }
candle-nn = { version = "0.9.2-alpha", features = ["cuda"] }
candle-transformers = { version = "0.9.2-alpha", features = ["cuda"] }

# Internal crates
context-graph-core = { path = "crates/context-graph-core" }
context-graph-embeddings = { path = "crates/context-graph-embeddings" }
context-graph-cuda = { path = "crates/context-graph-cuda" }

[profile.release]
lto = "thin"
codegen-units = 1
strip = true

[profile.dev]
# Faster compile times for development
opt-level = 0
debug = true
